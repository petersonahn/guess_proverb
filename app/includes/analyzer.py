"""
ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ì†ë‹´ ë‚œì´ë„ ë¶„ì„ í´ë˜ìŠ¤

ì´ ëª¨ë“ˆì€ ì–¸ì–´í•™ì  íŠ¹ì„±ê³¼ AI ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì†ë‹´ì˜ ë‚œì´ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- ì–¸ì–´í•™ì  ë¶„ì„ (60%): ë³µì¡ì„±, ì–´íœ˜, êµ¬ì¡°, ë¹ˆë„
- AI ëª¨ë¸ ë¶„ì„ (40%): models--jhgan--ko-sroberta-multitask ì„ë² ë”© íŠ¹ì„± ë¶„ì„

ì£¼ìš” ê¸°ëŠ¥:
1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (proverb_game.proverb í…Œì´ë¸”)
2. í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ë¶„ì„ (ì–¸ì–´í•™ì  + AI ëª¨ë¸)
3. models--jhgan--ko-sroberta-multitask ëª¨ë¸ ì „ìš© ì‚¬ìš©
4. ë°°ì¹˜ ì²˜ë¦¬ (16ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬)
5. ë¶„ì„ ê²°ê³¼ ìºì‹± ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
6. ì§„í–‰ë¥  í‘œì‹œ (tqdm)

ì‚¬ìš© ì˜ˆì‹œ:
    analyzer = ProverbDifficultyAnalyzer()
    result = analyzer.analyze_proverb_difficulty(proverb_id=1)
    all_results = analyzer.batch_analyze_all_proverbs()
"""

import os
import sys
import time
import traceback
import logging
import re
from typing import Dict, List, Optional, Union, Any, Tuple
from datetime import datetime

# config ë° database ëª¨ë“ˆ import
try:
    import sys
    import os
    # í˜„ì¬ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ì„ sys.pathì— ì¶”ê°€
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)  # app ë””ë ‰í† ë¦¬
    root_dir = os.path.dirname(parent_dir)     # í”„ë¡œì íŠ¸ ë£¨íŠ¸
    sys.path.insert(0, root_dir)
    
    from app.core.config import proverb_config
    from app.includes.dbconn import ProverbDatabase
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("í•´ê²° ë°©ë²•: config.py, dbconn.py íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    import torch
    from sentence_transformers import SentenceTransformer
    from transformers import AutoTokenizer, AutoModel
    from tqdm import tqdm
    import numpy as np
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    print("í•´ê²° ë°©ë²•: pip install -r requirements.txt")
    sys.exit(1)


class ProverbDifficultyAnalyzer:
    """
    ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ë¶„ì„ í´ë˜ìŠ¤
    
    ì–¸ì–´í•™ì  íŠ¹ì„±ê³¼ AI ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì†ë‹´ì˜ ë‚œì´ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    íŠ¹ì§•:
    - ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (proverb_game.proverb í…Œì´ë¸”)
    - models--jhgan--ko-sroberta-multitask ëª¨ë¸ ì „ìš© ì‚¬ìš©
    - í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„: ì–¸ì–´í•™ì  ë¶„ì„(60%) + AI ì„ë² ë”© ë¶„ì„(40%)
    - 3ë‹¨ê³„ ë‚œì´ë„ ë¶„ë¥˜ (1ì , 2ì , 3ì )
    - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™” (16ê°œì”© ì²˜ë¦¬)
    - ë¶„ì„ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ë¶„ì„ ë°©ì§€
    """
    
    def __init__(self):
        """
        ğŸ¤– ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì´ˆê¸°í™” (í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„)
        """
        self.config = proverb_config
        self.model_name = self.config.MODEL_NAME
        self.device = self.config.DEVICE
        self.cache_dir = self.config.MODEL_CACHE_DIR
        self.batch_size = self.config.BATCH_SIZE_ANALYSIS
        
        # AI ëª¨ë¸ ì´ˆê¸°í™”
        self.sentence_model = None
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.db = None
        
        # ë¶„ì„ ê²°ê³¼ ìºì‹œ
        self.analysis_cache = {} if self.config.ENABLE_CACHING else None
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "total_analyzed": 0,
            "cache_hits": 0,
            "total_processing_time": 0.0,
            "batch_count": 0
        }
        
        print(f"ğŸ¯ ì†ë‹´ ê²Œì„ ë‚œì´ë„ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        print(f"ğŸ¤– AI ëª¨ë¸: {self.model_name}")
        print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ”¬ ë¶„ì„ ë°©ì‹: ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬ (AI ì‚¬ìš©ë¹ˆë„ 60% + ì–¸ì–´ì ë³µì¡ì„± 40%)")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ğŸ’¾ ìºì‹±: {'í™œì„±í™”' if self.config.ENABLE_CACHING else 'ë¹„í™œì„±í™”'}")
        
        # AI ëª¨ë¸ ë¡œë”©
        if not self._load_ai_model():
            raise Exception("AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        if not self._connect_database():
            raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        print(f"âœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ!")
    
    def _load_ai_model(self) -> bool:
        """
        ğŸ¤– models--jhgan--ko-sroberta-multitask ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ëª¨ë¸ ë¡œë”© ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("â³ AI ëª¨ë¸ ë¡œë”© ì¤‘...")
            print(f"ğŸ“ ëª¨ë¸: {self.model_name}")
            print(f"ğŸ“ ìºì‹œ ë””ë ‰í† ë¦¬: {self.cache_dir}")
            
            # SentenceTransformer ëª¨ë¸ ë¡œë”©
            self.sentence_model = SentenceTransformer(
                self.model_name,
                cache_folder=self.cache_dir,
                device=self.device
            )
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            test_text = "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤"
            test_embedding = self.sentence_model.encode([test_text], convert_to_tensor=True)
            
            print(f"âœ… AI ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
            print(f"ğŸ“Š ì„ë² ë”© ì°¨ì›: {test_embedding.shape[1]}")
            print(f"ğŸ’» ëª¨ë¸ ë””ë°”ì´ìŠ¤: {test_embedding.device}")
            
            return True
            
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return False
    
    def analyze_usage_frequency_with_ai(self, proverb_text: str) -> Dict[str, float]:
        """
        ğŸ§  AI ëª¨ë¸ì„ í™œìš©í•œ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ (ìƒˆë¡œìš´ ì ‘ê·¼ë²•)
        
        ì†ë‹´ê³¼ "ì¼ë°˜ í•œêµ­ì–´ ì‚¬ìš© íŒ¨í„´"ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ì‚¬ìš© ë¹ˆë„ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, float]: AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ íŠ¹ì„±
        """
        try:
            # ì†ë‹´ ì„ë² ë”© ìƒì„±
            proverb_embedding = self.sentence_model.encode([proverb_text], convert_to_tensor=True)[0]
            
            # ì†ë‹´ê³¼ ìœ ì‚¬í•œ í•œêµ­ì–´ í‘œí˜„ íŒ¨í„´ë“¤ (ì†ë‹´ì— íŠ¹í™”)
            common_usage_patterns = [
                # ì¼ìƒì—ì„œ ìì£¼ ì¸ìš©ë˜ëŠ” í‘œí˜„ íŒ¨í„´
                "í•­ìƒ ë§í•˜ë“¯ì´",
                "ëˆ„êµ¬ë‚˜ ì•„ëŠ” ì´ì•¼ê¸°ì§€ë§Œ",
                "ì–´ë¥¸ë“¤ì´ ë§ì”€í•˜ì‹œê¸¸",
                "ì˜›ë‚ ë¶€í„° ì „í•´ì˜¤ëŠ” ë§ë¡œ",
                "ìš°ë¦¬ê°€ í”íˆ í•˜ëŠ” ë§ì´",
                
                # êµìœ¡/í›ˆê³„ì  ë¬¸ë§¥
                "ì¤‘ìš”í•œ êµí›ˆì´ ìˆë‹¤ë©´",
                "ê²½í—˜ìœ¼ë¡œ ë°°ìš´ ê²ƒì€",
                "ì§€í˜œë¡œìš´ ë§ì”€ì´ ìˆì£ ",
                "ìƒê°í•´ë³¼ ë§Œí•œ ì´ì•¼ê¸°",
                "ê¸°ì–µí•´ë‘˜ ë§Œí•œ ë§ì”€",
                
                # ì¼ë°˜ì  ì§€ì‹/ìƒì‹ í‘œí˜„
                "ë§ì€ ì‚¬ëŒë“¤ì´ ë§í•˜ëŠ”",
                "ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„",
                "ëˆ„êµ¬ë‚˜ ê³µê°í•˜ëŠ”",
                "í”íˆ ë“¤ì–´ë³¸ ì´ì•¼ê¸°",
                "ë„ë¦¬ ì•Œë ¤ì§„ ë§ì”€"
            ]
            
            # ê° íŒ¨í„´ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            pattern_embeddings = self.sentence_model.encode(common_usage_patterns, convert_to_tensor=True)
            
            from sentence_transformers.util import cos_sim
            similarities = cos_sim(proverb_embedding, pattern_embeddings)
            
            # 1. ì¼ìƒ ëŒ€í™”ì™€ì˜ ìœ ì‚¬ë„ (0-4ë²ˆ íŒ¨í„´)
            daily_similarity = similarities[0:5].mean().item()
            
            # 2. êµìœ¡ì  ë¬¸ë§¥ê³¼ì˜ ìœ ì‚¬ë„ (5-9ë²ˆ íŒ¨í„´)  
            educational_similarity = similarities[5:10].mean().item()
            
            # 3. ë¯¸ë””ì–´/ë¬¸í•™ì  í‘œí˜„ê³¼ì˜ ìœ ì‚¬ë„ (10-14ë²ˆ íŒ¨í„´)
            media_similarity = similarities[10:15].mean().item()
            
            # 4. ì „ì²´ í‰ê·  ìœ ì‚¬ë„
            overall_similarity = similarities.mean().item()
            
            # 5. ë¬¸ì¥ êµ¬ì¡° ì¹œìˆ™ì„± (ê°„ë‹¨í•œ êµ¬ì¡°ì¼ìˆ˜ë¡ ì¹œìˆ™í•¨)
            words = proverb_text.split()
            structure_familiarity = max(0.0, min(1.0 - (len(words) - 4) / 8.0, 1.0))  # 4-12ë‹¨ì–´ ê¸°ì¤€, 1.0 ì œí•œ
            
            return {
                "daily_context_similarity": max(0.0, daily_similarity),           # ì¼ìƒ ë¬¸ë§¥ ìœ ì‚¬ë„
                "educational_similarity": max(0.0, educational_similarity),       # êµìœ¡ì  ìœ ì‚¬ë„
                "media_similarity": max(0.0, media_similarity),                   # ë¯¸ë””ì–´ ìœ ì‚¬ë„
                "overall_familiarity": max(0.0, overall_similarity),             # ì „ì²´ ì¹œìˆ™ë„
                "structure_familiarity": structure_familiarity                    # êµ¬ì¡°ì  ì¹œìˆ™ì„±
            }
            
        except Exception as e:
            print(f"âš ï¸ AI ì‚¬ìš© ë¹ˆë„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                "daily_context_similarity": 0.5,
                "educational_similarity": 0.5,
                "media_similarity": 0.5,
                "overall_familiarity": 0.5,
                "structure_familiarity": 0.5
            }
    
    def calculate_linguistic_complexity(self, proverb_text: str) -> float:
        """
        ğŸ“ ì–¸ì–´ì  ë³µì¡ì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
        
        Returns:
            float: ì–¸ì–´ì  ë³µì¡ì„± ì ìˆ˜
            - 0.0: ë§¤ìš° ê°„ë‹¨ (ë ˆë²¨ 1ì— ì í•©)
            - 0.5: ë³´í†µ ë³µì¡ (ë ˆë²¨ 2ì— ì í•©)
            - 1.0: ë§¤ìš° ë³µì¡ (ë ˆë²¨ 3ì— ì í•©)
        """
        try:
            clean_text = proverb_text.replace(' ', '')
            words = proverb_text.split()
            
            # 1. ê¸€ì ìˆ˜ ê¸°ë°˜ ë³µì¡ì„± (0.0 ~ 1.0)
            # 5ê¸€ì ì´í•˜: 0.0, 15ê¸€ì ì´ìƒ: 1.0
            char_complexity = min(max(len(clean_text) - 5, 0) / 10.0, 1.0)
            
            # 2. ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ë³µì¡ì„± (0.0 ~ 1.0)
            # 3ë‹¨ì–´ ì´í•˜: 0.0, 8ë‹¨ì–´ ì´ìƒ: 1.0
            word_complexity = min(max(len(words) - 3, 0) / 5.0, 1.0)
            
            # 3. í‰ê·  ë‹¨ì–´ ê¸¸ì´ ë³µì¡ì„± (0.0 ~ 1.0)
            # í‰ê·  2ê¸€ì: 0.0, í‰ê·  4ê¸€ì ì´ìƒ: 1.0
            avg_word_len = len(clean_text) / max(len(words), 1)
            word_len_complexity = min(max(avg_word_len - 2, 0) / 2.0, 1.0)
            
            # 4. ìŒì„±í•™ì  ë³µì¡ì„± (ë°›ì¹¨ì´ ë§ì„ìˆ˜ë¡ ë³µì¡)
            final_consonants = 0
            for char in clean_text:
                # í•œê¸€ ë°›ì¹¨ ê²€ì‚¬
                if 'ê°€' <= char <= 'í£':
                    char_code = ord(char) - ord('ê°€')
                    final_consonant = char_code % 28
                    if final_consonant > 0:  # ë°›ì¹¨ì´ ìˆìŒ
                        final_consonants += 1
            
            consonant_ratio = final_consonants / max(len(clean_text), 1)
            phonetic_complexity = min(consonant_ratio * 2, 1.0)
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ë³µì¡ì„± ê³„ì‚°
            final_complexity = (
                char_complexity * 0.3 +
                word_complexity * 0.3 +
                word_len_complexity * 0.2 +
                phonetic_complexity * 0.2
            )
            
            return round(min(max(final_complexity, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì–¸ì–´ì  ë³µì¡ì„± ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_vocabulary_difficulty(self, proverb_text: str) -> float:
        """
        ğŸ“š ì–´íœ˜ ë‚œì´ë„ ì ìˆ˜ ê³„ì‚° (í†µê³„ì  ì ‘ê·¼, í•˜ë“œì½”ë”© ìµœì†Œí™”)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: ì–´íœ˜ ë‚œì´ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì–´ë ¤ìš´ ì–´íœ˜)
        """
        try:
            words = proverb_text.split()
            total_words = len(words)
            
            if total_words == 0:
                return 0.5
            
            # 1. í‰ê·  ë‹¨ì–´ ê¸¸ì´ (ê¸´ ë‹¨ì–´ì¼ìˆ˜ë¡ ì–´ë ¤ì›€)
            avg_word_length = sum(len(word) for word in words) / total_words
            length_complexity = min(max(avg_word_length - 2, 0) / 3.0, 1.0)
            
            # 2. í•œê¸€ ìëª¨ ë³µì¡ì„± (ë³µì¡í•œ ììŒ/ëª¨ìŒ ì¡°í•©)
            complex_chars = 0
            total_chars = 0
            for word in words:
                for char in word:
                    if 'ê°€' <= char <= 'í£':
                        total_chars += 1
                        # ë³µì¡í•œ ë°›ì¹¨ì´ë‚˜ ë³µí•© ëª¨ìŒ
                        char_code = ord(char) - ord('ê°€')
                        final_consonant = char_code % 28
                        if final_consonant > 16:  # ë³µì¡í•œ ë°›ì¹¨ (ã„º, ã„», ã„¼ ë“±)
                            complex_chars += 1
            
            char_complexity = complex_chars / max(total_chars, 1)
            
            # 3. ì–´íœ˜ ë‹¤ì–‘ì„± (ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ë³µì¡)
            unique_words = len(set(words))
            diversity = unique_words / total_words
            
            # 4. ìŒì„±í•™ì  ë³µì¡ì„± (ì–´ë ¤ìš´ ë°œìŒ íŒ¨í„´)
            phonetic_complexity = 0
            for word in words:
                # ì—°ì†ëœ ììŒì´ë‚˜ ë³µì¡í•œ íŒ¨í„´
                if len(word) >= 3:
                    # 3ê¸€ì ì´ìƒ ë‹¨ì–´ì—ì„œ ë³µì¡í•œ íŒ¨í„´ ê°ì§€
                    if any(consonant in word for consonant in ['ã„±ã„±', 'ã„´ã„´', 'ã„¹ã„¹']):
                        phonetic_complexity += 0.2
            
            phonetic_score = min(phonetic_complexity / total_words, 1.0)
            
            # ìµœì¢… ì–´íœ˜ ë‚œì´ë„ (í†µê³„ì  ì ‘ê·¼)
            vocab_difficulty = (
                length_complexity * 0.4 +      # ë‹¨ì–´ ê¸¸ì´
                char_complexity * 0.3 +        # ìëª¨ ë³µì¡ì„±
                diversity * 0.2 +               # ì–´íœ˜ ë‹¤ì–‘ì„±
                phonetic_score * 0.1           # ìŒì„±í•™ì  ë³µì¡ì„±
            )
            
            return round(min(max(vocab_difficulty, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì–´íœ˜ ë‚œì´ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_structural_simplicity(self, proverb_text: str) -> float:
        """
        ğŸ—ï¸ êµ¬ì¡°ì  ë‹¨ìˆœì„± ì ìˆ˜ ê³„ì‚° (í†µê³„ì  ì ‘ê·¼)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: êµ¬ì¡°ì  ë‹¨ìˆœì„± ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë‹¨ìˆœí•¨)
        """
        try:
            words = proverb_text.split()
            total_words = len(words)
            
            if total_words == 0:
                return 0.5
            
            # 1. ë¬¸ì¥ ê¸¸ì´ ë‹¨ìˆœì„± (ì§§ì„ìˆ˜ë¡ ë‹¨ìˆœ)
            length_simplicity = max(0.0, 1.0 - (total_words - 3) / 8.0)
            
            # 2. ì–´ë¯¸ ë³µì¡ì„± (ë³µì¡í•œ ì–´ë¯¸ê°€ ì ì„ìˆ˜ë¡ ë‹¨ìˆœ)
            complex_endings = ['ë©´ì„œ', 'ì§€ë§Œ', 'ë”ë¼ë„', 'ì—ë„', 'ë¼ë„']
            ending_complexity = sum(1 for ending in complex_endings if ending in proverb_text)
            ending_simplicity = max(0.0, 1.0 - ending_complexity / 3.0)
            
            # 3. ëŒ€ì¹­ì„± (ì†ë‹´ì˜ íŠ¹ì§• - ëŒ€ì¹­ì ì¼ìˆ˜ë¡ ê¸°ì–µí•˜ê¸° ì‰¬ì›€)
            symmetry_score = self._calculate_symmetry_score(proverb_text)
            
            # 4. ë°˜ë³µì„± (ë°˜ë³µ íŒ¨í„´ì´ ìˆì„ìˆ˜ë¡ ë‹¨ìˆœ)
            repetition_score = self._calculate_repetition_score(proverb_text)
            
            # 5. êµ¬ë‘ì /íŠ¹ìˆ˜ë¬¸ì ë³µì¡ì„±
            punctuation_count = sum(1 for char in proverb_text if char in '.,!?;:')
            punctuation_simplicity = max(0.0, 1.0 - punctuation_count / 3.0)
            
            # ìµœì¢… ë‹¨ìˆœì„± ì ìˆ˜ (í†µê³„ì  ì ‘ê·¼)
            simplicity = (
                length_simplicity * 0.3 +        # ê¸¸ì´ ë‹¨ìˆœì„±
                ending_simplicity * 0.25 +       # ì–´ë¯¸ ë‹¨ìˆœì„±
                symmetry_score * 0.2 +           # ëŒ€ì¹­ì„±
                repetition_score * 0.15 +        # ë°˜ë³µì„±
                punctuation_simplicity * 0.1     # êµ¬ë‘ì  ë‹¨ìˆœì„±
            )
            
            return round(min(max(simplicity, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡°ì  ë‹¨ìˆœì„± ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def _calculate_symmetry_score(self, text: str) -> float:
        """ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        words = text.split()
        
        if len(words) <= 2:
            return 0.8  # ì§§ì€ ì†ë‹´ì€ ëŒ€ì¹­ì„± ë†’ìŒ
        
        # ë‹¨ì–´ ìˆ˜ê°€ ì§ìˆ˜ì¸ì§€ (ëŒ€ì¹­ êµ¬ì¡° ê°€ëŠ¥ì„±)
        even_bonus = 0.3 if len(words) % 2 == 0 else 0.0
        
        # ì•ë’¤ ëŒ€ì¹­ êµ¬ì¡° í™•ì¸
        mid = len(words) // 2
        front_half = words[:mid]
        back_half = words[mid:] if len(words) % 2 == 0 else words[mid+1:]
        
        # êµ¬ì¡°ì  ìœ ì‚¬ì„± (ê¸¸ì´ê°€ ë¹„ìŠ·í•œê°€)
        if front_half and back_half:
            len_similarity = 1 - abs(len(front_half) - len(back_half)) / max(len(front_half), len(back_half))
            structure_bonus = len_similarity * 0.4
        else:
            structure_bonus = 0.0
        
        # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë‚˜ êµ¬ì¡°
        repeated_words = len(set(words)) < len(words)
        repeat_bonus = 0.3 if repeated_words else 0.0
        
        return min(even_bonus + structure_bonus + repeat_bonus, 1.0)
    
    def _calculate_repetition_score(self, text: str) -> float:
        """ë°˜ë³µ íŒ¨í„´ ì ìˆ˜ ê³„ì‚°"""
        words = text.split()
        
        if len(words) <= 2:
            return 0.5
        
        # ë‹¨ì–´ ë°˜ë³µ
        unique_words = len(set(words))
        repetition_ratio = 1 - (unique_words / len(words))
        
        # ìŒì„±ì  ë°˜ë³µ (ìš´ìœ¨)
        syllable_patterns = []
        for word in words:
            if len(word) > 0:
                syllable_patterns.append(word[-1])  # ë§ˆì§€ë§‰ ê¸€ì
        
        unique_endings = len(set(syllable_patterns))
        sound_repetition = 1 - (unique_endings / max(len(syllable_patterns), 1))
        
        return (repetition_ratio * 0.6 + sound_repetition * 0.4)
    
    def estimate_usage_frequency(self, proverb_text: str) -> float:
        """
        ğŸ“Š ì‚¬ìš© ë¹ˆë„ ì¶”ì • (í†µê³„ì  ì ‘ê·¼, í•˜ë“œì½”ë”© ìµœì†Œí™”)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: ì‚¬ìš© ë¹ˆë„ ì¶”ì •ê°’ (ë†’ì„ìˆ˜ë¡ ìì£¼ ì‚¬ìš©ë¨ = ì‰¬ì›€)
        """
        try:
            clean_text = proverb_text.replace(' ', '')
            words = proverb_text.split()
            
            # 1. ê¸¸ì´ ê¸°ë°˜ ë¹ˆë„ (ì§§ì„ìˆ˜ë¡ ê¸°ì–µí•˜ê¸° ì‰¬ì›Œ ìì£¼ ì‚¬ìš©)
            length_score = max(0.0, 1.0 - (len(clean_text) - 4) / 12.0)
            
            # 2. ë‹¨ì–´ ë‹¨ìˆœì„± (ë‹¨ìˆœí•œ ë‹¨ì–´ì¼ìˆ˜ë¡ ìì£¼ ì‚¬ìš©)
            avg_word_length = sum(len(word) for word in words) / max(len(words), 1)
            word_simplicity = max(0.0, 1.0 - (avg_word_length - 2) / 4.0)
            
            # 3. ìŒì„±í•™ì  ë‹¨ìˆœì„± (ë°œìŒí•˜ê¸° ì‰¬ìš¸ìˆ˜ë¡ ìì£¼ ì‚¬ìš©)
            phonetic_simplicity = 0
            for word in words:
                if len(word) <= 3:  # ì§§ì€ ë‹¨ì–´
                    phonetic_simplicity += 0.2
                # ë°˜ë³µ ìŒì„± íŒ¨í„´ (ê¸°ì–µí•˜ê¸° ì‰¬ì›€)
                if len(set(word)) < len(word):  # ë°˜ë³µë˜ëŠ” ê¸€ì
                    phonetic_simplicity += 0.1
            
            phonetic_score = min(phonetic_simplicity / len(words), 1.0)
            
            # 4. ë¦¬ë“¬ê° (ìš´ìœ¨ì´ ìˆì„ìˆ˜ë¡ ê¸°ì–µí•˜ê¸° ì‰¬ì›Œ ìì£¼ ì‚¬ìš©)
            rhythm_score = 0
            if len(words) % 2 == 0:  # ì§ìˆ˜ ë‹¨ì–´ (ëŒ€ì¹­ì„±)
                rhythm_score += 0.3
            
            # ë‹¨ì–´ ê¸¸ì´ íŒ¨í„´ì˜ ê·œì¹™ì„±
            word_lengths = [len(word) for word in words]
            if len(set(word_lengths)) <= 2:  # ë¹„ìŠ·í•œ ê¸¸ì´ì˜ ë‹¨ì–´ë“¤
                rhythm_score += 0.2
            
            rhythm_score = min(rhythm_score, 1.0)
            
            # 5. êµ¬ì¡°ì  ì¹œìˆ™ì„± (ì¼ë°˜ì ì¸ ë¬¸ì¥ êµ¬ì¡°ì¼ìˆ˜ë¡ ìì£¼ ì‚¬ìš©)
            structure_familiarity = 0
            if 3 <= len(words) <= 7:  # ì ë‹¹í•œ ê¸¸ì´
                structure_familiarity += 0.4
            
            # ë°˜ë³µ êµ¬ì¡°ë‚˜ ëŒ€ì¹­ êµ¬ì¡°
            if len(words) >= 4:
                mid = len(words) // 2
                if words[:mid] == words[mid:] or len(set(words)) < len(words):
                    structure_familiarity += 0.3
            
            structure_score = min(structure_familiarity, 1.0)
            
            # ìµœì¢… ì‚¬ìš© ë¹ˆë„ ì ìˆ˜ (í†µê³„ì  ì ‘ê·¼)
            frequency_score = (
                length_score * 0.3 +           # ê¸¸ì´ ë‹¨ìˆœì„±
                word_simplicity * 0.25 +       # ë‹¨ì–´ ë‹¨ìˆœì„±
                phonetic_score * 0.2 +         # ìŒì„±í•™ì  ë‹¨ìˆœì„±
                rhythm_score * 0.15 +          # ë¦¬ë“¬ê°
                structure_score * 0.1          # êµ¬ì¡°ì  ì¹œìˆ™ì„±
            )
            
            return round(min(max(frequency_score, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì‚¬ìš© ë¹ˆë„ ì¶”ì • ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_final_difficulty(self, proverb_text: str) -> Dict[str, Any]:
        """
        ğŸ¯ ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬ ë‚œì´ë„ ê³„ì‚° (ìƒˆë¡œìš´ ì ‘ê·¼ë²•)
        
        ì‚¬ìš© ë¹ˆë„(60%) + ì–¸ì–´ì  ë³µì¡ì„±(40%)ìœ¼ë¡œ ë‚œì´ë„ë¥¼ íŒë³„í•©ë‹ˆë‹¤.
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼
        """
        try:
            # 1. AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ (60%) - ì£¼ìš” ì§€í‘œ
            ai_frequency_analysis = self.analyze_usage_frequency_with_ai(proverb_text)
            
            # ì‚¬ìš© ë¹ˆë„ ì¢…í•© ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìì£¼ ì‚¬ìš©ë¨ = ì‰¬ì›€)
            usage_frequency_score = (
                ai_frequency_analysis["daily_context_similarity"] * 0.3 +    # ì¼ìƒ ë¬¸ë§¥
                ai_frequency_analysis["educational_similarity"] * 0.25 +     # êµìœ¡ì  ë¬¸ë§¥
                ai_frequency_analysis["media_similarity"] * 0.2 +            # ë¯¸ë””ì–´ ë¬¸ë§¥
                ai_frequency_analysis["overall_familiarity"] * 0.15 +        # ì „ì²´ ì¹œìˆ™ë„
                ai_frequency_analysis["structure_familiarity"] * 0.1         # êµ¬ì¡°ì  ì¹œìˆ™ì„±
            )
            
            # 2. ì–¸ì–´ì  ë³µì¡ì„± ë¶„ì„ (40%) - ë³´ì¡° ì§€í‘œ
            linguistic_complexity = self.calculate_linguistic_complexity(proverb_text)
            vocab_difficulty = self.calculate_vocabulary_difficulty(proverb_text)
            structural_simplicity = self.calculate_structural_simplicity(proverb_text)
            
            # ì–¸ì–´ì  ë³µì¡ì„± ì¢…í•© ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë³µì¡í•¨ = ì–´ë ¤ì›€)
            complexity_score = (
                linguistic_complexity * 0.4 +          # ì–¸ì–´ì  ë³µì¡ì„±
                vocab_difficulty * 0.35 +              # ì–´íœ˜ ë‚œì´ë„
                (1 - structural_simplicity) * 0.25     # êµ¬ì¡°ì  ë³µì¡ì„±
            )
            
            # 3. ìµœì¢… ë‚œì´ë„ ì ìˆ˜ ê³„ì‚°
            # ì‚¬ìš© ë¹ˆë„ê°€ ë†’ì„ìˆ˜ë¡ ì‰½ê³ , ë³µì¡ì„±ì´ ë†’ì„ìˆ˜ë¡ ì–´ë ¤ì›€
            final_score = (1 - usage_frequency_score) * 0.6 + complexity_score * 0.4
            
            # ì ìˆ˜ë¥¼ 1-3 ë ˆë²¨ë¡œ ë³€í™˜ (ìµœì  ì„±ëŠ¥ ê²½ê³„ê°’)
            if final_score <= 0.58:  # ì‰¬ìš´ ì†ë‹´
                difficulty_level = 1  # ì‰¬ì›€
                confidence = 1.0 - final_score * 1.2
            elif final_score <= 0.68:  # ì¤‘ê°„ ë²”ìœ„
                difficulty_level = 2  # ë³´í†µ
                confidence = 1.0 - abs(final_score - 0.63) * 3
            else:
                difficulty_level = 3  # ì–´ë ¤ì›€
                confidence = (final_score - 0.68) * 3 + 0.4
            
            confidence = min(max(confidence, 0.3), 1.0)
            
            # ì„¤ëª… ìƒì„± (ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬)
            explanation = self._generate_frequency_based_explanation(
                difficulty_level, final_score, usage_frequency_score, complexity_score,
                ai_frequency_analysis
            )
            
            return {
                "difficulty_level": difficulty_level,
                "confidence": round(confidence, 3),
                "final_score": round(final_score, 3),
                "explanation": explanation,
                "breakdown": {
                    "usage_frequency_analysis": {
                        **ai_frequency_analysis,
                        "usage_frequency_score": round(usage_frequency_score, 3)
                    },
                    "linguistic_complexity_analysis": {
                        "linguistic_complexity": linguistic_complexity,
                        "vocabulary_difficulty": vocab_difficulty,
                        "structural_simplicity": structural_simplicity,
                        "complexity_score": round(complexity_score, 3)
                    }
                },
                "weights": {
                    "usage_frequency": 0.6,
                    "linguistic_complexity": 0.4
                }
            }
            
        except Exception as e:
            print(f"âš ï¸ ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬ ë‚œì´ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return {
                "difficulty_level": 2,
                "confidence": 0.5,
                "final_score": 0.5,
                "explanation": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "breakdown": {},
                "weights": {}
            }
    
    def _generate_explanation(self, level: int, score: float, ling: float, 
                            vocab: float, struct: float, freq: float) -> str:
        """ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„± (ê¸°ì¡´ ë²„ì „)"""
        level_names = {1: "ì‰¬ì›€", 2: "ë³´í†µ", 3: "ì–´ë ¤ì›€"}
        level_name = level_names[level]
        
        reasons = []
        
        # ì£¼ìš” ìš”ì¸ ë¶„ì„
        if ling < 0.3:
            reasons.append("ë‹¨ìˆœí•œ ì–¸ì–´ êµ¬ì¡°")
        elif ling > 0.7:
            reasons.append("ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡°")
            
        if vocab < 0.3:
            reasons.append("ì¼ìƒì  ì–´íœ˜")
        elif vocab > 0.7:
            reasons.append("ì–´ë ¤ìš´ ì–´íœ˜")
            
        if struct > 0.7:
            reasons.append("ëŒ€ì¹­ì  êµ¬ì¡°")
        elif struct < 0.3:
            reasons.append("ë³µì¡í•œ ë¬¸ë²•")
            
        if freq > 0.7:
            reasons.append("ìì£¼ ì‚¬ìš©ë˜ëŠ” í‘œí˜„")
        elif freq < 0.3:
            reasons.append("ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ í‘œí˜„")
        
        explanation = f"{level_name} ë‚œì´ë„ (ì ìˆ˜: {score:.2f})"
        if reasons:
            explanation += f" - {', '.join(reasons[:3])}"
            
        return explanation
    
    def _generate_hybrid_explanation(self, level: int, final_score: float, 
                                   linguistic_score: float, ai_score: float,
                                   ling: float, vocab: float, struct: float, freq: float,
                                   embedding_chars: Dict[str, float]) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„±"""
        level_names = {1: "ì‰¬ì›€", 2: "ë³´í†µ", 3: "ì–´ë ¤ì›€"}
        level_name = level_names[level]
        
        reasons = []
        
        # ì–¸ì–´í•™ì  ìš”ì¸ ë¶„ì„
        if ling < 0.3:
            reasons.append("ë‹¨ìˆœí•œ ì–¸ì–´ êµ¬ì¡°")
        elif ling > 0.7:
            reasons.append("ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡°")
            
        if vocab < 0.3:
            reasons.append("ì¼ìƒì  ì–´íœ˜")
        elif vocab > 0.7:
            reasons.append("ì–´ë ¤ìš´ ì–´íœ˜")
            
        # AI ëª¨ë¸ ë¶„ì„ ìš”ì¸
        if embedding_chars["complexity"] > 0.7:
            reasons.append("ì˜ë¯¸ì  ë³µì¡ì„± ë†’ìŒ")
        elif embedding_chars["complexity"] < 0.3:
            reasons.append("ì˜ë¯¸ì  ë‹¨ìˆœì„±")
            
        if embedding_chars["semantic_density"] > 0.7:
            reasons.append("ì˜ë¯¸ ë°€ë„ ë†’ìŒ")
        elif embedding_chars["semantic_density"] < 0.3:
            reasons.append("ì˜ë¯¸ ë°€ë„ ë‚®ìŒ")
        
        # ì£¼ìš” ë¶„ì„ ë°©ì‹ í‘œì‹œ
        dominant_analysis = "ì–¸ì–´í•™ì " if linguistic_score > ai_score else "AI ëª¨ë¸"
        
        explanation = f"{level_name} ë‚œì´ë„ (ì ìˆ˜: {final_score:.2f}, {dominant_analysis} ë¶„ì„ ìš°ì„¸)"
        if reasons:
            explanation += f" - {', '.join(reasons[:3])}"
            
        return explanation
    
    def _generate_frequency_based_explanation(self, level: int, final_score: float,
                                            usage_frequency_score: float, complexity_score: float,
                                            ai_analysis: Dict[str, float]) -> str:
        """ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬ ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„±"""
        level_names = {1: "ì‰¬ì›€", 2: "ë³´í†µ", 3: "ì–´ë ¤ì›€"}
        level_name = level_names[level]
        
        reasons = []
        
        # ì‚¬ìš© ë¹ˆë„ ìš”ì¸ ë¶„ì„
        if usage_frequency_score > 0.7:
            reasons.append("ì¼ìƒì ìœ¼ë¡œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‘œí˜„")
        elif usage_frequency_score < 0.3:
            reasons.append("ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ í‘œí˜„")
        
        # AI ë¶„ì„ ì„¸ë¶€ ìš”ì¸
        if ai_analysis["daily_context_similarity"] > 0.6:
            reasons.append("ì¼ìƒ ëŒ€í™”ì— ì¹œìˆ™í•œ íŒ¨í„´")
        elif ai_analysis["educational_similarity"] > 0.6:
            reasons.append("êµìœ¡ì  ë§¥ë½ì—ì„œ ì¹œìˆ™í•¨")
        elif ai_analysis["media_similarity"] > 0.6:
            reasons.append("ë¯¸ë””ì–´ì—ì„œ ìì£¼ ì ‘í•˜ëŠ” í˜•íƒœ")
        
        # ë³µì¡ì„± ìš”ì¸ ë¶„ì„
        if complexity_score > 0.7:
            reasons.append("ì–¸ì–´ì ìœ¼ë¡œ ë³µì¡í•œ êµ¬ì¡°")
        elif complexity_score < 0.3:
            reasons.append("ì–¸ì–´ì ìœ¼ë¡œ ë‹¨ìˆœí•œ êµ¬ì¡°")
        
        # ì£¼ìš” ë¶„ì„ ë°©ì‹ í‘œì‹œ
        dominant_factor = "ì‚¬ìš© ë¹ˆë„" if usage_frequency_score > complexity_score else "ì–¸ì–´ì  ë³µì¡ì„±"
        
        explanation = f"{level_name} ë‚œì´ë„ (ì ìˆ˜: {final_score:.2f}, {dominant_factor} ê¸°ì¤€)"
        if reasons:
            explanation += f" - {', '.join(reasons[:3])}"
            
        return explanation
    
    def _connect_database(self) -> bool:
        """
        ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("â³ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
            self.db = ProverbDatabase()
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            if not self.db.test_connection():
                print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            if not self.db.check_table_exists():
                print("âŒ proverb í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return False
            
            proverb_count = self.db.get_proverb_count()
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ! (ì†ë‹´ {proverb_count}ê°œ)")
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def analyze_proverb_difficulty(self, proverb_id: int) -> Dict[str, Any]:
        """
        ğŸ¯ ì–¸ì–´í•™ì  íŠ¹ì„± ê¸°ë°˜ ì†ë‹´ ë‚œì´ë„ ë¶„ì„ (ì™„ì „ ì¬ì„¤ê³„)
        
        ì°¸ì¡° ì†ë‹´ê³¼ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ëŒ€ì‹  ì–¸ì–´í•™ì  íŠ¹ì„±ë§Œìœ¼ë¡œ ë‚œì´ë„ë¥¼ íŒë³„í•©ë‹ˆë‹¤.
        
        Args:
            proverb_id (int): ë°ì´í„°ë² ì´ìŠ¤ ì†ë‹´ ID
        
        Returns:
            Dict: ë‚œì´ë„ ë¶„ì„ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì†ë‹´ ì¡°íšŒ
            proverb_data = self.db.get_proverb_by_id(proverb_id)
            if not proverb_data:
                return {
                    "proverb_id": proverb_id,
                    "full_proverb": "",
                    "difficulty_level": 0,
                    "confidence": 0.0,
                    "score": 0,
                    "processing_time": time.time() - start_time,
                    "message": f"ì†ë‹´ ID {proverb_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }
            
            full_proverb = proverb_data['full_proverb']
            actual_proverb_id = proverb_data['id']
            
            # 2. ìºì‹œ í™•ì¸
            cache_key = f"id_{actual_proverb_id}"
            if self.analysis_cache and cache_key in self.analysis_cache:
                cached_result = self.analysis_cache[cache_key].copy()
                cached_result["processing_time"] = time.time() - start_time
                cached_result["message"] += " (ìºì‹œë¨)"
                self.stats["cache_hits"] += 1
                return cached_result
            
            print(f"ğŸ” ì†ë‹´ ë‚œì´ë„ ë¶„ì„: '{full_proverb}'")
            
            # 3. ì–¸ì–´í•™ì  íŠ¹ì„± ê¸°ë°˜ ë‚œì´ë„ ê³„ì‚° (ìƒˆë¡œìš´ ë°©ì‹)
            analysis_result = self.calculate_final_difficulty(full_proverb)
            
            # 4. ê²°ê³¼ ì •ë¦¬
            processing_time = time.time() - start_time
            level_info = self.config.PROVERB_DIFFICULTY_LEVELS[analysis_result['difficulty_level']]
            
            result = {
                "proverb_id": actual_proverb_id,
                "full_proverb": full_proverb,
                "difficulty_level": analysis_result['difficulty_level'],
                "confidence": analysis_result['confidence'],
                "score": level_info["score"],
                "processing_time": round(processing_time, 3),
                "message": analysis_result['explanation'],
                "analysis_breakdown": analysis_result['breakdown'],
                "final_score": analysis_result['final_score']
            }
            
            # 5. ìºì‹œ ì €ì¥
            if self.analysis_cache:
                self.analysis_cache[cache_key] = result.copy()
            
            # 6. í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["total_analyzed"] += 1
            self.stats["total_processing_time"] += processing_time
            
            level_name = level_info['name']
            print(f"âœ… ë¶„ì„ ì™„ë£Œ: {level_name} (ì‹ ë¢°ë„: {analysis_result['confidence']:.1%}, {processing_time:.3f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            error_message = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(f"âŒ {error_message}")
            
            return {
                "proverb_id": proverb_id,
                "full_proverb": "",
                "difficulty_level": 0,
                "confidence": 0.0,
                "score": 0,
                "processing_time": time.time() - start_time,
                "message": error_message
            }
    
    def batch_analyze_all_proverbs(self) -> List[Dict[str, Any]]:
        """
        ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì†ë‹´ì„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
        
        16ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ í™•ë³´í•˜ê³ ,
        ì§„í–‰ë¥ ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        
        Returns:
            List[Dict]: ëª¨ë“  ì†ë‹´ì˜ ë¶„ì„ ê²°ê³¼
        """
        print("ğŸ“Š ì „ì²´ ì†ë‹´ ë°°ì¹˜ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        start_time = time.time()
        all_results = []
        
        try:
            # ì „ì²´ ì†ë‹´ ê°œìˆ˜ ì¡°íšŒ
            total_proverbs = self.db.get_proverb_count()
            if total_proverbs == 0:
                print("âŒ ë¶„ì„í•  ì†ë‹´ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            print(f"ğŸ“š ì´ {total_proverbs}ê°œ ì†ë‹´ ë¶„ì„ ì˜ˆì •")
            print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}ê°œ")
            
            # ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
            total_batches = (total_proverbs + self.batch_size - 1) // self.batch_size
            print(f"ğŸ”„ ì´ {total_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬")
            
            # ì§„í–‰ë¥  ë°” ì„¤ì •
            with tqdm(total=total_proverbs, desc="ì†ë‹´ ë¶„ì„ ì§„í–‰", unit="ê°œ") as pbar:
                
                for batch_idx in range(total_batches):
                    offset = batch_idx * self.batch_size
                    
                    # ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ
                    batch_proverbs = self.db.get_proverbs_batch(offset, self.batch_size)
                    
                    if not batch_proverbs:
                        break
                    
                    print(f"\n[ë°°ì¹˜ {batch_idx + 1}/{total_batches}] {len(batch_proverbs)}ê°œ ì†ë‹´ ì²˜ë¦¬ ì¤‘...")
                    
                    # ë°°ì¹˜ ë‚´ ê° ì†ë‹´ ë¶„ì„
                    batch_results = []
                    for proverb_data in batch_proverbs:
                        result = self.analyze_proverb_difficulty(proverb_id=proverb_data['id'])
                        batch_results.append(result)
                        pbar.update(1)
                    
                    all_results.extend(batch_results)
                    self.stats["batch_count"] += 1
                    
                    # ë°°ì¹˜ ì™„ë£Œ ì •ë³´
                    success_count = sum(1 for r in batch_results if r['difficulty_level'] > 0)
                    print(f"âœ… ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ: {success_count}/{len(batch_results)}ê°œ ì„±ê³µ")
            
            # ì „ì²´ ê²°ê³¼ í†µê³„
            total_time = time.time() - start_time
            success_results = [r for r in all_results if r['difficulty_level'] > 0]
            
            print(f"\n" + "=" * 60)
            print(f"ğŸ“ˆ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ ê²°ê³¼:")
            print(f"  - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"  - ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ ì†ë‹´: {len(success_results)}/{len(all_results)}ê°œ")
            print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(all_results):.3f}ì´ˆ/ê°œ")
            print(f"  - ìºì‹œ ì ì¤‘ë¥ : {self.stats['cache_hits']}/{self.stats['total_analyzed']} ({self.stats['cache_hits']/max(1,self.stats['total_analyzed'])*100:.1f}%)")
            
            # ë‚œì´ë„ë³„ ë¶„í¬
            self._print_difficulty_distribution(success_results)
            
            return all_results
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ“‹ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            return all_results
    
    def _print_difficulty_distribution(self, results: List[Dict[str, Any]]) -> None:
        """
        ğŸ“Š ë‚œì´ë„ ë¶„í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        
        Args:
            results: ë¶„ì„ ê²°ê³¼ ëª©ë¡
        """
        if not results:
            return
        
        # ë‚œì´ë„ë³„ ê°œìˆ˜ ê³„ì‚°
        difficulty_counts = {1: 0, 2: 0, 3: 0}
        total_score = 0
        
        for result in results:
            level = result.get('difficulty_level', 0)
            if level in difficulty_counts:
                difficulty_counts[level] += 1
                total_score += result.get('score', 0)
        
        total_count = len(results)
        
        print(f"\nğŸ“Š ë‚œì´ë„ ë¶„í¬:")
        for level, count in difficulty_counts.items():
            level_info = self.config.PROVERB_DIFFICULTY_LEVELS[level]
            percentage = (count / total_count) * 100 if total_count > 0 else 0
            print(f"  - {level_info['name']}: {count}ê°œ ({percentage:.1f}%)")
        
        avg_score = total_score / total_count if total_count > 0 else 0
        print(f"\nğŸ¯ í‰ê·  ì ìˆ˜: {avg_score:.2f}ì ")
        print(f"ğŸ† ì´ íšë“ ê°€ëŠ¥ ì ìˆ˜: {total_score}ì ")
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """
        ğŸ“ˆ ë¶„ì„ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict: í†µê³„ ì •ë³´
        """
        return {
            "total_analyzed": self.stats["total_analyzed"],
            "cache_hits": self.stats["cache_hits"],
            "cache_hit_rate": self.stats["cache_hits"] / max(1, self.stats["total_analyzed"]),
            "total_processing_time": self.stats["total_processing_time"],
            "average_processing_time": self.stats["total_processing_time"] / max(1, self.stats["total_analyzed"]),
            "batch_count": self.stats["batch_count"],
            "cache_size": len(self.analysis_cache) if self.analysis_cache else 0,
            "analysis_method": "í•˜ì´ë¸Œë¦¬ë“œ (ì–¸ì–´í•™ì  + AI ëª¨ë¸)",
            "ai_model": self.model_name
        }
    
    def clear_cache(self):
        """ğŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if self.analysis_cache:
            cache_size = len(self.analysis_cache)
            self.analysis_cache.clear()
            print(f"ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ ({cache_size}ê°œ í•­ëª© ì‚­ì œ)")
    
    def close(self):
        """ğŸ”’ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•˜ê³  ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if self.db:
            self.db.close()
        
        # ìºì‹œ ì •ë¦¬
        if self.analysis_cache:
            self.analysis_cache.clear()
        
        print("âœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…ë£Œ")


def test_difficulty_analyzer():
    """
    ğŸ§ª ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸
    """
    print("ğŸ§ª ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = ProverbDifficultyAnalyzer()
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        test_cases = [
            ("í‹°ëŒ ëª¨ì•„ íƒœì‚°", 1),                    # 5ê¸€ì, ë‹¨ìˆœêµ¬ì¡° â†’ ì‰¬ì›€
            ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", 1),    # ì¼ìƒì–´, ëŒ€ì¹­êµ¬ì¡° â†’ ì‰¬ì›€
            ("ë“±ì” ë°‘ì´ ì–´ë‘¡ë‹¤", 2),                  # ì¤‘ê°„ê¸¸ì´, ì€ìœ  â†’ ë³´í†µ
            ("ê°€ìë‹ˆ íƒœì‚°ì´ìš” ëŒì•„ì„œìë‹ˆ ìˆ­ì‚°ì´ë¼", 3) # ê¸¸ê³ ë³µì¡, ê³ ì–´ â†’ ì–´ë ¤ì›€
        ]
        
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ì„:")
        print("-" * 60)
        print(f"{'ì†ë‹´':<30} {'ì˜ˆìƒ':<4} {'ì‹¤ì œ':<4} {'ì ìˆ˜':<6} {'ì‹ ë¢°ë„':<8} {'ì„¤ëª…'}")
        print("-" * 60)
        
        correct_predictions = 0
        total_tests = len(test_cases)
        
        for proverb_text, expected_level in test_cases:
            # ì§ì ‘ ë¶„ì„ (í…ŒìŠ¤íŠ¸ìš©)
            analysis_result = analyzer.calculate_final_difficulty(proverb_text)
            actual_level = analysis_result['difficulty_level']
            confidence = analysis_result['confidence']
            score = analysis_result['final_score']
            
            # ê²°ê³¼ ì¶œë ¥
            proverb_short = proverb_text[:25] + "..." if len(proverb_text) > 25 else proverb_text
            status = "âœ…" if actual_level == expected_level else "âŒ"
            print(f"{proverb_short:<30} {expected_level:<4} {actual_level:<4} {score:<6.2f} {confidence:<8.1%} {status}")
            
            if actual_level == expected_level:
                correct_predictions += 1
        
        accuracy = correct_predictions / total_tests * 100
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {correct_predictions}/{total_tests} ì •í™• ({accuracy:.1f}%)")
        
        # 1. ê°œë³„ ì†ë‹´ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ë°ì´í„°ë² ì´ìŠ¤)
        print(f"\nğŸ” ë°ì´í„°ë² ì´ìŠ¤ ê°œë³„ ì†ë‹´ ë¶„ì„ í…ŒìŠ¤íŠ¸:")
        print("-" * 40)
        
        # IDë¡œ ë¶„ì„
        result1 = analyzer.analyze_proverb_difficulty(proverb_id=1)
        if result1['difficulty_level'] > 0:
            print(f"âœ… ID ë¶„ì„ ì„±ê³µ:")
            print(f"   ì†ë‹´: {result1['full_proverb']}")
            print(f"   ë‚œì´ë„: {result1['difficulty_level']}ë‹¨ê³„ ({result1['score']}ì )")
            print(f"   ì‹ ë¢°ë„: {result1['confidence']:.1%}")
            print(f"   ì²˜ë¦¬ì‹œê°„: {result1['processing_time']:.3f}ì´ˆ")
            print(f"   ì„¤ëª…: {result1['message']}")
        
        # ë‹¤ë¥¸ IDë¡œ ë¶„ì„
        result2 = analyzer.analyze_proverb_difficulty(proverb_id=2)
        if result2['difficulty_level'] > 0:
            print(f"\nâœ… ì¶”ê°€ ë¶„ì„ ì„±ê³µ:")
            print(f"   ì†ë‹´: {result2['full_proverb']}")
            print(f"   ë‚œì´ë„: {result2['difficulty_level']}ë‹¨ê³„ ({result2['score']}ì )")
            print(f"   ì‹ ë¢°ë„: {result2['confidence']:.1%}")
        
        # 2. ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œë§Œ)
        print(f"\nğŸ“¦ ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œ ì†ë‹´):")
        print("-" * 40)
        
        # ì„ì‹œë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ 5ë¡œ ì„¤ì •
        original_batch_size = analyzer.batch_size
        analyzer.batch_size = 5
        
        batch_results = []
        batch_proverbs = analyzer.db.get_proverbs_batch(0, 5)
        
        for proverb_data in batch_proverbs:
            result = analyzer.analyze_proverb_difficulty(proverb_id=proverb_data['id'])
            batch_results.append(result)
        
        # ê²°ê³¼ í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥
        print(f"\nğŸ“‹ ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”:")
        print(f"{'ID':<4} {'ì†ë‹´':<30} {'ë‚œì´ë„':<8} {'ì ìˆ˜':<4} {'ì‹ ë¢°ë„':<8}")
        print("-" * 60)
        
        for result in batch_results:
            if result['difficulty_level'] > 0:
                proverb_short = result['full_proverb'][:25] + "..." if len(result['full_proverb']) > 25 else result['full_proverb']
                level_info = proverb_config.PROVERB_DIFFICULTY_LEVELS[result['difficulty_level']]
                print(f"{result['proverb_id']:<4} {proverb_short:<30} {level_info['name']:<8} {result['score']:<4} {result['confidence']:.1%}")
        
        # ë°°ì¹˜ í¬ê¸° ë³µì›
        analyzer.batch_size = original_batch_size
        
        # 3. í†µê³„ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“ˆ ë¶„ì„ í†µê³„:")
        print("-" * 40)
        stats = analyzer.get_analysis_statistics()
        print(f"ì´ ë¶„ì„ íšŸìˆ˜: {stats['total_analyzed']}ê°œ")
        print(f"ìºì‹œ ì ì¤‘: {stats['cache_hits']}íšŒ ({stats['cache_hit_rate']:.1%})")
        print(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {stats['average_processing_time']:.3f}ì´ˆ")
        print(f"ìºì‹œ í¬ê¸°: {stats['cache_size']}ê°œ")
        print(f"ë¶„ì„ ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (ì–¸ì–´í•™ì  + AI ëª¨ë¸)")
        
        # 4. ì •ë¦¬
        analyzer.close()
        
        print(f"\nâœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print(f"ğŸ“‹ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
        return False


if __name__ == "__main__":
    """
    ğŸš€ ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ í˜¸ì¶œ
    
    ì‹¤í–‰ ë°©ë²•:
        python analyzer.py
    """
    print("ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ë¶„ì„ê¸°")
    print("ğŸ”¬ ë¶„ì„ ë°©ì‹: ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬ (AI ì‚¬ìš©ë¹ˆë„ 60% + ì–¸ì–´ì ë³µì¡ì„± 40%)")
    print("ğŸ“Š ì°¸ì¡° ì†ë‹´ ë°©ì‹ ì œê±° - ì„ë² ë”© ë²¡í„° íŠ¹ì„± ì§ì ‘ ë¶„ì„")
    print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: proverb_game.proverb (root/0000)")
    print()
    
    success = test_difficulty_analyzer()
    sys.exit(0 if success else 1)