"""
ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ì†ë‹´ ë‚œì´ë„ ë¶„ì„ í´ë˜ìŠ¤ (v1.2)

README v1.2ì— ë”°ë¥¸ í˜ì‹ ì ì¸ ë‚œì´ë„ ë¶„ì„ ì‹œìŠ¤í…œ:
- í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„: ì–¸ì–´í•™ì  íŠ¹ì„±(40%) + AI ëª¨ë¸ ë¶„ì„(60%)
- ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬: ì¼ìƒì  ì‚¬ìš© ë¹ˆë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ì •í™•í•œ ë‚œì´ë„ ì¸¡ì •
- íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜: ì†ë‹´ë¼ë¦¬ ë¹„êµ â†’ ì†ë‹´ vs ì¼ë°˜ í•œêµ­ì–´ ì‚¬ìš© íŒ¨í„´ ë¹„êµ
- jhgan/ko-sroberta-multitask ëª¨ë¸ í™œìš©í•œ ê³ ë„ì˜ ë‚œì´ë„ ë¶„ì„

ì£¼ìš” ê¸°ëŠ¥:
1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (proverb_game.proverb í…Œì´ë¸”)
2. AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ (ì¼ìƒ/êµìœ¡/ë¯¸ë””ì–´ ë¬¸ë§¥ê³¼ì˜ ìœ ì‚¬ë„)
3. ì–¸ì–´í•™ì  ë³µì¡ì„± ë¶„ì„ (ì–´íœ˜, êµ¬ì¡°, í†µê³„ì  íŠ¹ì„±)
4. 3ë‹¨ê³„ ì ìˆ˜ ì‹œìŠ¤í…œ (1ì /2ì /3ì )
5. ê³ ì„±ëŠ¥ ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬
6. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ì‚¬ìš© ì˜ˆì‹œ:
    analyzer = ProverbDifficultyAnalyzer()
    result = analyzer.analyze_proverb_difficulty(proverb_id=1)
    all_results = analyzer.batch_analyze_all_proverbs()
"""

import os
import sys
import time
import traceback
import logging
from typing import Dict, List, Optional, Union, Any, Tuple
from datetime import datetime
import numpy as np

# config ë° database ëª¨ë“ˆ import
try:
    import sys
    import os
    # í˜„ì¬ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ì„ sys.pathì— ì¶”ê°€
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)  # app ë””ë ‰í† ë¦¬
    root_dir = os.path.dirname(parent_dir)     # í”„ë¡œì íŠ¸ ë£¨íŠ¸
    sys.path.insert(0, root_dir)
    
    from app.core.config import proverb_config
    from app.includes.dbconn import ProverbDatabase
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("í•´ê²° ë°©ë²•: config.py, dbconn.py íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    import torch
    from sentence_transformers import SentenceTransformer
    from transformers import AutoTokenizer, AutoModel
    from tqdm import tqdm
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    print("í•´ê²° ë°©ë²•: pip install -r requirements.txt")
    sys.exit(1)


class ProverbDifficultyAnalyzer:
    """
    ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ë¶„ì„ í´ë˜ìŠ¤ (v1.2)
    
    README v1.2ì— ë”°ë¥¸ í˜ì‹ ì ì¸ ë‚œì´ë„ ë¶„ì„ ì‹œìŠ¤í…œ:
    - í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„: ì–¸ì–´í•™ì  íŠ¹ì„±(40%) + AI ëª¨ë¸ ë¶„ì„(60%)
    - ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬: ì¼ìƒì  ì‚¬ìš© ë¹ˆë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•œ ì •í™•í•œ ë‚œì´ë„ ì¸¡ì •
    - íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜: ì†ë‹´ë¼ë¦¬ ë¹„êµ â†’ ì†ë‹´ vs ì¼ë°˜ í•œêµ­ì–´ ì‚¬ìš© íŒ¨í„´ ë¹„êµ
    
    íŠ¹ì§•:
    - ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (proverb_game.proverb í…Œì´ë¸”)
    - jhgan/ko-sroberta-multitask ëª¨ë¸ ì „ìš©
    - AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ (ì¼ìƒ/êµìœ¡/ë¯¸ë””ì–´ ë¬¸ë§¥)
    - 3ë‹¨ê³„ ë‚œì´ë„ ë¶„ë¥˜ (1ì , 2ì , 3ì )
    - ê³ ì„±ëŠ¥ ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬
    """
    
    def __init__(self):
        """
        ğŸ¤– ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        1-2ë‹¨ê³„ì—ì„œ ê²€ì¦ëœ ëª¨ë¸ ë¡œë”© ì½”ë“œë¥¼ ì¬ì‚¬ìš©í•˜ê³ 
        ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.config = proverb_config
        self.model_name = self.config.MODEL_NAME
        self.device = self.config.DEVICE
        self.cache_dir = self.config.MODEL_CACHE_DIR
        self.batch_size = self.config.BATCH_SIZE_ANALYSIS
        
        # ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜
        self.sentence_model = None
        self.tokenizer = None
        self.transformer_model = None
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.db = None
        
        # ë¶„ì„ ê²°ê³¼ ìºì‹œ
        self.analysis_cache = {} if self.config.ENABLE_CACHING else None
        
        # ë‚œì´ë„ ë¶„ì„ì„ ìœ„í•œ ì°¸ì¡° ì†ë‹´ë“¤
        self.reference_proverbs = self._get_reference_proverbs()
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "total_analyzed": 0,
            "cache_hits": 0,
            "total_processing_time": 0.0,
            "batch_count": 0
        }
        
        print(f"ğŸ¯ ì†ë‹´ ê²Œì„ ë‚œì´ë„ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        print(f"ğŸ“ ëª¨ë¸: {self.model_name}")
        print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ğŸ’¾ ìºì‹±: {'í™œì„±í™”' if self.config.ENABLE_CACHING else 'ë¹„í™œì„±í™”'}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        if not self._connect_database():
            raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        # ëª¨ë¸ ë¡œë”©
        if not self._load_models():
            raise Exception("AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        
        print(f"âœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ!")
    
    def _get_reference_proverbs(self) -> Dict[int, List[str]]:
        """
        ğŸ§ª ê° ë‚œì´ë„ë³„ ì°¸ì¡° ì†ë‹´ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Returns:
            Dict[int, List[str]]: ë‚œì´ë„ë³„ ì°¸ì¡° ì†ë‹´ ëª©ë¡
        """
        reference_proverbs = {}
        for level, info in self.config.PROVERB_DIFFICULTY_LEVELS.items():
            reference_proverbs[level] = info['examples']
        return reference_proverbs
    
    def _connect_database(self) -> bool:
        """
        ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("â³ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
            self.db = ProverbDatabase()
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            if not self.db.test_connection():
                print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            if not self.db.check_table_exists():
                print("âŒ proverb í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return False
            
            proverb_count = self.db.get_proverb_count()
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ! (ì†ë‹´ {proverb_count}ê°œ)")
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _load_models(self) -> bool:
        """
        ğŸ¤– AI ëª¨ë¸ë“¤ì„ ë¡œë”©í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ëª¨ë¸ ë¡œë”© ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("â³ AI ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.cache_dir, exist_ok=True)
            
            # SentenceTransformer ëª¨ë¸ ë¡œë”©
            self.sentence_model = SentenceTransformer(
                self.model_name,
                cache_folder=self.cache_dir,
                device=self.device
            )
            
            # Transformers ëª¨ë¸ ë¡œë”© (í˜¸í™˜ì„±ìš©)
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=self.cache_dir
            )
            
            self.transformer_model = AutoModel.from_pretrained(
                self.model_name,
                cache_dir=self.cache_dir
            ).to(self.device)
            
            print("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ“‹ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            return False
    
    def analyze_embedding_characteristics(self, proverb_text: str) -> Dict[str, float]:
        """
        ğŸ§  AI ëª¨ë¸ ì„ë² ë”©ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ë‚œì´ë„ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
        
        ì°¸ì¡° ì†ë‹´ê³¼ì˜ ìœ ì‚¬ë„ ëŒ€ì‹ , ì„ë² ë”© ë²¡í„° ìì²´ì˜ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, float]: ì„ë² ë”© ê¸°ë°˜ ë‚œì´ë„ íŠ¹ì„±
        """
        try:
            # ì„ë² ë”© ìƒì„±
            embedding = self.sentence_model.encode([proverb_text], convert_to_tensor=True)[0]
            embedding_np = embedding.cpu().numpy()
            
            # ì •ê·œí™” ê³„ìˆ˜ë“¤ì„ ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ì¡°ì •
            
            # 1. ë²¡í„° í¬ê¸° (norm) - ì˜ë¯¸ì  ë°€ë„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            vector_norm = float(np.linalg.norm(embedding_np))
            # ì¼ë°˜ì ìœ¼ë¡œ 5-15 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            norm_score = max(0.0, min((vector_norm - 5.0) / 10.0, 1.0))
            
            # 2. ë²¡í„° ë¶„ì‚° - ì˜ë¯¸ì  ë³µì¡ì„± (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            variance = float(np.var(embedding_np))
            # ì¼ë°˜ì ìœ¼ë¡œ 0.01-0.1 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            variance_score = max(0.0, min((variance - 0.01) / 0.09, 1.0))
            
            # 3. ì–‘ìˆ˜/ìŒìˆ˜ ë¹„ìœ¨ - ê·¹ì„± ë¶„í¬ (ê·¸ëŒ€ë¡œ ìœ ì§€)
            positive_ratio = float(np.sum(embedding_np > 0) / len(embedding_np))
            polarity_score = abs(positive_ratio - 0.5) * 2  # 0.5ì—ì„œ ë©€ìˆ˜ë¡ ë³µì¡
            
            # 4. ì ˆëŒ“ê°’ í‰ê·  - í™œì„±í™” ê°•ë„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            abs_mean = float(np.mean(np.abs(embedding_np)))
            # ì¼ë°˜ì ìœ¼ë¡œ 0.1-0.3 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            activation_score = max(0.0, min((abs_mean - 0.1) / 0.2, 1.0))
            
            # 5. ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì˜ ì°¨ì´ - ë™ì  ë²”ìœ„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            dynamic_range = float(np.max(embedding_np) - np.min(embedding_np))
            # ì¼ë°˜ì ìœ¼ë¡œ 1-4 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            range_score = max(0.0, min((dynamic_range - 1.0) / 3.0, 1.0))
            
            # ğŸ”§ ì¶”ê°€: í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ë³´ì •
            text_length = len(proverb_text.replace(' ', ''))
            length_factor = min(text_length / 15.0, 1.0)  # ê¸¸ìˆ˜ë¡ ë³µì¡
            
            # ìµœì¢… ì ìˆ˜ì— ê¸¸ì´ ë³´ì • ì ìš©
            return {
                "semantic_density": norm_score * 0.8 + length_factor * 0.2,      # ì˜ë¯¸ì  ë°€ë„
                "complexity": variance_score * 0.7 + length_factor * 0.3,        # ë³µì¡ì„±
                "polarity": polarity_score,                                       # ê·¹ì„±
                "activation": activation_score * 0.8 + length_factor * 0.2,      # í™œì„±í™” ê°•ë„
                "dynamic_range": range_score * 0.9 + length_factor * 0.1         # ë™ì  ë²”ìœ„
            }
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© íŠ¹ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                "semantic_density": 0.5,
                "complexity": 0.5,
                "polarity": 0.5,
                "activation": 0.5,
                "dynamic_range": 0.5
            }
    
    def calculate_linguistic_complexity(self, proverb_text: str) -> float:
        """
        ğŸ“ ì–¸ì–´ì  ë³µì¡ì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
        
        Returns:
            float: ì–¸ì–´ì  ë³µì¡ì„± ì ìˆ˜
            - 0.0: ë§¤ìš° ê°„ë‹¨ (ë ˆë²¨ 1ì— ì í•©)
            - 0.5: ë³´í†µ ë³µì¡ (ë ˆë²¨ 2ì— ì í•©)
            - 1.0: ë§¤ìš° ë³µì¡ (ë ˆë²¨ 3ì— ì í•©)
        """
        try:
            clean_text = proverb_text.replace(' ', '')
            words = proverb_text.split()
            
            # 1. ê¸€ì ìˆ˜ ê¸°ë°˜ ë³µì¡ì„± (0.0 ~ 1.0)
            # 5ê¸€ì ì´í•˜: 0.0, 15ê¸€ì ì´ìƒ: 1.0
            char_complexity = min(max(len(clean_text) - 5, 0) / 10.0, 1.0)
            
            # 2. ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ë³µì¡ì„± (0.0 ~ 1.0)
            # 3ë‹¨ì–´ ì´í•˜: 0.0, 8ë‹¨ì–´ ì´ìƒ: 1.0
            word_complexity = min(max(len(words) - 3, 0) / 5.0, 1.0)
            
            # 3. í‰ê·  ë‹¨ì–´ ê¸¸ì´ ë³µì¡ì„± (0.0 ~ 1.0)
            # í‰ê·  2ê¸€ì: 0.0, í‰ê·  4ê¸€ì ì´ìƒ: 1.0
            avg_word_len = len(clean_text) / max(len(words), 1)
            word_len_complexity = min(max(avg_word_len - 2, 0) / 2.0, 1.0)
            
            # 4. ìŒì„±í•™ì  ë³µì¡ì„± (ë°›ì¹¨ì´ ë§ì„ìˆ˜ë¡ ë³µì¡)
            final_consonants = 0
            for char in clean_text:
                # í•œê¸€ ë°›ì¹¨ ê²€ì‚¬
                if 'ê°€' <= char <= 'í£':
                    char_code = ord(char) - ord('ê°€')
                    final_consonant = char_code % 28
                    if final_consonant > 0:  # ë°›ì¹¨ì´ ìˆìŒ
                        final_consonants += 1
            
            consonant_ratio = final_consonants / max(len(clean_text), 1)
            phonetic_complexity = min(consonant_ratio * 2, 1.0)
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ë³µì¡ì„± ê³„ì‚°
            final_complexity = (
                char_complexity * 0.3 +
                word_complexity * 0.3 +
                word_len_complexity * 0.2 +
                phonetic_complexity * 0.2
            )
            
            return round(min(max(final_complexity, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì–¸ì–´ì  ë³µì¡ì„± ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_vocabulary_difficulty(self, proverb_text: str) -> float:
        """
        ğŸ“š ì–´íœ˜ ë‚œì´ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: ì–´íœ˜ ë‚œì´ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì–´ë ¤ìš´ ì–´íœ˜)
        """
        try:
            # ì‰¬ìš´ ì¼ìƒ ì–´íœ˜ë“¤
            easy_vocab = [
                'ë§', 'ì‚¬ëŒ', 'ë¬¼', 'ì§‘', 'ê¸¸', 'ì†', 'ëˆˆ', 'ë§ˆìŒ', 'ëˆ', 'ë°¥',
                'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'í¬ë‹¤', 'ì‘ë‹¤',
                'ë†’ë‹¤', 'ë‚®ë‹¤', 'ë¹ ë¥´ë‹¤', 'ëŠë¦¬ë‹¤', 'ë§ë‹¤', 'ì ë‹¤', 'ìƒˆë‹¤', 'ëŠ™ë‹¤',
                'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ë³´ë‹¤', 'ë“£ë‹¤', 'ë¨¹ë‹¤', 'ìë‹¤', 'ì¼ì–´ë‚˜ë‹¤'
            ]
            
            # ë³µì¡í•œ ì–´íœ˜ íŒ¨í„´ (í•œìì–´, ê³ ì–´, ì „ë¬¸ìš©ì–´)
            complex_patterns = [
                # í•œìì–´
                'ê¸ˆê°•ì‚°', 'íƒœì‚°', 'ìˆ­ì‚°', 'ë°±ì§€ì¥', 'ì‚¬ê³µ', 'ì›ìˆ­ì´', 'ì‹í›„ê²½',
                # ê³ ì–´/ì „í†µì–´
                'ê³ ì™€ì•¼', 'ê³±ë‹¤', 'í¸ì´ë¼', 'ì €ë¦¬ë‹¤', 'ë§ë“¤ë©´', 'ë‚™ì´', 'ì„ì',
                # ë³µí•©ì–´/ì „ë¬¸ì–´
                'ë¿Œë¦¬ê¸°', 'ë–¨ì–´ì§„ë‹¤', 'ì €ë¦¬ë‹¤', 'ë§‘ì•„ì•¼', 'ì–´ë‘¡ë‹¤'
            ]
            
            # í˜„ëŒ€ì  í‘œí˜„
            modern_patterns = ['í•˜ë©´', 'ë˜ë©´', 'ìˆìœ¼ë©´', 'ì—†ìœ¼ë©´', 'ë•Œë¬¸ì—', 'ê·¸ë˜ì„œ']
            
            words = proverb_text.split()
            total_words = len(words)
            
            if total_words == 0:
                return 0.5
            
            # ì‰¬ìš´ ì–´íœ˜ ë¹„ìœ¨ ê³„ì‚°
            easy_count = sum(1 for word in words if any(easy in word for easy in easy_vocab))
            easy_ratio = easy_count / total_words
            
            # ë³µì¡í•œ ì–´íœ˜ ë¹„ìœ¨ ê³„ì‚°
            complex_count = sum(1 for pattern in complex_patterns if pattern in proverb_text)
            complex_ratio = min(complex_count / total_words, 1.0)
            
            # í˜„ëŒ€ì  í‘œí˜„ ë¹„ìœ¨
            modern_count = sum(1 for pattern in modern_patterns if pattern in proverb_text)
            modern_ratio = min(modern_count / total_words, 1.0)
            
            # í•œìì–´ ì¶”ì • (ìŒì„±í•™ì  íŠ¹ì„± ê¸°ë°˜)
            hanja_like_count = 0
            for word in words:
                if len(word) >= 2:
                    # í•œìì–´ëŠ” ë³´í†µ ëª¨ìŒ ë¹„ìœ¨ì´ ë‚®ê³  'ã…ã…“ã…—ã…œ' ê°™ì€ ê¸°ë³¸ ëª¨ìŒì´ ì ìŒ
                    basic_vowels = sum(1 for char in word if char in 'ã…ã…“ã…—ã…œã…¡ã…£')
                    if len(word) > 0 and basic_vowels / len(word) < 0.5:  # ê¸°ë³¸ ëª¨ìŒ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ í•œìì–´ ê°€ëŠ¥ì„±
                        hanja_like_count += 1
            
            hanja_ratio = hanja_like_count / max(total_words, 1)
            
            # ìµœì¢… ì–´íœ˜ ë‚œì´ë„ ê³„ì‚°
            vocab_difficulty = (
                complex_ratio * 0.4 +           # ë³µì¡í•œ ì–´íœ˜ ë¹„ìœ¨
                hanja_ratio * 0.3 +             # í•œìì–´ ì¶”ì • ë¹„ìœ¨
                (1 - easy_ratio) * 0.2 +        # ì‰¬ìš´ ì–´íœ˜ ë¶€ì¡±
                (1 - modern_ratio) * 0.1        # í˜„ëŒ€ì  í‘œí˜„ ë¶€ì¡±
            )
            
            return round(min(max(vocab_difficulty, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì–´íœ˜ ë‚œì´ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_structural_simplicity(self, proverb_text: str) -> float:
        """
        ğŸ—ï¸ êµ¬ì¡°ì  ë‹¨ìˆœì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: êµ¬ì¡°ì  ë‹¨ìˆœì„± ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë‹¨ìˆœí•¨)
        """
        try:
            # ë³µì¡í•œ ë¬¸ë²• êµ¬ì¡° ê°ì§€
            complex_grammar = [
                'ë©´', 'í•˜ë©´', 'ë‹ˆ', 'ì§€ë§Œ', 'ì–´ì•¼', 'ë©´ì„œ',  # ì¡°ê±´ë¬¸/ì—°ê²°ì–´ë¯¸
                'ì´ìš”', 'ë¼', 'ë¡œë‹¤', 'êµ¬ë‚˜', 'ì´ë¼',        # ê³ ì–´ ì¢…ê²°ì–´ë¯¸
                'ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'ë¶€í„°', 'ê¹Œì§€'       # ë³µí•© ì¡°ì‚¬
            ]
            
            # ë³µì¡í•œ êµ¬ì¡° íŒ¨í„´ ê°œìˆ˜
            complex_count = sum(1 for pattern in complex_grammar if pattern in proverb_text)
            
            # ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ë³µì¡ì„±
            words = proverb_text.split()
            length_complexity = min(len(words) / 8.0, 1.0)  # 8ë‹¨ì–´ ì´ìƒì´ë©´ ë³µì¡
            
            # ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚° (ì†ë‹´ì˜ íŠ¹ì§• - ëŒ€ì¹­ì ì¼ìˆ˜ë¡ ë‹¨ìˆœí•¨)
            symmetry_score = self._calculate_symmetry_score(proverb_text)
            
            # ë°˜ë³µ íŒ¨í„´ ì ìˆ˜ (ë°˜ë³µë ìˆ˜ë¡ ë‹¨ìˆœí•¨)
            repetition_score = self._calculate_repetition_score(proverb_text)
            
            # ìµœì¢… ë‹¨ìˆœì„± ì ìˆ˜ ê³„ì‚°
            # ë³µì¡í•œ êµ¬ì¡°ê°€ ì ê³ , ëŒ€ì¹­ì„±ê³¼ ë°˜ë³µì„±ì´ ë†’ì„ìˆ˜ë¡ ë‹¨ìˆœí•¨
            simplicity = (
                (1 - min(complex_count / 3.0, 1.0)) * 0.4 +  # ë³µì¡í•œ êµ¬ì¡° ì ìŒ
                (1 - length_complexity) * 0.3 +              # ì§§ì€ ê¸¸ì´
                symmetry_score * 0.2 +                       # ëŒ€ì¹­ì„±
                repetition_score * 0.1                       # ë°˜ë³µì„±
            )
            
            return round(min(max(simplicity, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡°ì  ë‹¨ìˆœì„± ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def _calculate_symmetry_score(self, text: str) -> float:
        """ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        words = text.split()
        
        if len(words) <= 2:
            return 0.8  # ì§§ì€ ì†ë‹´ì€ ëŒ€ì¹­ì„± ë†’ìŒ
        
        # ë‹¨ì–´ ìˆ˜ê°€ ì§ìˆ˜ì¸ì§€ (ëŒ€ì¹­ êµ¬ì¡° ê°€ëŠ¥ì„±)
        even_bonus = 0.3 if len(words) % 2 == 0 else 0.0
        
        # ì•ë’¤ ëŒ€ì¹­ êµ¬ì¡° í™•ì¸
        mid = len(words) // 2
        front_half = words[:mid]
        back_half = words[mid:] if len(words) % 2 == 0 else words[mid+1:]
        
        # êµ¬ì¡°ì  ìœ ì‚¬ì„± (ê¸¸ì´ê°€ ë¹„ìŠ·í•œê°€)
        if front_half and back_half:
            len_similarity = 1 - abs(len(front_half) - len(back_half)) / max(len(front_half), len(back_half))
            structure_bonus = len_similarity * 0.4
        else:
            structure_bonus = 0.0
        
        # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë‚˜ êµ¬ì¡°
        repeated_words = len(set(words)) < len(words)
        repeat_bonus = 0.3 if repeated_words else 0.0
        
        return min(even_bonus + structure_bonus + repeat_bonus, 1.0)
    
    def _calculate_repetition_score(self, text: str) -> float:
        """ë°˜ë³µ íŒ¨í„´ ì ìˆ˜ ê³„ì‚°"""
        words = text.split()
        
        if len(words) <= 2:
            return 0.5
        
        # ë‹¨ì–´ ë°˜ë³µ
        unique_words = len(set(words))
        repetition_ratio = 1 - (unique_words / len(words))
        
        # ìŒì„±ì  ë°˜ë³µ (ìš´ìœ¨)
        syllable_patterns = []
        for word in words:
            if len(word) > 0:
                syllable_patterns.append(word[-1])  # ë§ˆì§€ë§‰ ê¸€ì
        
        unique_endings = len(set(syllable_patterns))
        sound_repetition = 1 - (unique_endings / max(len(syllable_patterns), 1))
        
        return (repetition_ratio * 0.6 + sound_repetition * 0.4)
    
    def analyze_usage_frequency_with_ai(self, proverb_text: str) -> float:
        """
        ğŸ§  AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ (0.0 ~ 1.0)
        
        README v1.2ì— ë”°ë¥¸ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜:
        - ê¸°ì¡´: ì†ë‹´ë¼ë¦¬ ë¹„êµ â†’ ìƒˆë¡œìš´: ì†ë‹´ vs ì¼ë°˜ í•œêµ­ì–´ ì‚¬ìš© íŒ¨í„´ ë¹„êµ
        - ì¼ìƒ/êµìœ¡/ë¯¸ë””ì–´ ë¬¸ë§¥ì—ì„œì˜ ì¹œìˆ™ë„ë¥¼ AIë¡œ ì •í™•í•˜ê²Œ ë¶„ì„
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: ì‚¬ìš© ë¹ˆë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìì£¼ ì‚¬ìš©ë¨ = ì‰¬ì›€)
        """
        try:
            # AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            if not self.sentence_model:
                return 0.5
            
            # 1. ì¼ìƒ ë¬¸ë§¥ íŒ¨í„´ë“¤ (15ê°œ íŒ¨í„´)
            daily_contexts = [
                "í•­ìƒ ë§í•˜ë“¯ì´", "ëˆ„êµ¬ë‚˜ ì•„ëŠ” ì´ì•¼ê¸°", "í”íˆ ë§í•˜ëŠ”", "ìì£¼ ë“£ëŠ” ë§",
                "ì¼ìƒì—ì„œ ì“°ëŠ”", "í‰ë²”í•œ ì´ì•¼ê¸°", "ê°„ë‹¨í•œ ë§", "ì‰¬ìš´ ì´ì•¼ê¸°",
                "ë§ì´ ì“°ëŠ” ë§", "í”í•œ ì´ì•¼ê¸°", "ë³´í†µ ë§", "ì¼ë°˜ì ì¸ ì´ì•¼ê¸°",
                "ìì£¼ í•˜ëŠ” ë§", "í”íˆ í•˜ëŠ” ì´ì•¼ê¸°", "í‰ì†Œì— ë§í•˜ëŠ”"
            ]
            
            # 2. êµìœ¡ ë¬¸ë§¥ íŒ¨í„´ë“¤
            education_contexts = [
                "ì¤‘ìš”í•œ êµí›ˆ", "ì§€í˜œë¡œìš´ ë§ì”€", "ê°€ë¥´ì¹˜ëŠ” ì´ì•¼ê¸°", "ë°°ìš°ëŠ” ë‚´ìš©",
                "êµìœ¡ì ì¸ ë§", "êµí›ˆì´ ë˜ëŠ”", "í•™ìŠµí•˜ëŠ” ë‚´ìš©", "ê°€ë¥´ì¹¨ì´ ë˜ëŠ”"
            ]
            
            # 3. ë¯¸ë””ì–´ ë¬¸ë§¥ íŒ¨í„´ë“¤
            media_contexts = [
                "ë§ì€ ì‚¬ëŒë“¤ì´ ë§í•˜ëŠ”", "ë„ë¦¬ ì•Œë ¤ì§„", "ìœ ëª…í•œ ì´ì•¼ê¸°", "ì¸ê¸° ìˆëŠ” ë§",
                "í™”ì œê°€ ë˜ëŠ”", "ë§ì´ íšŒìë˜ëŠ”", "ë„ë¦¬ í¼ì§„", "ì¸ê¸° ìˆëŠ”"
            ]
            
            # 4. ì¼ë°˜ì ì¸ í•œêµ­ì–´ ë¬¸ì¥ êµ¬ì¡° íŒ¨í„´ë“¤
            general_structures = [
                "ì´ëŸ° ë§ì´ ìˆë‹¤", "ë§ë¡œëŠ” ì´ë ‡ë‹¤", "ë³´í†µ ì´ë ‡ê²Œ ë§í•œë‹¤",
                "í”íˆ í•˜ëŠ” ë§", "ìì£¼ ë“£ëŠ” ì´ì•¼ê¸°", "ë§ì´ í•˜ëŠ” ë§"
            ]
            
            # ëª¨ë“  ì°¸ì¡° íŒ¨í„´ë“¤ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ê²°í•©
            all_reference_patterns = daily_contexts + education_contexts + media_contexts + general_structures
            
            # ì†ë‹´ê³¼ ì°¸ì¡° íŒ¨í„´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            proverb_embedding = self.sentence_model.encode([proverb_text], convert_to_tensor=True)[0]
            reference_embeddings = self.sentence_model.encode(all_reference_patterns, convert_to_tensor=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for ref_embedding in reference_embeddings:
                similarity = torch.cosine_similarity(proverb_embedding.unsqueeze(0), ref_embedding.unsqueeze(0))
                similarities.append(similarity.item())
            
            # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            daily_similarity = np.mean(similarities[:len(daily_contexts)])
            education_similarity = np.mean(similarities[len(daily_contexts):len(daily_contexts)+len(education_contexts)])
            media_similarity = np.mean(similarities[len(daily_contexts)+len(education_contexts):len(daily_contexts)+len(education_contexts)+len(media_contexts)])
            structure_similarity = np.mean(similarities[len(daily_contexts)+len(education_contexts)+len(media_contexts):])
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì‚¬ìš© ë¹ˆë„ ì ìˆ˜ ê³„ì‚°
            # ì¼ìƒ ë¬¸ë§¥ì´ ê°€ì¥ ì¤‘ìš” (40%), êµìœ¡(30%), ë¯¸ë””ì–´(20%), êµ¬ì¡°(10%)
            usage_frequency_score = (
                daily_similarity * 0.4 +      # ì¼ìƒ ë¬¸ë§¥ (ê°€ì¥ ì¤‘ìš”)
                education_similarity * 0.3 +   # êµìœ¡ ë¬¸ë§¥
                media_similarity * 0.2 +       # ë¯¸ë””ì–´ ë¬¸ë§¥
                structure_similarity * 0.1     # êµ¬ì¡°ì  ì¹œìˆ™ì„±
            )
            
            # ì ìˆ˜ë¥¼ 0.0~1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
            normalized_score = max(0.0, min(usage_frequency_score, 1.0))
            
            return round(normalized_score, 3)
            
        except Exception as e:
            print(f"âš ï¸ AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_final_difficulty(self, proverb_text: str) -> Dict[str, Any]:
        """
        ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ìµœì¢… ë‚œì´ë„ ê³„ì‚° (README v1.2 ë°©ì‹)
        
        í˜ì‹ ì ì¸ ë¶„ì„ ë°©ì‹:
        - ì–¸ì–´í•™ì  íŠ¹ì„± ë¶„ì„ (40%): ë³µì¡ì„±, ì–´íœ˜, êµ¬ì¡°
        - AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ (60%): ì¼ìƒ/êµìœ¡/ë¯¸ë””ì–´ ë¬¸ë§¥ê³¼ì˜ ìœ ì‚¬ë„
        - íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜: ì†ë‹´ë¼ë¦¬ ë¹„êµ â†’ ì†ë‹´ vs ì¼ë°˜ í•œêµ­ì–´ ì‚¬ìš© íŒ¨í„´ ë¹„êµ
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼
        """
        try:
            # 1. ì–¸ì–´í•™ì  ë¶„ì„ (40%)
            linguistic_complexity = self.calculate_linguistic_complexity(proverb_text)
            vocab_difficulty = self.calculate_vocabulary_difficulty(proverb_text)
            structural_simplicity = self.calculate_structural_simplicity(proverb_text)
            
            # 2. AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ (60% ê°€ì¤‘ì¹˜ì˜ í•µì‹¬)
            usage_frequency = self.analyze_usage_frequency_with_ai(proverb_text)
            
            # ì–¸ì–´í•™ì  ì¢…í•© ì ìˆ˜ (40% ê°€ì¤‘ì¹˜)
            linguistic_score = (
                linguistic_complexity * 0.4 +        # ì–¸ì–´ì  ë³µì¡ì„±
                vocab_difficulty * 0.3 +              # ì–´íœ˜ ë‚œì´ë„
                (1 - structural_simplicity) * 0.2 +   # êµ¬ì¡°ì  ë³µì¡ì„±
                (1 - usage_frequency) * 0.1           # AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ì˜ ì—­
            )
            
            # AI ëª¨ë¸ ì„ë² ë”© ë¶„ì„ (60% ê°€ì¤‘ì¹˜ì˜ í•µì‹¬)
            embedding_characteristics = self.analyze_embedding_characteristics(proverb_text)
            
            # AI ëª¨ë¸ ì¢…í•© ì ìˆ˜ (ì‚¬ìš© ë¹ˆë„ ì¤‘ì‹¬)
            ai_score = (
                (1 - usage_frequency) * 0.5 +         # AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ (ê°€ì¥ ì¤‘ìš”)
                embedding_characteristics["complexity"] * 0.2 +      # ì˜ë¯¸ì  ë³µì¡ì„±
                embedding_characteristics["semantic_density"] * 0.15 + # ì˜ë¯¸ì  ë°€ë„
                embedding_characteristics["polarity"] * 0.1 +        # ê·¹ì„±
                embedding_characteristics["activation"] * 0.05       # í™œì„±í™” ê°•ë„
            )
            
            # 3. ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (README v1.2 ê°€ì¤‘ì¹˜: ì–¸ì–´í•™ì  40% + AI 60%)
            final_score = linguistic_score * 0.4 + ai_score * 0.6
            
            # ì ìˆ˜ë¥¼ 1-3 ë ˆë²¨ë¡œ ë³€í™˜ (ìµœì í™”ëœ ê²½ê³„ê°’)
            if final_score <= 0.45:  # 0.35 â†’ 0.45ë¡œ ìƒí–¥ ì¡°ì •
                difficulty_level = 1  # ì‰¬ì›€
                confidence = 1.0 - final_score * 1.8
            elif final_score <= 0.65:
                difficulty_level = 2  # ë³´í†µ
                confidence = 1.0 - abs(final_score - 0.55) * 2
            else:
                difficulty_level = 3  # ì–´ë ¤ì›€
                confidence = (final_score - 0.65) * 2 + 0.3
            
            confidence = min(max(confidence, 0.3), 1.0)
            
            # ì„¤ëª… ìƒì„± (README v1.2 í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „)
            explanation = self._generate_hybrid_explanation(
                difficulty_level, final_score, linguistic_score, ai_score,
                linguistic_complexity, vocab_difficulty, structural_simplicity, usage_frequency,
                embedding_characteristics
            )
            
            return {
                "difficulty_level": difficulty_level,
                "confidence": round(confidence, 3),
                "final_score": round(final_score, 3),
                "explanation": explanation,
                "breakdown": {
                    "linguistic_analysis": {
                        "linguistic_complexity": linguistic_complexity,
                        "vocabulary_difficulty": vocab_difficulty,
                        "structural_simplicity": structural_simplicity,
                        "usage_frequency": usage_frequency,
                        "linguistic_score": round(linguistic_score, 3)
                    },
                    "ai_analysis": {
                        **embedding_characteristics,
                        "ai_score": round(ai_score, 3)
                    }
                },
                "weights": {
                    "linguistic_analysis": 0.4,
                    "ai_model_analysis": 0.6
                }
            }
            
        except Exception as e:
            print(f"âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return {
                "difficulty_level": 2,
                "confidence": 0.5,
                "final_score": 0.5,
                "explanation": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "breakdown": {},
                "weights": {}
            }
    
    def _generate_hybrid_explanation(self, level: int, final_score: float, 
                                   linguistic_score: float, ai_score: float,
                                   ling: float, vocab: float, struct: float, freq: float,
                                   embedding_chars: Dict[str, float]) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„±"""
        level_names = {1: "ì‰¬ì›€", 2: "ë³´í†µ", 3: "ì–´ë ¤ì›€"}
        level_name = level_names[level]
        
        reasons = []
        
        # ì–¸ì–´í•™ì  ìš”ì¸ ë¶„ì„
        if ling < 0.3:
            reasons.append("ë‹¨ìˆœí•œ ì–¸ì–´ êµ¬ì¡°")
        elif ling > 0.7:
            reasons.append("ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡°")
            
        if vocab < 0.3:
            reasons.append("ì¼ìƒì  ì–´íœ˜")
        elif vocab > 0.7:
            reasons.append("ì–´ë ¤ìš´ ì–´íœ˜")
            
        # AI ëª¨ë¸ ë¶„ì„ ìš”ì¸
        if embedding_chars["complexity"] > 0.7:
            reasons.append("ì˜ë¯¸ì  ë³µì¡ì„± ë†’ìŒ")
        elif embedding_chars["complexity"] < 0.3:
            reasons.append("ì˜ë¯¸ì  ë‹¨ìˆœì„±")
            
        if embedding_chars["semantic_density"] > 0.7:
            reasons.append("ì˜ë¯¸ ë°€ë„ ë†’ìŒ")
        elif embedding_chars["semantic_density"] < 0.3:
            reasons.append("ì˜ë¯¸ ë°€ë„ ë‚®ìŒ")
        
        # ì£¼ìš” ë¶„ì„ ë°©ì‹ í‘œì‹œ (README v1.2 ë°©ì‹)
        dominant_analysis = "AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„" if ai_score > linguistic_score else "ì–¸ì–´í•™ì  íŠ¹ì„±"
        
        explanation = f"{level_name} ë‚œì´ë„ (ì ìˆ˜: {final_score:.2f}, {dominant_analysis} ë¶„ì„ ìš°ì„¸)"
        if reasons:
            explanation += f" - {', '.join(reasons[:3])}"
            
        return explanation
    
    def analyze_proverb_difficulty(self, proverb_id: int) -> Dict[str, Any]:
        """
        ğŸ¯ ë°ì´í„°ë² ì´ìŠ¤ ì†ë‹´ì˜ ë‚œì´ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            proverb_id (int): ë°ì´í„°ë² ì´ìŠ¤ ì†ë‹´ ID
            
        Returns:
            Dict: ë‚œì´ë„ ë¶„ì„ ê²°ê³¼
            {
                "proverb_id": 1,
                "full_proverb": "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", 
                "difficulty_level": 2,
                "confidence": 0.85,
                "score": 2,
                "processing_time": 0.15,
                "message": "ë¶„ì„ ì™„ë£Œ"
            }
        """
        start_time = time.time()
        
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì†ë‹´ ì¡°íšŒ
            proverb_data = self.db.get_proverb_by_id(proverb_id)
            if not proverb_data:
                return {
                    "proverb_id": proverb_id,
                    "full_proverb": "",
                    "difficulty_level": 0,
                    "confidence": 0.0,
                    "score": 0,
                    "processing_time": time.time() - start_time,
                    "message": f"ì†ë‹´ ID {proverb_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }
            
            full_proverb = proverb_data['full_proverb']
            actual_proverb_id = proverb_data['id']
            
            # 2. ìºì‹œ í™•ì¸
            cache_key = f"id_{actual_proverb_id}"
            
            if self.analysis_cache and cache_key in self.analysis_cache:
                cached_result = self.analysis_cache[cache_key].copy()
                cached_result["processing_time"] = time.time() - start_time
                cached_result["message"] += " (ìºì‹œë¨)"
                self.stats["cache_hits"] += 1
                return cached_result
            
            print(f"ğŸ” ì†ë‹´ ë‚œì´ë„ ë¶„ì„: '{full_proverb}'")
            
            # 3. ì–¸ì–´í•™ì  íŠ¹ì„± ê¸°ë°˜ ë‚œì´ë„ ê³„ì‚° (ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
            analysis_result = self.calculate_final_difficulty(full_proverb)
            
            # 4. ê²°ê³¼ ì •ë¦¬
            processing_time = time.time() - start_time
            level_info = self.config.PROVERB_DIFFICULTY_LEVELS[analysis_result['difficulty_level']]
            
            result = {
                "proverb_id": actual_proverb_id,
                "full_proverb": full_proverb,
                "difficulty_level": analysis_result['difficulty_level'],
                "confidence": analysis_result['confidence'],
                "score": level_info["score"],
                "processing_time": round(processing_time, 3),
                "message": analysis_result['explanation'],
                "analysis_breakdown": analysis_result['breakdown'],
                "final_score": analysis_result['final_score']
            }
            
            # 8. ìºì‹œ ì €ì¥
            if self.analysis_cache:
                self.analysis_cache[cache_key] = result.copy()
            
            # 9. í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["total_analyzed"] += 1
            self.stats["total_processing_time"] += processing_time
            
            level_name = level_info['name']
            print(f"âœ… ë¶„ì„ ì™„ë£Œ: {level_name} (ì‹ ë¢°ë„: {analysis_result['confidence']:.1%}, {processing_time:.3f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            error_message = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(f"âŒ {error_message}")
            
            return {
                "proverb_id": proverb_id,
                "full_proverb": "",
                "difficulty_level": 0,
                "confidence": 0.0,
                "score": 0,
                "processing_time": time.time() - start_time,
                "message": error_message
            }
    
    def batch_analyze_all_proverbs(self) -> List[Dict[str, Any]]:
        """
        ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì†ë‹´ì„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
        
        16ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ í™•ë³´í•˜ê³ ,
        ì§„í–‰ë¥ ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        
        Returns:
            List[Dict]: ëª¨ë“  ì†ë‹´ì˜ ë¶„ì„ ê²°ê³¼
        """
        print("ğŸ“Š ì „ì²´ ì†ë‹´ ë°°ì¹˜ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        start_time = time.time()
        all_results = []
        
        try:
            # ì „ì²´ ì†ë‹´ ê°œìˆ˜ ì¡°íšŒ
            total_proverbs = self.db.get_proverb_count()
            if total_proverbs == 0:
                print("âŒ ë¶„ì„í•  ì†ë‹´ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            print(f"ğŸ“š ì´ {total_proverbs}ê°œ ì†ë‹´ ë¶„ì„ ì˜ˆì •")
            print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}ê°œ")
            
            # ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
            total_batches = (total_proverbs + self.batch_size - 1) // self.batch_size
            print(f"ğŸ”„ ì´ {total_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬")
            
            # ì§„í–‰ë¥  ë°” ì„¤ì •
            with tqdm(total=total_proverbs, desc="ì†ë‹´ ë¶„ì„ ì§„í–‰", unit="ê°œ") as pbar:
                
                for batch_idx in range(total_batches):
                    offset = batch_idx * self.batch_size
                    
                    # ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ
                    batch_proverbs = self.db.get_proverbs_batch(offset, self.batch_size)
                    
                    if not batch_proverbs:
                        break
                    
                    print(f"\n[ë°°ì¹˜ {batch_idx + 1}/{total_batches}] {len(batch_proverbs)}ê°œ ì†ë‹´ ì²˜ë¦¬ ì¤‘...")
                    
                    # ë°°ì¹˜ ë‚´ ê° ì†ë‹´ ë¶„ì„
                    batch_results = []
                    for proverb_data in batch_proverbs:
                        result = self.analyze_proverb_difficulty(proverb_id=proverb_data['id'])
                        batch_results.append(result)
                        pbar.update(1)
                    
                    all_results.extend(batch_results)
                    self.stats["batch_count"] += 1
                    
                    # ë°°ì¹˜ ì™„ë£Œ ì •ë³´
                    success_count = sum(1 for r in batch_results if r['difficulty_level'] > 0)
                    print(f"âœ… ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ: {success_count}/{len(batch_results)}ê°œ ì„±ê³µ")
            
            # ì „ì²´ ê²°ê³¼ í†µê³„
            total_time = time.time() - start_time
            success_results = [r for r in all_results if r['difficulty_level'] > 0]
            
            print(f"\n" + "=" * 60)
            print(f"ğŸ“ˆ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ ê²°ê³¼:")
            print(f"  - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"  - ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ ì†ë‹´: {len(success_results)}/{len(all_results)}ê°œ")
            print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(all_results):.3f}ì´ˆ/ê°œ")
            print(f"  - ìºì‹œ ì ì¤‘ë¥ : {self.stats['cache_hits']}/{self.stats['total_analyzed']} ({self.stats['cache_hits']/max(1,self.stats['total_analyzed'])*100:.1f}%)")
            
            # ë‚œì´ë„ë³„ ë¶„í¬
            self._print_difficulty_distribution(success_results)
            
            return all_results
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ“‹ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            return all_results
    
    def _print_difficulty_distribution(self, results: List[Dict[str, Any]]) -> None:
        """
        ğŸ“Š ë‚œì´ë„ ë¶„í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        
        Args:
            results: ë¶„ì„ ê²°ê³¼ ëª©ë¡
        """
        if not results:
            return
        
        # ë‚œì´ë„ë³„ ê°œìˆ˜ ê³„ì‚°
        difficulty_counts = {1: 0, 2: 0, 3: 0}
        total_score = 0
        
        for result in results:
            level = result.get('difficulty_level', 0)
            if level in difficulty_counts:
                difficulty_counts[level] += 1
                total_score += result.get('score', 0)
        
        total_count = len(results)
        
        print(f"\nğŸ“Š ë‚œì´ë„ ë¶„í¬:")
        for level, count in difficulty_counts.items():
            level_info = self.config.PROVERB_DIFFICULTY_LEVELS[level]
            percentage = (count / total_count) * 100 if total_count > 0 else 0
            print(f"  - {level_info['name']}: {count}ê°œ ({percentage:.1f}%)")
        
        avg_score = total_score / total_count if total_count > 0 else 0
        print(f"\nğŸ¯ í‰ê·  ì ìˆ˜: {avg_score:.2f}ì ")
        print(f"ğŸ† ì´ íšë“ ê°€ëŠ¥ ì ìˆ˜: {total_score}ì ")
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """
        ğŸ“ˆ ë¶„ì„ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict: í†µê³„ ì •ë³´
        """
        return {
            "total_analyzed": self.stats["total_analyzed"],
            "cache_hits": self.stats["cache_hits"],
            "cache_hit_rate": self.stats["cache_hits"] / max(1, self.stats["total_analyzed"]),
            "total_processing_time": self.stats["total_processing_time"],
            "average_processing_time": self.stats["total_processing_time"] / max(1, self.stats["total_analyzed"]),
            "batch_count": self.stats["batch_count"],
            "cache_size": len(self.analysis_cache) if self.analysis_cache else 0,
            "analysis_method": "í•˜ì´ë¸Œë¦¬ë“œ (ì–¸ì–´í•™ì  + AI ëª¨ë¸)",
            "ai_model": self.model_name
        }
    
    def clear_cache(self):
        """ğŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if self.analysis_cache:
            cache_size = len(self.analysis_cache)
            self.analysis_cache.clear()
            print(f"ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ ({cache_size}ê°œ í•­ëª© ì‚­ì œ)")
    
    def close(self):
        """ğŸ”’ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•˜ê³  ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if self.db:
            self.db.close()
        
        # ìºì‹œ ì •ë¦¬
        if self.analysis_cache:
            self.analysis_cache.clear()
        
        print("âœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…ë£Œ")


def test_difficulty_analyzer():
    """
    ğŸ§ª ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸
    """
    print("ğŸ§ª ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = ProverbDifficultyAnalyzer()
        
        # 1. ê°œë³„ ì†ë‹´ ë¶„ì„ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ” ê°œë³„ ì†ë‹´ ë¶„ì„ í…ŒìŠ¤íŠ¸:")
        print("-" * 40)
        
        # IDë¡œ ë¶„ì„
        result1 = analyzer.analyze_proverb_difficulty(proverb_id=1)
        if result1['difficulty_level'] > 0:
            print(f"âœ… ID ë¶„ì„ ì„±ê³µ:")
            print(f"   ì†ë‹´: {result1['full_proverb']}")
            print(f"   ë‚œì´ë„: {result1['difficulty_level']}ë‹¨ê³„ ({result1['score']}ì )")
            print(f"   ì‹ ë¢°ë„: {result1['confidence']:.1%}")
            print(f"   ì²˜ë¦¬ì‹œê°„: {result1['processing_time']:.3f}ì´ˆ")
        
        # ë‹¤ë¥¸ IDë¡œ ë¶„ì„
        result2 = analyzer.analyze_proverb_difficulty(proverb_id=2)
        if result2['difficulty_level'] > 0:
            print(f"\nâœ… ì¶”ê°€ ë¶„ì„ ì„±ê³µ:")
            print(f"   ì†ë‹´: {result2['full_proverb']}")
            print(f"   ë‚œì´ë„: {result2['difficulty_level']}ë‹¨ê³„ ({result2['score']}ì )")
            print(f"   ì‹ ë¢°ë„: {result2['confidence']:.1%}")
        
        # 2. ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œë§Œ)
        print(f"\nğŸ“¦ ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œ ì†ë‹´):")
        print("-" * 40)
        
        # ì„ì‹œë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ 5ë¡œ ì„¤ì •
        original_batch_size = analyzer.batch_size
        analyzer.batch_size = 5
        
        batch_results = []
        batch_proverbs = analyzer.db.get_proverbs_batch(0, 5)
        
        for proverb_data in batch_proverbs:
            result = analyzer.analyze_proverb_difficulty(proverb_id=proverb_data['id'])
            batch_results.append(result)
        
        # ê²°ê³¼ í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥
        print(f"\nğŸ“‹ ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”:")
        print(f"{'ID':<4} {'ì†ë‹´':<30} {'ë‚œì´ë„':<8} {'ì ìˆ˜':<4} {'ì‹ ë¢°ë„':<8}")
        print("-" * 60)
        
        for result in batch_results:
            if result['difficulty_level'] > 0:
                proverb_short = result['full_proverb'][:25] + "..." if len(result['full_proverb']) > 25 else result['full_proverb']
                level_info = proverb_config.PROVERB_DIFFICULTY_LEVELS[result['difficulty_level']]
                print(f"{result['proverb_id']:<4} {proverb_short:<30} {level_info['name']:<8} {result['score']:<4} {result['confidence']:.1%}")
        
        # ë°°ì¹˜ í¬ê¸° ë³µì›
        analyzer.batch_size = original_batch_size
        
        # 3. í†µê³„ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“ˆ ë¶„ì„ í†µê³„:")
        print("-" * 40)
        stats = analyzer.get_analysis_statistics()
        print(f"ì´ ë¶„ì„ íšŸìˆ˜: {stats['total_analyzed']}ê°œ")
        print(f"ìºì‹œ ì ì¤‘: {stats['cache_hits']}íšŒ ({stats['cache_hit_rate']:.1%})")
        print(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {stats['average_processing_time']:.3f}ì´ˆ")
        print(f"ìºì‹œ í¬ê¸°: {stats['cache_size']}ê°œ")
        
        # 4. ì •ë¦¬
        analyzer.close()
        
        print(f"\nâœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print(f"ğŸ“‹ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
        return False


if __name__ == "__main__":
    """
    ğŸš€ ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ í˜¸ì¶œ
    
    ì‹¤í–‰ ë°©ë²•:
        python difficulty_analyzer_new.py
    """
    print("ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° (v1.2)")
    print("ğŸ”¬ ë¶„ì„ ë°©ì‹: ì–¸ì–´í•™ì  íŠ¹ì„±(40%) + AI ê¸°ë°˜ ì‚¬ìš© ë¹ˆë„ ë¶„ì„(60%)")
    print("ğŸ“Š íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜: ì†ë‹´ë¼ë¦¬ ë¹„êµ â†’ ì†ë‹´ vs ì¼ë°˜ í•œêµ­ì–´ ì‚¬ìš© íŒ¨í„´ ë¹„êµ")
    print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: proverb_game.proverb (root/0000)")
    print()
    
    success = test_difficulty_analyzer()
    sys.exit(0 if success else 1)
