"""
ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ì†ë‹´ ë‚œì´ë„ ë¶„ì„ í´ë˜ìŠ¤

ì´ ëª¨ë“ˆì€ ì–¸ì–´í•™ì  íŠ¹ì„±ê³¼ AI ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì†ë‹´ì˜ ë‚œì´ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- ì–¸ì–´í•™ì  ë¶„ì„ (60%): ë³µì¡ì„±, ì–´íœ˜, êµ¬ì¡°, ë¹ˆë„
- AI ëª¨ë¸ ë¶„ì„ (40%): models--jhgan--ko-sroberta-multitask ì„ë² ë”© íŠ¹ì„± ë¶„ì„

ì£¼ìš” ê¸°ëŠ¥:
1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (proverb_game.proverb í…Œì´ë¸”)
2. í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ë¶„ì„ (ì–¸ì–´í•™ì  + AI ëª¨ë¸)
3. models--jhgan--ko-sroberta-multitask ëª¨ë¸ ì „ìš© ì‚¬ìš©
4. ë°°ì¹˜ ì²˜ë¦¬ (16ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬)
5. ë¶„ì„ ê²°ê³¼ ìºì‹± ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
6. ì§„í–‰ë¥  í‘œì‹œ (tqdm)

ì‚¬ìš© ì˜ˆì‹œ:
    analyzer = ProverbDifficultyAnalyzer()
    result = analyzer.analyze_proverb_difficulty(proverb_id=1)
    all_results = analyzer.batch_analyze_all_proverbs()
"""

import os
import sys
import time
import traceback
import logging
import re
from typing import Dict, List, Optional, Union, Any, Tuple
from datetime import datetime

# config ë° database ëª¨ë“ˆ import
try:
    import sys
    import os
    # í˜„ì¬ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë“¤ì„ sys.pathì— ì¶”ê°€
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)  # app ë””ë ‰í† ë¦¬
    root_dir = os.path.dirname(parent_dir)     # í”„ë¡œì íŠ¸ ë£¨íŠ¸
    sys.path.insert(0, root_dir)
    
    from app.core.config import proverb_config
    from app.includes.dbconn import ProverbDatabase
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("í•´ê²° ë°©ë²•: config.py, dbconn.py íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    import torch
    from sentence_transformers import SentenceTransformer
    from transformers import AutoTokenizer, AutoModel
    from tqdm import tqdm
    import numpy as np
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
    print("í•´ê²° ë°©ë²•: pip install -r requirements.txt")
    sys.exit(1)


class ProverbDifficultyAnalyzer:
    """
    ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ë¶„ì„ í´ë˜ìŠ¤
    
    ì–¸ì–´í•™ì  íŠ¹ì„±ê³¼ AI ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì†ë‹´ì˜ ë‚œì´ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    íŠ¹ì§•:
    - ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (proverb_game.proverb í…Œì´ë¸”)
    - models--jhgan--ko-sroberta-multitask ëª¨ë¸ ì „ìš© ì‚¬ìš©
    - í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„: ì–¸ì–´í•™ì  ë¶„ì„(60%) + AI ì„ë² ë”© ë¶„ì„(40%)
    - 3ë‹¨ê³„ ë‚œì´ë„ ë¶„ë¥˜ (1ì , 2ì , 3ì )
    - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™” (16ê°œì”© ì²˜ë¦¬)
    - ë¶„ì„ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ë¶„ì„ ë°©ì§€
    """
    
    def __init__(self):
        """
        ğŸ¤– ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì´ˆê¸°í™” (í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„)
        """
        self.config = proverb_config
        self.model_name = self.config.MODEL_NAME
        self.device = self.config.DEVICE
        self.cache_dir = self.config.MODEL_CACHE_DIR
        self.batch_size = self.config.BATCH_SIZE_ANALYSIS
        
        # AI ëª¨ë¸ ì´ˆê¸°í™”
        self.sentence_model = None
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.db = None
        
        # ë¶„ì„ ê²°ê³¼ ìºì‹œ
        self.analysis_cache = {} if self.config.ENABLE_CACHING else None
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "total_analyzed": 0,
            "cache_hits": 0,
            "total_processing_time": 0.0,
            "batch_count": 0
        }
        
        print(f"ğŸ¯ ì†ë‹´ ê²Œì„ ë‚œì´ë„ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        print(f"ğŸ¤– AI ëª¨ë¸: {self.model_name}")
        print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ”¬ ë¶„ì„ ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (ì–¸ì–´í•™ì  80% + AI ëª¨ë¸ 20%)")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ğŸ’¾ ìºì‹±: {'í™œì„±í™”' if self.config.ENABLE_CACHING else 'ë¹„í™œì„±í™”'}")
        
        # AI ëª¨ë¸ ë¡œë”©
        if not self._load_ai_model():
            raise Exception("AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        if not self._connect_database():
            raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        print(f"âœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ!")
    
    def _load_ai_model(self) -> bool:
        """
        ğŸ¤– models--jhgan--ko-sroberta-multitask ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ëª¨ë¸ ë¡œë”© ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("â³ AI ëª¨ë¸ ë¡œë”© ì¤‘...")
            print(f"ğŸ“ ëª¨ë¸: {self.model_name}")
            print(f"ğŸ“ ìºì‹œ ë””ë ‰í† ë¦¬: {self.cache_dir}")
            
            # SentenceTransformer ëª¨ë¸ ë¡œë”©
            self.sentence_model = SentenceTransformer(
                self.model_name,
                cache_folder=self.cache_dir,
                device=self.device
            )
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            test_text = "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤"
            test_embedding = self.sentence_model.encode([test_text], convert_to_tensor=True)
            
            print(f"âœ… AI ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
            print(f"ğŸ“Š ì„ë² ë”© ì°¨ì›: {test_embedding.shape[1]}")
            print(f"ğŸ’» ëª¨ë¸ ë””ë°”ì´ìŠ¤: {test_embedding.device}")
            
            return True
            
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return False
    
    def analyze_embedding_characteristics(self, proverb_text: str) -> Dict[str, float]:
        """
        ğŸ§  AI ëª¨ë¸ ì„ë² ë”©ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ë‚œì´ë„ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
        
        ì°¸ì¡° ì†ë‹´ê³¼ì˜ ìœ ì‚¬ë„ ëŒ€ì‹ , ì„ë² ë”© ë²¡í„° ìì²´ì˜ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, float]: ì„ë² ë”© ê¸°ë°˜ ë‚œì´ë„ íŠ¹ì„±
        """
        try:
            # ì„ë² ë”© ìƒì„±
            embedding = self.sentence_model.encode([proverb_text], convert_to_tensor=True)[0]
            embedding_np = embedding.cpu().numpy()
            
            # ì •ê·œí™” ê³„ìˆ˜ë“¤ì„ ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ì¡°ì •
            
            # 1. ë²¡í„° í¬ê¸° (norm) - ì˜ë¯¸ì  ë°€ë„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            vector_norm = float(np.linalg.norm(embedding_np))
            # ì¼ë°˜ì ìœ¼ë¡œ 5-15 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            norm_score = max(0.0, min((vector_norm - 5.0) / 10.0, 1.0))
            
            # 2. ë²¡í„° ë¶„ì‚° - ì˜ë¯¸ì  ë³µì¡ì„± (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            variance = float(np.var(embedding_np))
            # ì¼ë°˜ì ìœ¼ë¡œ 0.01-0.1 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            variance_score = max(0.0, min((variance - 0.01) / 0.09, 1.0))
            
            # 3. ì–‘ìˆ˜/ìŒìˆ˜ ë¹„ìœ¨ - ê·¹ì„± ë¶„í¬ (ê·¸ëŒ€ë¡œ ìœ ì§€)
            positive_ratio = float(np.sum(embedding_np > 0) / len(embedding_np))
            polarity_score = abs(positive_ratio - 0.5) * 2  # 0.5ì—ì„œ ë©€ìˆ˜ë¡ ë³µì¡
            
            # 4. ì ˆëŒ“ê°’ í‰ê·  - í™œì„±í™” ê°•ë„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            abs_mean = float(np.mean(np.abs(embedding_np)))
            # ì¼ë°˜ì ìœ¼ë¡œ 0.1-0.3 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            activation_score = max(0.0, min((abs_mean - 0.1) / 0.2, 1.0))
            
            # 5. ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì˜ ì°¨ì´ - ë™ì  ë²”ìœ„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            dynamic_range = float(np.max(embedding_np) - np.min(embedding_np))
            # ì¼ë°˜ì ìœ¼ë¡œ 1-4 ë²”ìœ„ì´ë¯€ë¡œ ì¡°ì •
            range_score = max(0.0, min((dynamic_range - 1.0) / 3.0, 1.0))
            
            # ğŸ”§ ì¶”ê°€: í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ë³´ì •
            text_length = len(proverb_text.replace(' ', ''))
            length_factor = min(text_length / 15.0, 1.0)  # ê¸¸ìˆ˜ë¡ ë³µì¡
            
            # ìµœì¢… ì ìˆ˜ì— ê¸¸ì´ ë³´ì • ì ìš©
            return {
                "semantic_density": norm_score * 0.8 + length_factor * 0.2,      # ì˜ë¯¸ì  ë°€ë„
                "complexity": variance_score * 0.7 + length_factor * 0.3,        # ë³µì¡ì„±
                "polarity": polarity_score,                                       # ê·¹ì„±
                "activation": activation_score * 0.8 + length_factor * 0.2,      # í™œì„±í™” ê°•ë„
                "dynamic_range": range_score * 0.9 + length_factor * 0.1         # ë™ì  ë²”ìœ„
            }
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© íŠ¹ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                "semantic_density": 0.5,
                "complexity": 0.5,
                "polarity": 0.5,
                "activation": 0.5,
                "dynamic_range": 0.5
            }
    
    def calculate_linguistic_complexity(self, proverb_text: str) -> float:
        """
        ğŸ“ ì–¸ì–´ì  ë³µì¡ì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
        
        Returns:
            float: ì–¸ì–´ì  ë³µì¡ì„± ì ìˆ˜
            - 0.0: ë§¤ìš° ê°„ë‹¨ (ë ˆë²¨ 1ì— ì í•©)
            - 0.5: ë³´í†µ ë³µì¡ (ë ˆë²¨ 2ì— ì í•©)
            - 1.0: ë§¤ìš° ë³µì¡ (ë ˆë²¨ 3ì— ì í•©)
        """
        try:
            clean_text = proverb_text.replace(' ', '')
            words = proverb_text.split()
            
            # 1. ê¸€ì ìˆ˜ ê¸°ë°˜ ë³µì¡ì„± (0.0 ~ 1.0)
            # 5ê¸€ì ì´í•˜: 0.0, 15ê¸€ì ì´ìƒ: 1.0
            char_complexity = min(max(len(clean_text) - 5, 0) / 10.0, 1.0)
            
            # 2. ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ë³µì¡ì„± (0.0 ~ 1.0)
            # 3ë‹¨ì–´ ì´í•˜: 0.0, 8ë‹¨ì–´ ì´ìƒ: 1.0
            word_complexity = min(max(len(words) - 3, 0) / 5.0, 1.0)
            
            # 3. í‰ê·  ë‹¨ì–´ ê¸¸ì´ ë³µì¡ì„± (0.0 ~ 1.0)
            # í‰ê·  2ê¸€ì: 0.0, í‰ê·  4ê¸€ì ì´ìƒ: 1.0
            avg_word_len = len(clean_text) / max(len(words), 1)
            word_len_complexity = min(max(avg_word_len - 2, 0) / 2.0, 1.0)
            
            # 4. ìŒì„±í•™ì  ë³µì¡ì„± (ë°›ì¹¨ì´ ë§ì„ìˆ˜ë¡ ë³µì¡)
            final_consonants = 0
            for char in clean_text:
                # í•œê¸€ ë°›ì¹¨ ê²€ì‚¬
                if 'ê°€' <= char <= 'í£':
                    char_code = ord(char) - ord('ê°€')
                    final_consonant = char_code % 28
                    if final_consonant > 0:  # ë°›ì¹¨ì´ ìˆìŒ
                        final_consonants += 1
            
            consonant_ratio = final_consonants / max(len(clean_text), 1)
            phonetic_complexity = min(consonant_ratio * 2, 1.0)
            
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ë³µì¡ì„± ê³„ì‚°
            final_complexity = (
                char_complexity * 0.3 +
                word_complexity * 0.3 +
                word_len_complexity * 0.2 +
                phonetic_complexity * 0.2
            )
            
            return round(min(max(final_complexity, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì–¸ì–´ì  ë³µì¡ì„± ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_vocabulary_difficulty(self, proverb_text: str) -> float:
        """
        ğŸ“š ì–´íœ˜ ë‚œì´ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: ì–´íœ˜ ë‚œì´ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì–´ë ¤ìš´ ì–´íœ˜)
        """
        try:
            # ì‰¬ìš´ ì¼ìƒ ì–´íœ˜ë“¤
            easy_vocab = [
                'ë§', 'ì‚¬ëŒ', 'ë¬¼', 'ì§‘', 'ê¸¸', 'ì†', 'ëˆˆ', 'ë§ˆìŒ', 'ëˆ', 'ë°¥',
                'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'í¬ë‹¤', 'ì‘ë‹¤',
                'ë†’ë‹¤', 'ë‚®ë‹¤', 'ë¹ ë¥´ë‹¤', 'ëŠë¦¬ë‹¤', 'ë§ë‹¤', 'ì ë‹¤', 'ìƒˆë‹¤', 'ëŠ™ë‹¤',
                'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ë³´ë‹¤', 'ë“£ë‹¤', 'ë¨¹ë‹¤', 'ìë‹¤', 'ì¼ì–´ë‚˜ë‹¤'
            ]
            
            # ë³µì¡í•œ ì–´íœ˜ íŒ¨í„´ (í•œìì–´, ê³ ì–´, ì „ë¬¸ìš©ì–´)
            complex_patterns = [
                # í•œìì–´
                'ê¸ˆê°•ì‚°', 'íƒœì‚°', 'ìˆ­ì‚°', 'ë°±ì§€ì¥', 'ì‚¬ê³µ', 'ì›ìˆ­ì´', 'ì‹í›„ê²½',
                # ê³ ì–´/ì „í†µì–´
                'ê³ ì™€ì•¼', 'ê³±ë‹¤', 'í¸ì´ë¼', 'ì €ë¦¬ë‹¤', 'ë§ë“¤ë©´', 'ë‚™ì´', 'ì„ì',
                # ë³µí•©ì–´/ì „ë¬¸ì–´
                'ë¿Œë¦¬ê¸°', 'ë–¨ì–´ì§„ë‹¤', 'ì €ë¦¬ë‹¤', 'ë§‘ì•„ì•¼', 'ì–´ë‘¡ë‹¤'
            ]
            
            # í˜„ëŒ€ì  í‘œí˜„
            modern_patterns = ['í•˜ë©´', 'ë˜ë©´', 'ìˆìœ¼ë©´', 'ì—†ìœ¼ë©´', 'ë•Œë¬¸ì—', 'ê·¸ë˜ì„œ']
            
            words = proverb_text.split()
            total_words = len(words)
            
            if total_words == 0:
                return 0.5
            
            # ì‰¬ìš´ ì–´íœ˜ ë¹„ìœ¨ ê³„ì‚°
            easy_count = sum(1 for word in words if any(easy in word for easy in easy_vocab))
            easy_ratio = easy_count / total_words
            
            # ë³µì¡í•œ ì–´íœ˜ ë¹„ìœ¨ ê³„ì‚°
            complex_count = sum(1 for pattern in complex_patterns if pattern in proverb_text)
            complex_ratio = min(complex_count / total_words, 1.0)
            
            # í˜„ëŒ€ì  í‘œí˜„ ë¹„ìœ¨
            modern_count = sum(1 for pattern in modern_patterns if pattern in proverb_text)
            modern_ratio = min(modern_count / total_words, 1.0)
            
            # í•œìì–´ ì¶”ì • (ìŒì„±í•™ì  íŠ¹ì„± ê¸°ë°˜)
            hanja_like_count = 0
            for word in words:
                if len(word) >= 2:
                    # í•œìì–´ëŠ” ë³´í†µ ëª¨ìŒ ë¹„ìœ¨ì´ ë‚®ê³  'ã…ã…“ã…—ã…œ' ê°™ì€ ê¸°ë³¸ ëª¨ìŒì´ ì ìŒ
                    basic_vowels = sum(1 for char in word if char in 'ã…ã…“ã…—ã…œã…¡ã…£')
                    if len(word) > 0 and basic_vowels / len(word) < 0.5:  # ê¸°ë³¸ ëª¨ìŒ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ í•œìì–´ ê°€ëŠ¥ì„±
                        hanja_like_count += 1
            
            hanja_ratio = hanja_like_count / max(total_words, 1)
            
            # ìµœì¢… ì–´íœ˜ ë‚œì´ë„ ê³„ì‚°
            vocab_difficulty = (
                complex_ratio * 0.4 +           # ë³µì¡í•œ ì–´íœ˜ ë¹„ìœ¨
                hanja_ratio * 0.3 +             # í•œìì–´ ì¶”ì • ë¹„ìœ¨
                (1 - easy_ratio) * 0.2 +        # ì‰¬ìš´ ì–´íœ˜ ë¶€ì¡±
                (1 - modern_ratio) * 0.1        # í˜„ëŒ€ì  í‘œí˜„ ë¶€ì¡±
            )
            
            return round(min(max(vocab_difficulty, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì–´íœ˜ ë‚œì´ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_structural_simplicity(self, proverb_text: str) -> float:
        """
        ğŸ—ï¸ êµ¬ì¡°ì  ë‹¨ìˆœì„± ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: êµ¬ì¡°ì  ë‹¨ìˆœì„± ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë‹¨ìˆœí•¨)
        """
        try:
            # ë³µì¡í•œ ë¬¸ë²• êµ¬ì¡° ê°ì§€
            complex_grammar = [
                'ë©´', 'í•˜ë©´', 'ë‹ˆ', 'ì§€ë§Œ', 'ì–´ì•¼', 'ë©´ì„œ',  # ì¡°ê±´ë¬¸/ì—°ê²°ì–´ë¯¸
                'ì´ìš”', 'ë¼', 'ë¡œë‹¤', 'êµ¬ë‚˜', 'ì´ë¼',        # ê³ ì–´ ì¢…ê²°ì–´ë¯¸
                'ì—ì„œ', 'ìœ¼ë¡œ', 'ì—ê²Œ', 'ë¶€í„°', 'ê¹Œì§€'       # ë³µí•© ì¡°ì‚¬
            ]
            
            # ë³µì¡í•œ êµ¬ì¡° íŒ¨í„´ ê°œìˆ˜
            complex_count = sum(1 for pattern in complex_grammar if pattern in proverb_text)
            
            # ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ë³µì¡ì„±
            words = proverb_text.split()
            length_complexity = min(len(words) / 8.0, 1.0)  # 8ë‹¨ì–´ ì´ìƒì´ë©´ ë³µì¡
            
            # ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚° (ì†ë‹´ì˜ íŠ¹ì§• - ëŒ€ì¹­ì ì¼ìˆ˜ë¡ ë‹¨ìˆœí•¨)
            symmetry_score = self._calculate_symmetry_score(proverb_text)
            
            # ë°˜ë³µ íŒ¨í„´ ì ìˆ˜ (ë°˜ë³µë ìˆ˜ë¡ ë‹¨ìˆœí•¨)
            repetition_score = self._calculate_repetition_score(proverb_text)
            
            # ìµœì¢… ë‹¨ìˆœì„± ì ìˆ˜ ê³„ì‚°
            # ë³µì¡í•œ êµ¬ì¡°ê°€ ì ê³ , ëŒ€ì¹­ì„±ê³¼ ë°˜ë³µì„±ì´ ë†’ì„ìˆ˜ë¡ ë‹¨ìˆœí•¨
            simplicity = (
                (1 - min(complex_count / 3.0, 1.0)) * 0.4 +  # ë³µì¡í•œ êµ¬ì¡° ì ìŒ
                (1 - length_complexity) * 0.3 +              # ì§§ì€ ê¸¸ì´
                symmetry_score * 0.2 +                       # ëŒ€ì¹­ì„±
                repetition_score * 0.1                       # ë°˜ë³µì„±
            )
            
            return round(min(max(simplicity, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡°ì  ë‹¨ìˆœì„± ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def _calculate_symmetry_score(self, text: str) -> float:
        """ëŒ€ì¹­ì„± ì ìˆ˜ ê³„ì‚°"""
        words = text.split()
        
        if len(words) <= 2:
            return 0.8  # ì§§ì€ ì†ë‹´ì€ ëŒ€ì¹­ì„± ë†’ìŒ
        
        # ë‹¨ì–´ ìˆ˜ê°€ ì§ìˆ˜ì¸ì§€ (ëŒ€ì¹­ êµ¬ì¡° ê°€ëŠ¥ì„±)
        even_bonus = 0.3 if len(words) % 2 == 0 else 0.0
        
        # ì•ë’¤ ëŒ€ì¹­ êµ¬ì¡° í™•ì¸
        mid = len(words) // 2
        front_half = words[:mid]
        back_half = words[mid:] if len(words) % 2 == 0 else words[mid+1:]
        
        # êµ¬ì¡°ì  ìœ ì‚¬ì„± (ê¸¸ì´ê°€ ë¹„ìŠ·í•œê°€)
        if front_half and back_half:
            len_similarity = 1 - abs(len(front_half) - len(back_half)) / max(len(front_half), len(back_half))
            structure_bonus = len_similarity * 0.4
        else:
            structure_bonus = 0.0
        
        # ë°˜ë³µë˜ëŠ” ë‹¨ì–´ë‚˜ êµ¬ì¡°
        repeated_words = len(set(words)) < len(words)
        repeat_bonus = 0.3 if repeated_words else 0.0
        
        return min(even_bonus + structure_bonus + repeat_bonus, 1.0)
    
    def _calculate_repetition_score(self, text: str) -> float:
        """ë°˜ë³µ íŒ¨í„´ ì ìˆ˜ ê³„ì‚°"""
        words = text.split()
        
        if len(words) <= 2:
            return 0.5
        
        # ë‹¨ì–´ ë°˜ë³µ
        unique_words = len(set(words))
        repetition_ratio = 1 - (unique_words / len(words))
        
        # ìŒì„±ì  ë°˜ë³µ (ìš´ìœ¨)
        syllable_patterns = []
        for word in words:
            if len(word) > 0:
                syllable_patterns.append(word[-1])  # ë§ˆì§€ë§‰ ê¸€ì
        
        unique_endings = len(set(syllable_patterns))
        sound_repetition = 1 - (unique_endings / max(len(syllable_patterns), 1))
        
        return (repetition_ratio * 0.6 + sound_repetition * 0.4)
    
    def estimate_usage_frequency(self, proverb_text: str) -> float:
        """
        ğŸ“Š ì‚¬ìš© ë¹ˆë„ ì¶”ì • (0.0 ~ 1.0)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            float: ì‚¬ìš© ë¹ˆë„ ì¶”ì •ê°’ (ë†’ì„ìˆ˜ë¡ ìì£¼ ì‚¬ìš©ë¨ = ì‰¬ì›€)
        """
        try:
            clean_text = proverb_text.replace(' ', '')
            
            # 1. ê¸¸ì´ ê¸°ë°˜: ì§§ì„ìˆ˜ë¡ ìì£¼ ì‚¬ìš©
            # 5ê¸€ì ì´í•˜: 1.0, 15ê¸€ì ì´ìƒ: 0.0
            length_score = max(0.0, 1.0 - (len(clean_text) - 5) / 10.0)
            
            # 2. í˜„ëŒ€ì  ì–´íœ˜ í¬í•¨ ì—¬ë¶€
            modern_words = [
                'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë§', 'ì‚¬ëŒ', 'ì§‘', 'ë¬¼',
                'í•˜ë©´', 'ë˜ë©´', 'ë•Œë¬¸ì—', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°'
            ]
            
            modern_count = sum(1 for word in modern_words if word in proverb_text)
            modern_score = min(modern_count / 2.0, 1.0)
            
            # 3. êµìœ¡ê³¼ì • ì¶œí˜„ ë¹ˆë„ (ì´ˆë“±í•™êµì—ì„œ ë°°ìš°ëŠ” ì†ë‹´)
            elementary_patterns = [
                'ê°€ëŠ” ë§', 'ì˜¤ëŠ” ë§', 'í‹°ëŒ', 'íƒœì‚°', 'ë“±ì”', 'ë°‘', 'ì–´ë‘¡ë‹¤',
                'ë‚´ ì½”', 'ì„ ì', 'ê°€ì¬', 'ê²Œ', 'í¸ì´ë¼'
            ]
            
            elementary_count = sum(1 for pattern in elementary_patterns if pattern in proverb_text)
            education_score = min(elementary_count / 1.0, 1.0)
            
            # 4. ì¼ìƒ ìƒí™© ì—°ê´€ì„±
            daily_contexts = [
                'ë§', 'ì‚¬ëŒ', 'ì§‘', 'ë¬¼', 'ë°¥', 'ëˆ', 'ê¸¸', 'ì†', 'ëˆˆ'
            ]
            
            daily_count = sum(1 for context in daily_contexts if context in proverb_text)
            daily_score = min(daily_count / 2.0, 1.0)
            
            # ìµœì¢… ì‚¬ìš© ë¹ˆë„ ì ìˆ˜
            frequency_score = (
                length_score * 0.4 +      # ê¸¸ì´ (ê°€ì¥ ì¤‘ìš”)
                education_score * 0.3 +   # êµìœ¡ê³¼ì • ì¶œí˜„
                modern_score * 0.2 +      # í˜„ëŒ€ì  ì–´íœ˜
                daily_score * 0.1         # ì¼ìƒ ì—°ê´€ì„±
            )
            
            return round(min(max(frequency_score, 0.0), 1.0), 3)
            
        except Exception as e:
            print(f"âš ï¸ ì‚¬ìš© ë¹ˆë„ ì¶”ì • ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def calculate_final_difficulty(self, proverb_text: str) -> Dict[str, Any]:
        """
        ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ìµœì¢… ë‚œì´ë„ ê³„ì‚° (ì–¸ì–´í•™ì  ë¶„ì„ + AI ëª¨ë¸)
        
        Args:
            proverb_text: ë¶„ì„í•  ì†ë‹´ í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼
        """
        try:
            # 1. ì–¸ì–´í•™ì  ë¶„ì„ (60%)
            linguistic_complexity = self.calculate_linguistic_complexity(proverb_text)
            vocab_difficulty = self.calculate_vocabulary_difficulty(proverb_text)
            structural_simplicity = self.calculate_structural_simplicity(proverb_text)
            usage_frequency = self.estimate_usage_frequency(proverb_text)
            
            # ì–¸ì–´í•™ì  ì¢…í•© ì ìˆ˜
            linguistic_score = (
                linguistic_complexity * 0.4 +        # ì–¸ì–´ì  ë³µì¡ì„±
                vocab_difficulty * 0.3 +              # ì–´íœ˜ ë‚œì´ë„
                (1 - structural_simplicity) * 0.2 +   # êµ¬ì¡°ì  ë³µì¡ì„±
                (1 - usage_frequency) * 0.1           # ì‚¬ìš© ë¹ˆë„ì˜ ì—­
            )
            
            # 2. AI ëª¨ë¸ ì„ë² ë”© ë¶„ì„ (40%)
            embedding_characteristics = self.analyze_embedding_characteristics(proverb_text)
            
            # AI ëª¨ë¸ ì¢…í•© ì ìˆ˜
            ai_score = (
                embedding_characteristics["complexity"] * 0.3 +      # ì˜ë¯¸ì  ë³µì¡ì„±
                embedding_characteristics["semantic_density"] * 0.25 + # ì˜ë¯¸ì  ë°€ë„
                embedding_characteristics["polarity"] * 0.2 +        # ê·¹ì„±
                embedding_characteristics["activation"] * 0.15 +     # í™œì„±í™” ê°•ë„
                embedding_characteristics["dynamic_range"] * 0.1     # ë™ì  ë²”ìœ„
            )
            
            # 3. ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ìµœì í™”ëœ ê°€ì¤‘ì¹˜: ì–¸ì–´í•™ì  80% + AI 20%)
            final_score = linguistic_score * 0.8 + ai_score * 0.2
            
            # ì ìˆ˜ë¥¼ 1-3 ë ˆë²¨ë¡œ ë³€í™˜ (ìµœì í™”ëœ ê²½ê³„ê°’)
            if final_score <= 0.45:  # 0.35 â†’ 0.45ë¡œ ìƒí–¥ ì¡°ì •
                difficulty_level = 1  # ì‰¬ì›€
                confidence = 1.0 - final_score * 1.8
            elif final_score <= 0.65:
                difficulty_level = 2  # ë³´í†µ
                confidence = 1.0 - abs(final_score - 0.55) * 2
            else:
                difficulty_level = 3  # ì–´ë ¤ì›€
                confidence = (final_score - 0.65) * 2 + 0.3
            
            confidence = min(max(confidence, 0.3), 1.0)
            
            # ì„¤ëª… ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „)
            explanation = self._generate_hybrid_explanation(
                difficulty_level, final_score, linguistic_score, ai_score,
                linguistic_complexity, vocab_difficulty, structural_simplicity, usage_frequency,
                embedding_characteristics
            )
            
            return {
                "difficulty_level": difficulty_level,
                "confidence": round(confidence, 3),
                "final_score": round(final_score, 3),
                "explanation": explanation,
                "breakdown": {
                    "linguistic_analysis": {
                        "linguistic_complexity": linguistic_complexity,
                        "vocabulary_difficulty": vocab_difficulty,
                        "structural_simplicity": structural_simplicity,
                        "usage_frequency": usage_frequency,
                        "linguistic_score": round(linguistic_score, 3)
                    },
                    "ai_analysis": {
                        **embedding_characteristics,
                        "ai_score": round(ai_score, 3)
                    }
                },
                "weights": {
                    "linguistic_analysis": 0.8,
                    "ai_model_analysis": 0.2
                }
            }
            
        except Exception as e:
            print(f"âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return {
                "difficulty_level": 2,
                "confidence": 0.5,
                "final_score": 0.5,
                "explanation": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "breakdown": {},
                "weights": {}
            }
    
    def _generate_explanation(self, level: int, score: float, ling: float, 
                            vocab: float, struct: float, freq: float) -> str:
        """ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„± (ê¸°ì¡´ ë²„ì „)"""
        level_names = {1: "ì‰¬ì›€", 2: "ë³´í†µ", 3: "ì–´ë ¤ì›€"}
        level_name = level_names[level]
        
        reasons = []
        
        # ì£¼ìš” ìš”ì¸ ë¶„ì„
        if ling < 0.3:
            reasons.append("ë‹¨ìˆœí•œ ì–¸ì–´ êµ¬ì¡°")
        elif ling > 0.7:
            reasons.append("ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡°")
            
        if vocab < 0.3:
            reasons.append("ì¼ìƒì  ì–´íœ˜")
        elif vocab > 0.7:
            reasons.append("ì–´ë ¤ìš´ ì–´íœ˜")
            
        if struct > 0.7:
            reasons.append("ëŒ€ì¹­ì  êµ¬ì¡°")
        elif struct < 0.3:
            reasons.append("ë³µì¡í•œ ë¬¸ë²•")
            
        if freq > 0.7:
            reasons.append("ìì£¼ ì‚¬ìš©ë˜ëŠ” í‘œí˜„")
        elif freq < 0.3:
            reasons.append("ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ í‘œí˜„")
        
        explanation = f"{level_name} ë‚œì´ë„ (ì ìˆ˜: {score:.2f})"
        if reasons:
            explanation += f" - {', '.join(reasons[:3])}"
            
        return explanation
    
    def _generate_hybrid_explanation(self, level: int, final_score: float, 
                                   linguistic_score: float, ai_score: float,
                                   ling: float, vocab: float, struct: float, freq: float,
                                   embedding_chars: Dict[str, float]) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ ì„¤ëª… ìƒì„±"""
        level_names = {1: "ì‰¬ì›€", 2: "ë³´í†µ", 3: "ì–´ë ¤ì›€"}
        level_name = level_names[level]
        
        reasons = []
        
        # ì–¸ì–´í•™ì  ìš”ì¸ ë¶„ì„
        if ling < 0.3:
            reasons.append("ë‹¨ìˆœí•œ ì–¸ì–´ êµ¬ì¡°")
        elif ling > 0.7:
            reasons.append("ë³µì¡í•œ ì–¸ì–´ êµ¬ì¡°")
            
        if vocab < 0.3:
            reasons.append("ì¼ìƒì  ì–´íœ˜")
        elif vocab > 0.7:
            reasons.append("ì–´ë ¤ìš´ ì–´íœ˜")
            
        # AI ëª¨ë¸ ë¶„ì„ ìš”ì¸
        if embedding_chars["complexity"] > 0.7:
            reasons.append("ì˜ë¯¸ì  ë³µì¡ì„± ë†’ìŒ")
        elif embedding_chars["complexity"] < 0.3:
            reasons.append("ì˜ë¯¸ì  ë‹¨ìˆœì„±")
            
        if embedding_chars["semantic_density"] > 0.7:
            reasons.append("ì˜ë¯¸ ë°€ë„ ë†’ìŒ")
        elif embedding_chars["semantic_density"] < 0.3:
            reasons.append("ì˜ë¯¸ ë°€ë„ ë‚®ìŒ")
        
        # ì£¼ìš” ë¶„ì„ ë°©ì‹ í‘œì‹œ
        dominant_analysis = "ì–¸ì–´í•™ì " if linguistic_score > ai_score else "AI ëª¨ë¸"
        
        explanation = f"{level_name} ë‚œì´ë„ (ì ìˆ˜: {final_score:.2f}, {dominant_analysis} ë¶„ì„ ìš°ì„¸)"
        if reasons:
            explanation += f" - {', '.join(reasons[:3])}"
            
        return explanation
    
    def _connect_database(self) -> bool:
        """
        ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("â³ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
            self.db = ProverbDatabase()
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            if not self.db.test_connection():
                print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            if not self.db.check_table_exists():
                print("âŒ proverb í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return False
            
            proverb_count = self.db.get_proverb_count()
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ! (ì†ë‹´ {proverb_count}ê°œ)")
            return True
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def analyze_proverb_difficulty(self, proverb_id: int) -> Dict[str, Any]:
        """
        ğŸ¯ ì–¸ì–´í•™ì  íŠ¹ì„± ê¸°ë°˜ ì†ë‹´ ë‚œì´ë„ ë¶„ì„ (ì™„ì „ ì¬ì„¤ê³„)
        
        ì°¸ì¡° ì†ë‹´ê³¼ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ëŒ€ì‹  ì–¸ì–´í•™ì  íŠ¹ì„±ë§Œìœ¼ë¡œ ë‚œì´ë„ë¥¼ íŒë³„í•©ë‹ˆë‹¤.
        
        Args:
            proverb_id (int): ë°ì´í„°ë² ì´ìŠ¤ ì†ë‹´ ID
        
        Returns:
            Dict: ë‚œì´ë„ ë¶„ì„ ê²°ê³¼
        """
        start_time = time.time()
        
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì†ë‹´ ì¡°íšŒ
            proverb_data = self.db.get_proverb_by_id(proverb_id)
            if not proverb_data:
                return {
                    "proverb_id": proverb_id,
                    "full_proverb": "",
                    "difficulty_level": 0,
                    "confidence": 0.0,
                    "score": 0,
                    "processing_time": time.time() - start_time,
                    "message": f"ì†ë‹´ ID {proverb_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }
            
            full_proverb = proverb_data['full_proverb']
            actual_proverb_id = proverb_data['id']
            
            # 2. ìºì‹œ í™•ì¸
            cache_key = f"id_{actual_proverb_id}"
            if self.analysis_cache and cache_key in self.analysis_cache:
                cached_result = self.analysis_cache[cache_key].copy()
                cached_result["processing_time"] = time.time() - start_time
                cached_result["message"] += " (ìºì‹œë¨)"
                self.stats["cache_hits"] += 1
                return cached_result
            
            print(f"ğŸ” ì†ë‹´ ë‚œì´ë„ ë¶„ì„: '{full_proverb}'")
            
            # 3. ì–¸ì–´í•™ì  íŠ¹ì„± ê¸°ë°˜ ë‚œì´ë„ ê³„ì‚° (ìƒˆë¡œìš´ ë°©ì‹)
            analysis_result = self.calculate_final_difficulty(full_proverb)
            
            # 4. ê²°ê³¼ ì •ë¦¬
            processing_time = time.time() - start_time
            level_info = self.config.PROVERB_DIFFICULTY_LEVELS[analysis_result['difficulty_level']]
            
            result = {
                "proverb_id": actual_proverb_id,
                "full_proverb": full_proverb,
                "difficulty_level": analysis_result['difficulty_level'],
                "confidence": analysis_result['confidence'],
                "score": level_info["score"],
                "processing_time": round(processing_time, 3),
                "message": analysis_result['explanation'],
                "analysis_breakdown": analysis_result['breakdown'],
                "final_score": analysis_result['final_score']
            }
            
            # 5. ìºì‹œ ì €ì¥
            if self.analysis_cache:
                self.analysis_cache[cache_key] = result.copy()
            
            # 6. í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["total_analyzed"] += 1
            self.stats["total_processing_time"] += processing_time
            
            level_name = level_info['name']
            print(f"âœ… ë¶„ì„ ì™„ë£Œ: {level_name} (ì‹ ë¢°ë„: {analysis_result['confidence']:.1%}, {processing_time:.3f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            error_message = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(f"âŒ {error_message}")
            
            return {
                "proverb_id": proverb_id,
                "full_proverb": "",
                "difficulty_level": 0,
                "confidence": 0.0,
                "score": 0,
                "processing_time": time.time() - start_time,
                "message": error_message
            }
    
    def batch_analyze_all_proverbs(self) -> List[Dict[str, Any]]:
        """
        ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ì†ë‹´ì„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
        
        16ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ í™•ë³´í•˜ê³ ,
        ì§„í–‰ë¥ ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        
        Returns:
            List[Dict]: ëª¨ë“  ì†ë‹´ì˜ ë¶„ì„ ê²°ê³¼
        """
        print("ğŸ“Š ì „ì²´ ì†ë‹´ ë°°ì¹˜ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        start_time = time.time()
        all_results = []
        
        try:
            # ì „ì²´ ì†ë‹´ ê°œìˆ˜ ì¡°íšŒ
            total_proverbs = self.db.get_proverb_count()
            if total_proverbs == 0:
                print("âŒ ë¶„ì„í•  ì†ë‹´ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            print(f"ğŸ“š ì´ {total_proverbs}ê°œ ì†ë‹´ ë¶„ì„ ì˜ˆì •")
            print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}ê°œ")
            
            # ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
            total_batches = (total_proverbs + self.batch_size - 1) // self.batch_size
            print(f"ğŸ”„ ì´ {total_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬")
            
            # ì§„í–‰ë¥  ë°” ì„¤ì •
            with tqdm(total=total_proverbs, desc="ì†ë‹´ ë¶„ì„ ì§„í–‰", unit="ê°œ") as pbar:
                
                for batch_idx in range(total_batches):
                    offset = batch_idx * self.batch_size
                    
                    # ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ
                    batch_proverbs = self.db.get_proverbs_batch(offset, self.batch_size)
                    
                    if not batch_proverbs:
                        break
                    
                    print(f"\n[ë°°ì¹˜ {batch_idx + 1}/{total_batches}] {len(batch_proverbs)}ê°œ ì†ë‹´ ì²˜ë¦¬ ì¤‘...")
                    
                    # ë°°ì¹˜ ë‚´ ê° ì†ë‹´ ë¶„ì„
                    batch_results = []
                    for proverb_data in batch_proverbs:
                        result = self.analyze_proverb_difficulty(proverb_id=proverb_data['id'])
                        batch_results.append(result)
                        pbar.update(1)
                    
                    all_results.extend(batch_results)
                    self.stats["batch_count"] += 1
                    
                    # ë°°ì¹˜ ì™„ë£Œ ì •ë³´
                    success_count = sum(1 for r in batch_results if r['difficulty_level'] > 0)
                    print(f"âœ… ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ: {success_count}/{len(batch_results)}ê°œ ì„±ê³µ")
            
            # ì „ì²´ ê²°ê³¼ í†µê³„
            total_time = time.time() - start_time
            success_results = [r for r in all_results if r['difficulty_level'] > 0]
            
            print(f"\n" + "=" * 60)
            print(f"ğŸ“ˆ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ ê²°ê³¼:")
            print(f"  - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"  - ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ëœ ì†ë‹´: {len(success_results)}/{len(all_results)}ê°œ")
            print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(all_results):.3f}ì´ˆ/ê°œ")
            print(f"  - ìºì‹œ ì ì¤‘ë¥ : {self.stats['cache_hits']}/{self.stats['total_analyzed']} ({self.stats['cache_hits']/max(1,self.stats['total_analyzed'])*100:.1f}%)")
            
            # ë‚œì´ë„ë³„ ë¶„í¬
            self._print_difficulty_distribution(success_results)
            
            return all_results
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ“‹ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            return all_results
    
    def _print_difficulty_distribution(self, results: List[Dict[str, Any]]) -> None:
        """
        ğŸ“Š ë‚œì´ë„ ë¶„í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        
        Args:
            results: ë¶„ì„ ê²°ê³¼ ëª©ë¡
        """
        if not results:
            return
        
        # ë‚œì´ë„ë³„ ê°œìˆ˜ ê³„ì‚°
        difficulty_counts = {1: 0, 2: 0, 3: 0}
        total_score = 0
        
        for result in results:
            level = result.get('difficulty_level', 0)
            if level in difficulty_counts:
                difficulty_counts[level] += 1
                total_score += result.get('score', 0)
        
        total_count = len(results)
        
        print(f"\nğŸ“Š ë‚œì´ë„ ë¶„í¬:")
        for level, count in difficulty_counts.items():
            level_info = self.config.PROVERB_DIFFICULTY_LEVELS[level]
            percentage = (count / total_count) * 100 if total_count > 0 else 0
            print(f"  - {level_info['name']}: {count}ê°œ ({percentage:.1f}%)")
        
        avg_score = total_score / total_count if total_count > 0 else 0
        print(f"\nğŸ¯ í‰ê·  ì ìˆ˜: {avg_score:.2f}ì ")
        print(f"ğŸ† ì´ íšë“ ê°€ëŠ¥ ì ìˆ˜: {total_score}ì ")
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """
        ğŸ“ˆ ë¶„ì„ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict: í†µê³„ ì •ë³´
        """
        return {
            "total_analyzed": self.stats["total_analyzed"],
            "cache_hits": self.stats["cache_hits"],
            "cache_hit_rate": self.stats["cache_hits"] / max(1, self.stats["total_analyzed"]),
            "total_processing_time": self.stats["total_processing_time"],
            "average_processing_time": self.stats["total_processing_time"] / max(1, self.stats["total_analyzed"]),
            "batch_count": self.stats["batch_count"],
            "cache_size": len(self.analysis_cache) if self.analysis_cache else 0,
            "analysis_method": "í•˜ì´ë¸Œë¦¬ë“œ (ì–¸ì–´í•™ì  + AI ëª¨ë¸)",
            "ai_model": self.model_name
        }
    
    def clear_cache(self):
        """ğŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if self.analysis_cache:
            cache_size = len(self.analysis_cache)
            self.analysis_cache.clear()
            print(f"ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ ({cache_size}ê°œ í•­ëª© ì‚­ì œ)")
    
    def close(self):
        """ğŸ”’ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•˜ê³  ì—°ê²°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if self.db:
            self.db.close()
        
        # ìºì‹œ ì •ë¦¬
        if self.analysis_cache:
            self.analysis_cache.clear()
        
        print("âœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…ë£Œ")


def test_difficulty_analyzer():
    """
    ğŸ§ª ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸
    """
    print("ğŸ§ª ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = ProverbDifficultyAnalyzer()
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        test_cases = [
            ("í‹°ëŒ ëª¨ì•„ íƒœì‚°", 1),                    # 5ê¸€ì, ë‹¨ìˆœêµ¬ì¡° â†’ ì‰¬ì›€
            ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", 1),    # ì¼ìƒì–´, ëŒ€ì¹­êµ¬ì¡° â†’ ì‰¬ì›€
            ("ë“±ì” ë°‘ì´ ì–´ë‘¡ë‹¤", 2),                  # ì¤‘ê°„ê¸¸ì´, ì€ìœ  â†’ ë³´í†µ
            ("ê°€ìë‹ˆ íƒœì‚°ì´ìš” ëŒì•„ì„œìë‹ˆ ìˆ­ì‚°ì´ë¼", 3) # ê¸¸ê³ ë³µì¡, ê³ ì–´ â†’ ì–´ë ¤ì›€
        ]
        
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ì„:")
        print("-" * 60)
        print(f"{'ì†ë‹´':<30} {'ì˜ˆìƒ':<4} {'ì‹¤ì œ':<4} {'ì ìˆ˜':<6} {'ì‹ ë¢°ë„':<8} {'ì„¤ëª…'}")
        print("-" * 60)
        
        correct_predictions = 0
        total_tests = len(test_cases)
        
        for proverb_text, expected_level in test_cases:
            # ì§ì ‘ ë¶„ì„ (í…ŒìŠ¤íŠ¸ìš©)
            analysis_result = analyzer.calculate_final_difficulty(proverb_text)
            actual_level = analysis_result['difficulty_level']
            confidence = analysis_result['confidence']
            score = analysis_result['final_score']
            
            # ê²°ê³¼ ì¶œë ¥
            proverb_short = proverb_text[:25] + "..." if len(proverb_text) > 25 else proverb_text
            status = "âœ…" if actual_level == expected_level else "âŒ"
            print(f"{proverb_short:<30} {expected_level:<4} {actual_level:<4} {score:<6.2f} {confidence:<8.1%} {status}")
            
            if actual_level == expected_level:
                correct_predictions += 1
        
        accuracy = correct_predictions / total_tests * 100
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {correct_predictions}/{total_tests} ì •í™• ({accuracy:.1f}%)")
        
        # 1. ê°œë³„ ì†ë‹´ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ë°ì´í„°ë² ì´ìŠ¤)
        print(f"\nğŸ” ë°ì´í„°ë² ì´ìŠ¤ ê°œë³„ ì†ë‹´ ë¶„ì„ í…ŒìŠ¤íŠ¸:")
        print("-" * 40)
        
        # IDë¡œ ë¶„ì„
        result1 = analyzer.analyze_proverb_difficulty(proverb_id=1)
        if result1['difficulty_level'] > 0:
            print(f"âœ… ID ë¶„ì„ ì„±ê³µ:")
            print(f"   ì†ë‹´: {result1['full_proverb']}")
            print(f"   ë‚œì´ë„: {result1['difficulty_level']}ë‹¨ê³„ ({result1['score']}ì )")
            print(f"   ì‹ ë¢°ë„: {result1['confidence']:.1%}")
            print(f"   ì²˜ë¦¬ì‹œê°„: {result1['processing_time']:.3f}ì´ˆ")
            print(f"   ì„¤ëª…: {result1['message']}")
        
        # ë‹¤ë¥¸ IDë¡œ ë¶„ì„
        result2 = analyzer.analyze_proverb_difficulty(proverb_id=2)
        if result2['difficulty_level'] > 0:
            print(f"\nâœ… ì¶”ê°€ ë¶„ì„ ì„±ê³µ:")
            print(f"   ì†ë‹´: {result2['full_proverb']}")
            print(f"   ë‚œì´ë„: {result2['difficulty_level']}ë‹¨ê³„ ({result2['score']}ì )")
            print(f"   ì‹ ë¢°ë„: {result2['confidence']:.1%}")
        
        # 2. ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œë§Œ)
        print(f"\nğŸ“¦ ë°°ì¹˜ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œ ì†ë‹´):")
        print("-" * 40)
        
        # ì„ì‹œë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ 5ë¡œ ì„¤ì •
        original_batch_size = analyzer.batch_size
        analyzer.batch_size = 5
        
        batch_results = []
        batch_proverbs = analyzer.db.get_proverbs_batch(0, 5)
        
        for proverb_data in batch_proverbs:
            result = analyzer.analyze_proverb_difficulty(proverb_id=proverb_data['id'])
            batch_results.append(result)
        
        # ê²°ê³¼ í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥
        print(f"\nğŸ“‹ ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”:")
        print(f"{'ID':<4} {'ì†ë‹´':<30} {'ë‚œì´ë„':<8} {'ì ìˆ˜':<4} {'ì‹ ë¢°ë„':<8}")
        print("-" * 60)
        
        for result in batch_results:
            if result['difficulty_level'] > 0:
                proverb_short = result['full_proverb'][:25] + "..." if len(result['full_proverb']) > 25 else result['full_proverb']
                level_info = proverb_config.PROVERB_DIFFICULTY_LEVELS[result['difficulty_level']]
                print(f"{result['proverb_id']:<4} {proverb_short:<30} {level_info['name']:<8} {result['score']:<4} {result['confidence']:.1%}")
        
        # ë°°ì¹˜ í¬ê¸° ë³µì›
        analyzer.batch_size = original_batch_size
        
        # 3. í†µê³„ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“ˆ ë¶„ì„ í†µê³„:")
        print("-" * 40)
        stats = analyzer.get_analysis_statistics()
        print(f"ì´ ë¶„ì„ íšŸìˆ˜: {stats['total_analyzed']}ê°œ")
        print(f"ìºì‹œ ì ì¤‘: {stats['cache_hits']}íšŒ ({stats['cache_hit_rate']:.1%})")
        print(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {stats['average_processing_time']:.3f}ì´ˆ")
        print(f"ìºì‹œ í¬ê¸°: {stats['cache_size']}ê°œ")
        print(f"ë¶„ì„ ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (ì–¸ì–´í•™ì  + AI ëª¨ë¸)")
        
        # 4. ì •ë¦¬
        analyzer.close()
        
        print(f"\nâœ… ì†ë‹´ ë‚œì´ë„ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print(f"ğŸ“‹ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
        return False


if __name__ == "__main__":
    """
    ğŸš€ ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ í˜¸ì¶œ
    
    ì‹¤í–‰ ë°©ë²•:
        python analyzer.py
    """
    print("ğŸ¯ ì†ë‹´ ê²Œì„ - í•˜ì´ë¸Œë¦¬ë“œ ë‚œì´ë„ ë¶„ì„ê¸°")
    print("ğŸ”¬ ë¶„ì„ ë°©ì‹: ì–¸ì–´í•™ì  ë¶„ì„(80%) + models--jhgan--ko-sroberta-multitask ëª¨ë¸(20%)")
    print("ğŸ“Š ì°¸ì¡° ì†ë‹´ ë°©ì‹ ì œê±° - ì„ë² ë”© ë²¡í„° íŠ¹ì„± ì§ì ‘ ë¶„ì„")
    print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: proverb_game.proverb (root/0000)")
    print()
    
    success = test_difficulty_analyzer()
    sys.exit(0 if success else 1)