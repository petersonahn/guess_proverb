#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ§  AI í•™ìŠµ ê¸°ë°˜ ì§€ëŠ¥ì  ì†ë‹´ ìœ ì‚¬ë„ ê²€ì‚¬ ì‹œìŠ¤í…œ

í•˜ë“œì½”ë”©ì„ ìµœëŒ€í•œ ì¤„ì´ê³  proverb í…Œì´ë¸”ì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬
ì§€ëŠ¥ì ìœ¼ë¡œ ì˜ˆì™¸ì‚¬í•­ì„ ê°ì§€í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
- ë°ì´í„° ê¸°ë°˜ íŒ¨í„´ í•™ìŠµ
- ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ìë™ ê°ì§€
- ë™ë¬¼ ë¶„ë¥˜ ë° ì˜ë¯¸ êµ¬ë¶„ ìë™í™”
- ë¶€ì • í‘œí˜„ ë³€í™” ê°ì§€ ìë™í™”
- ë¬¸ë§¥ì  ê´€ë ¨ì„± ìë™ ë¶„ì„
"""

import mysql.connector
from sentence_transformers import SentenceTransformer, util
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import json
from typing import List, Dict, Tuple, Set
from functools import lru_cache
import logging
from collections import defaultdict, Counter
import pickle
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProverbDataLearner:
    """ì†ë‹´ ë°ì´í„° í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, db_config: Dict[str, str] = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            db_config: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
        """
        self.db_config = db_config or {
            'host': 'localhost',
            'user': 'root',
            'password': 'root1234',
            'database': 'proverb_game'
        }
        
        # í•™ìŠµëœ ë°ì´í„° ì €ì¥
        self.learned_patterns = {
            'animals': set(),
            'negation_words': set(),
            'opposite_pairs': set(),
            'context_groups': defaultdict(list),
            'semantic_clusters': {},
            'critical_words': defaultdict(float),
            'grammar_patterns': defaultdict(list),  # ë¬¸ë²•ì  ë³€í™” íŒ¨í„´
            'context_patterns': defaultdict(list),  # ë¬¸ë§¥ì  ì¼ê´€ì„± íŒ¨í„´
            'semantic_patterns': defaultdict(list),  # ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒ¨í„´
            'phonetic_patterns': defaultdict(list),  # ìŒì„±í•™ì  ìœ ì‚¬ì„± íŒ¨í„´
            'structural_patterns': defaultdict(list),  # êµ¬ì¡°ì  íŒ¨í„´
            'temporal_patterns': set(),  # ì‹œê°„ ê´€ë ¨ íŒ¨í„´
            'spatial_patterns': set(),  # ê³µê°„ ê´€ë ¨ íŒ¨í„´
            'emotional_patterns': set(),  # ê°ì • ê´€ë ¨ íŒ¨í„´
            'action_patterns': set(),  # í–‰ë™ ê´€ë ¨ íŒ¨í„´
            'object_patterns': set(),  # ì‚¬ë¬¼ ê´€ë ¨ íŒ¨í„´
            'quality_patterns': set(),  # ì„±ì§ˆ ê´€ë ¨ íŒ¨í„´
        }
        
        # ëª¨ë¸ë“¤
        self.bert_model = None
        self.tfidf_vectorizer = None
        self.semantic_clusters = None
        
        # í•™ìŠµ ë°ì´í„°
        self.proverb_data = []
        self.answer_embeddings = None
        
    def get_db_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
        try:
            return mysql.connector.connect(**self.db_config)
        except mysql.connector.Error as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def load_proverb_data(self) -> List[Dict]:
        """
        proverb í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë¡œë“œ
        
        Returns:
            List[Dict]: ì†ë‹´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        conn = self.get_db_connection()
        if not conn:
            return []
        
        try:
            cursor = conn.cursor(dictionary=True)
            cursor.execute("""
                SELECT id, question, answer, hint 
                FROM proverb 
                ORDER BY id
            """)
            
            data = cursor.fetchall()
            self.proverb_data = data
            logger.info(f"âœ… {len(data)}ê°œì˜ ì†ë‹´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return data
            
        except mysql.connector.Error as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
        finally:
            if conn:
                cursor.close()
                conn.close()
    
    def extract_animals_from_data(self) -> Set[str]:
        """
        ì‹¤ì œ ë°ì´í„°ì—ì„œ ë™ë¬¼ ë‹¨ì–´ë“¤ì„ ìë™ ì¶”ì¶œ
        
        Returns:
            Set[str]: ì¶”ì¶œëœ ë™ë¬¼ ë‹¨ì–´ë“¤
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        # ë™ë¬¼ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´ (í™•ì¥ ê°€ëŠ¥)
        animal_patterns = [
            r'ê°œ\w*', r'ê²Œ\w*', r'ì†Œ\w*', r'ë§\w*', r'ê³ ì–‘ì´\w*', r'í˜¸ë‘ì´\w*',
            r'ì›ìˆ­ì´\w*', r'ê°œêµ¬ë¦¬\w*', r'ì˜¬ì±™ì´\w*', r'ìƒˆìš°\w*', r'ê³ ë˜\w*',
            r'ë‹­\w*', r'ë±ìƒˆ\w*', r'í™©ìƒˆ\w*', r'ë²”\w*', r'ë‹¤ëŒì¥\w*', r'ë§ë‘¥ì´\w*',
            r'ê¼´ëšœê¸°\w*', r'ì¥\w*', r'ìƒˆ\w*', r'ìš©\w*', r'ë²¼ë£©\w*', r'ë¼ì§€\w*'
        ]
        
        animals = set()
        
        for proverb in self.proverb_data:
            text = proverb['answer'] + ' ' + proverb['question']
            
            for pattern in animal_patterns:
                matches = re.findall(pattern, text)
                animals.update(matches)
        
        # ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ë™ë¬¼ë“¤ ê·¸ë£¹í™”
        animal_groups = {
            'ê°‘ê°ë¥˜': ['ê²Œ', 'ìƒˆìš°', 'ê¼´ëšœê¸°', 'ë§ë‘¥ì´'],
            'í¬ìœ ë¥˜': ['ê°œ', 'ì†Œ', 'ë§', 'ê³ ì–‘ì´', 'í˜¸ë‘ì´', 'ì›ìˆ­ì´', 'ê°œêµ¬ë¦¬', 'ê³ ë˜', 'ë‹­', 'ë‹¤ëŒì¥', 'ë¼ì§€'],
            'ì¡°ë¥˜': ['ìƒˆ', 'ë±ìƒˆ', 'í™©ìƒˆ'],
            'ê¸°íƒ€': ['ìš©', 'ë²¼ë£©', 'ì¥', 'ì˜¬ì±™ì´']
        }
        
        self.learned_patterns['animals'] = animals
        self.learned_patterns['animal_groups'] = animal_groups
        
        logger.info(f"âœ… {len(animals)}ê°œì˜ ë™ë¬¼ ë‹¨ì–´ ì¶”ì¶œ ì™„ë£Œ")
        return animals
    
    def extract_negation_patterns(self) -> Set[str]:
        """
        ì‹¤ì œ ë°ì´í„°ì—ì„œ ë¶€ì • í‘œí˜„ íŒ¨í„´ë“¤ì„ ìë™ ì¶”ì¶œ
        
        Returns:
            Set[str]: ì¶”ì¶œëœ ë¶€ì • í‘œí˜„ë“¤
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        negation_patterns = set()
        
        for proverb in self.proverb_data:
            text = proverb['answer'] + ' ' + proverb['question']
            
            # ë¶€ì • í‘œí˜„ íŒ¨í„´ ë§¤ì¹­
            neg_patterns = [
                r'ì•ˆ\s*\w+', r'ì•Š\w*', r'ëª»\s*\w+', r'ì—†\w*', r'ì•„ë‹ˆ\w*',
                r'ì•„ë‹Œ\w*', r'ì•ˆ\w*', r'ëª»\w*'
            ]
            
            for pattern in neg_patterns:
                matches = re.findall(pattern, text)
                negation_patterns.update(matches)
        
        self.learned_patterns['negation_words'] = negation_patterns
        logger.info(f"âœ… {len(negation_patterns)}ê°œì˜ ë¶€ì • í‘œí˜„ ì¶”ì¶œ ì™„ë£Œ")
        return negation_patterns
    
    def learn_semantic_opposites(self) -> Dict[str, List[str]]:
        """
        BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ìë™ í•™ìŠµ (ì™„ì „ ìë™í™”)
        
        Returns:
            Dict[str, List[str]]: ë°˜ëŒ€ ê´€ê³„ ë§¤í•‘
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        # BERT ëª¨ë¸ ë¡œë“œ
        if not self.bert_model:
            self.bert_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
        
        # ëª¨ë“  ë‹¨ì–´ë“¤ ì¶”ì¶œ
        all_words = set()
        for proverb in self.proverb_data:
            words = re.findall(r'\w+', proverb['answer'] + ' ' + proverb['question'])
            all_words.update([w for w in words if len(w) >= 2])
        
        # ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ìë™ í•™ìŠµ
        opposite_pairs = defaultdict(list)
        
        logger.info(f"ğŸ” {len(all_words)}ê°œ ë‹¨ì–´ì˜ ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ìë™ í•™ìŠµ ì‹œì‘...")
        
        # ëª¨ë“  ë‹¨ì–´ ìŒì— ëŒ€í•´ BERT ê¸°ë°˜ ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ í•™ìŠµ
        for i, word1 in enumerate(all_words):
            if i % 10 == 0:  # ì§„í–‰ìƒí™© í‘œì‹œ
                logger.info(f"ì§„í–‰ë¥ : {i}/{len(all_words)} ({i/len(all_words)*100:.1f}%)")
            
            try:
                word1_embedding = self.bert_model.encode([word1])
                
                # ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = []
                for word2 in all_words:
                    if word2 != word1:
                        word2_embedding = self.bert_model.encode([word2])
                        similarity = cosine_similarity(word1_embedding, word2_embedding)[0][0]
                        similarities.append((word2, similarity))
                
                # ìœ ì‚¬ë„ê°€ ë§¤ìš° ë‚®ì€ ë‹¨ì–´ë“¤ (ë°˜ëŒ€ ì˜ë¯¸ ê°€ëŠ¥ì„±)
                similarities.sort(key=lambda x: x[1])
                low_similarity_words = [w for w, s in similarities[:5] if s < 0.2]  # 0.2 ì´í•˜ë¡œ ë” ì—„ê²©í•˜ê²Œ
                
                if low_similarity_words:
                    opposite_pairs[word1] = low_similarity_words
                    logger.info(f"ìë™ í•™ìŠµëœ ë°˜ëŒ€ ê´€ê³„: {word1} â†” {low_similarity_words}")
                        
            except Exception as e:
                logger.warning(f"ë‹¨ì–´ '{word1}' ë°˜ëŒ€ ê´€ê³„ í•™ìŠµ ì‹¤íŒ¨: {e}")
                continue
        
        self.learned_patterns['opposite_pairs'] = dict(opposite_pairs)
        logger.info(f"âœ… {len(opposite_pairs)}ê°œì˜ ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ìë™ í•™ìŠµ ì™„ë£Œ")
        return dict(opposite_pairs)
    
    def learn_synonyms_and_variations(self) -> Dict[str, List[str]]:
        """
        ë™ì˜ì–´ ë° ìœ ì‚¬ í‘œí˜„ í•™ìŠµ (ìƒˆë¡œ ì¶”ê°€)
        
        Returns:
            Dict[str, List[str]]: ë™ì˜ì–´/ìœ ì‚¬ í‘œí˜„ ë§¤í•‘
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        # BERT ëª¨ë¸ ë¡œë“œ
        if not self.bert_model:
            self.bert_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
        
        # ë™ì˜ì–´/ìœ ì‚¬ í‘œí˜„ ì‚¬ì „ (ì†ë‹´ë³„)
        synonym_pairs = defaultdict(list)
        
        # ìë™ìœ¼ë¡œ ë™ì˜ì–´ í•™ìŠµ (ì§„ì§œ AI í•™ìŠµ)
        # í•µì‹¬ ë‹¨ì–´ëŠ” ë™ì˜ì–´ë¡œ ì¸ì •í•˜ì§€ ì•ŠìŒ
        critical_words = {'íŒ¥', 'ì½©', 'ëŒ“ëŒ', 'ëŒ', 'ëš«ëŠ”ë‹¤', 'ê°œêµ¬ë¦¬', 'ì˜¬ì±™ì´', 'ê²Œ', 'ê°œ', 'ì†Œ', 'ë§', 'ë°©ê·€ë€', 'ë†ˆì´', 'ì„±ë‚¸ë‹¤', 'í™”ë‚¸ë‹¤', 'ê³ ì™€ì•¼', 'ê³±ë‹¤'}
        
        # ì•Œë ¤ì§„ ë™ì˜ì–´/ìœ ì‚¬ í‘œí˜„ë“¤ (ìµœì†Œí•œë§Œ ìˆ˜ë™ ì •ì˜)
        known_synonyms = {
            # ì–´ë¯¸ ë³€í™”ë§Œ í—ˆìš©
            'ëŒë“¯': ['ëŒë“¯ì´', 'ëŒì•„ê°€ë“¯'],
            'ê°ì¶”ë“¯': ['ê°ì¶”ë“¯ì´', 'ìˆ¨ê¸°ë“¯'],
            'ê°ì¶”ë“¯ì´': ['ê°ì¶”ë“¯', 'ìˆ¨ê¸°ë“¯'],
            
            # ë¶€ì • í‘œí˜„ (ì–´ë¯¸ ë³€í™”ë§Œ í—ˆìš©)
            'ì•ŠëŠ”ë‹¤': ['ì•ˆ í•œë‹¤', 'í•˜ì§€ ì•ŠëŠ”ë‹¤', 'ì•Šì•„'],
            'ì•ˆ í•œë‹¤': ['ì•ŠëŠ”ë‹¤', 'í•˜ì§€ ì•ŠëŠ”ë‹¤', 'ì•Šì•„'],
            'ì—†ë‹¤': ['ì—†ì–´', 'ì—†ë„¤', 'ì—†ìŠµë‹ˆë‹¤'],
            'ì—†ì–´': ['ì—†ë‹¤', 'ì—†ë„¤', 'ì—†ìŠµë‹ˆë‹¤'],
            
            # ìˆ«ì í‘œí˜„ (ë™ì˜ì–´ í—ˆìš©)
            'ì—´': ['10', 'ì‹­'],
            '10': ['ì—´', 'ì‹­'],
            'í•œ': ['1', 'ì¼'],
            '1': ['í•œ', 'ì¼'],
            
            # ì‹œê°„ í‘œí˜„ (ë™ì˜ì–´ í—ˆìš©)
            'ì‹œì ˆ': ['ë•Œ', 'ì ', 'ì‹œëŒ€'],
            'ë•Œ': ['ì‹œì ˆ', 'ì ', 'ì‹œëŒ€'],
            
            # ì£¼ì œì–´ (ë™ì˜ì–´ í—ˆìš©)
            'ë§': ['ë§ì”€', 'ì´ì•¼ê¸°'],
            'ë§ì”€': ['ë§', 'ì´ì•¼ê¸°'],
        }
        
        # ëª¨ë“  ë‹¨ì–´ë“¤ ì¶”ì¶œ
        all_words = set()
        for proverb in self.proverb_data:
            words = re.findall(r'\w+', proverb['answer'] + ' ' + proverb['question'])
            all_words.update([w for w in words if len(w) >= 2])
        
        # ì•Œë ¤ì§„ ë™ì˜ì–´ ê´€ê³„ ì ìš©
        for word in all_words:
            if word in known_synonyms:
                synonym_pairs[word] = known_synonyms[word]
        
        # BERT ê¸°ë°˜ ë™ì˜ì–´ ìë™ í•™ìŠµ (í•µì‹¬ ë‹¨ì–´ ì œì™¸)
        for word in all_words:
            if word not in synonym_pairs and word not in critical_words:  # í•µì‹¬ ë‹¨ì–´ëŠ” ì œì™¸
                try:
                    word_embedding = self.bert_model.encode([word])
                    
                    # ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (í•µì‹¬ ë‹¨ì–´ ì œì™¸)
                    similarities = []
                    for other_word in all_words:
                        if other_word != word and len(other_word) >= 2 and other_word not in critical_words:
                            other_embedding = self.bert_model.encode([other_word])
                            similarity = cosine_similarity(word_embedding, other_embedding)[0][0]
                            similarities.append((other_word, similarity))
                    
                    # ìœ ì‚¬ë„ê°€ ë†’ì€ ë‹¨ì–´ë“¤ (ë™ì˜ì–´ ê°€ëŠ¥ì„±) - ë§¤ìš° ì—„ê²©í•˜ê²Œ
                    similarities.sort(key=lambda x: x[1], reverse=True)
                    high_similarity_words = [w for w, s in similarities[:1] if s > 0.85]  # 0.85 ì´ìƒ, ìµœëŒ€ 1ê°œë§Œ
                    
                    if high_similarity_words:
                        synonym_pairs[word] = high_similarity_words
                        logger.info(f"ìë™ í•™ìŠµëœ ë™ì˜ì–´: {word} â†” {high_similarity_words[0]} (ìœ ì‚¬ë„: {similarities[0][1]:.3f})")
                        
                except Exception as e:
                    logger.warning(f"ë‹¨ì–´ '{word}' ë™ì˜ì–´ í•™ìŠµ ì‹¤íŒ¨: {e}")
                    continue
        
        self.learned_patterns['synonym_pairs'] = dict(synonym_pairs)
        logger.info(f"âœ… {len(synonym_pairs)}ê°œì˜ ë™ì˜ì–´/ìœ ì‚¬ í‘œí˜„ í•™ìŠµ ì™„ë£Œ")
        return dict(synonym_pairs)
    
    def learn_context_groups(self) -> Dict[str, List[str]]:
        """
        ë¬¸ë§¥ì  ê´€ë ¨ì„± ê·¸ë£¹ í•™ìŠµ
        
        Returns:
            Dict[str, List[str]]: ë¬¸ë§¥ ê·¸ë£¹ ë§¤í•‘
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        # TF-IDF ë²¡í„°í™”
        if not self.tfidf_vectorizer:
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=None,
                ngram_range=(1, 2)
            )
        
        # ëª¨ë“  ì†ë‹´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_texts = []
        for proverb in self.proverb_data:
            text = proverb['answer'] + ' ' + proverb['question']
            all_texts.append(text)
        
        # TF-IDF ë²¡í„° ìƒì„±
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë¬¸ë§¥ ê·¸ë£¹ ìƒì„±
        n_clusters = min(10, len(all_texts) // 5)  # ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(tfidf_matrix)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ì†ë‹´ ê·¸ë£¹í™”
        context_groups = defaultdict(list)
        for i, label in enumerate(cluster_labels):
            context_groups[f'group_{label}'].append(self.proverb_data[i]['answer'])
        
        self.learned_patterns['context_groups'] = dict(context_groups)
        logger.info(f"âœ… {len(context_groups)}ê°œì˜ ë¬¸ë§¥ ê·¸ë£¹ í•™ìŠµ ì™„ë£Œ")
        return dict(context_groups)
    
    def learn_critical_words(self) -> Dict[str, float]:
        """
        í•µì‹¬ ë‹¨ì–´ ì¤‘ìš”ë„ ìë™ í•™ìŠµ (ì§„ì§œ AI í•™ìŠµ)
        
        Returns:
            Dict[str, float]: ë‹¨ì–´ë³„ ì¤‘ìš”ë„ ì ìˆ˜
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        # 1. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_frequency = Counter()
        word_lengths = {}
        word_positions = defaultdict(list)  # ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚˜ëŠ” ìœ„ì¹˜ë“¤
        
        for proverb in self.proverb_data:
            words = re.findall(r'\w+', proverb['answer'])
            for i, word in enumerate(words):
                if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                    word_frequency[word] += 1
                    word_lengths[word] = len(word)
                    word_positions[word].append(i)  # ìœ„ì¹˜ ì €ì¥
        
        # 2. ìë™ìœ¼ë¡œ í•µì‹¬ ë‹¨ì–´ ì‹ë³„
        critical_words = {}
        total_proverbs = len(self.proverb_data)
        
        for word, freq in word_frequency.items():
            # 2-1. ë¹ˆë„ ê¸°ë°˜ ì¤‘ìš”ë„ (ë¹ˆë„ê°€ ë‚®ì„ìˆ˜ë¡ ì¤‘ìš”)
            rarity_score = (total_proverbs - freq) / total_proverbs
            
            # 2-2. ê¸¸ì´ ê¸°ë°˜ ì¤‘ìš”ë„ (ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ì¤‘ìš”)
            length_score = min(len(word) / 10.0, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì •ê·œí™”
            
            # 2-3. ìœ„ì¹˜ ê¸°ë°˜ ì¤‘ìš”ë„ (ì•ìª½ì— ë‚˜ì˜¬ìˆ˜ë¡ ì¤‘ìš”)
            avg_position = np.mean(word_positions[word]) if word_positions[word] else 0
            position_score = 1.0 - (avg_position / 10.0)  # ì•ìª½ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            
            # 2-4. ì˜ë¯¸ì  ì¤‘ìš”ë„ (ë™ë¬¼, ì‹ë¬¼, êµ¬ì²´ì  ëª…ì‚¬ ë“±)
            semantic_score = 0.0
            if word in ['ê°œêµ¬ë¦¬', 'ì˜¬ì±™ì´', 'ê²Œ', 'ê°œ', 'ì†Œ', 'ë§', 'ê³ ì–‘ì´', 'í˜¸ë‘ì´', 'ì›ìˆ­ì´', 'ë‹­', 'ë±ìƒˆ', 'í™©ìƒˆ', 'ë²”', 'ë‹¤ëŒì¥', 'ë§ë‘¥ì´', 'ê¼´ëšœê¸°', 'ì¥', 'ìƒˆ', 'ìš©', 'ë²¼ë£©', 'ë¼ì§€']:
                semantic_score += 0.3  # ë™ë¬¼
            if word in ['íŒ¥', 'ì½©', 'ê°œì‚´êµ¬', 'ì§„ì£¼', 'ëª©ê±¸ì´', 'ë³„', 'ìš°ë¬¼', 'ëŒ“ëŒ', 'ëŒ', 'ë°”ìœ„', 'ê³„ë€', 'ë°¥', 'ì£½', 'ë–¡', 'ì˜·', 'ì½”', 'ë°œ', 'ë¨¸ë¦¬', 'ì†', 'ëˆˆ', 'ê·€', 'ì…']:
                semantic_score += 0.3  # êµ¬ì²´ì  ëª…ì‚¬
            if word in ['ëš«ëŠ”ë‹¤', 'íŒë‹¤', 'ë–¨ì–´ì§„ë‹¤', 'ëŒë“¯', 'ê°ì¶”ë“¯', 'ì„±ë‚¸ë‹¤', 'í™”ë‚¸ë‹¤', 'ê³ ì™€ì•¼', 'ê³±ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ì•Œë‹¤', 'ëª¨ë¥´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì•ˆë˜ë‹¤', 'í•˜ë‹¤', 'ì•ˆí•˜ë‹¤']:
                semantic_score += 0.2  # í•µì‹¬ ë™ì‚¬/í˜•ìš©ì‚¬
            if word in ['ì‹œì ˆ', 'ë•Œ', 'ì ', 'ì‹œëŒ€', 'ëì—', 'ì¤‘ê°„', 'ì•', 'ë’¤', 'ìœ„', 'ì•„ë˜', 'ì•ˆ', 'ë°–', 'ì†', 'ê²‰']:
                semantic_score += 0.1  # ì‹œê°„/ê³µê°„ í‘œí˜„
            
            # 2-5. ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ íŠ¹ë³„ ì²˜ë¦¬ (ë§¤ìš° ì—„ê²©í•˜ê²Œ)
            if word in ['ì„', 'ë„‰', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´', 'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']:
                semantic_score += 0.5  # ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ì€ ë§¤ìš° ì¤‘ìš”
                rarity_score = 1.0  # ìˆ«ìëŠ” í•­ìƒ ì¤‘ìš”í•˜ê²Œ ì²˜ë¦¬
            
            # 2-6. ì¢…í•© ì¤‘ìš”ë„ ê³„ì‚°
            total_importance = (rarity_score * 0.4) + (length_score * 0.2) + (position_score * 0.1) + (semantic_score * 0.3)
            
            # 2-7. íŒ¨ë„í‹° ê³„ìˆ˜ ê³„ì‚° (ì¤‘ìš”ë„ê°€ ë†’ì„ìˆ˜ë¡ íŒ¨ë„í‹°ë„ ë†’ìŒ)
            if total_importance >= 0.8:  # ë§¤ìš° ì¤‘ìš”í•œ ë‹¨ì–´
                penalty_factor = 0.1  # 90% ê°ì 
            elif total_importance >= 0.6:  # ì¤‘ìš”í•œ ë‹¨ì–´
                penalty_factor = 0.2  # 80% ê°ì 
            elif total_importance >= 0.4:  # ë³´í†µ ì¤‘ìš”í•œ ë‹¨ì–´
                penalty_factor = 0.4  # 60% ê°ì 
            elif total_importance >= 0.2:  # ì•½ê°„ ì¤‘ìš”í•œ ë‹¨ì–´
                penalty_factor = 0.6  # 40% ê°ì 
            else:  # ëœ ì¤‘ìš”í•œ ë‹¨ì–´
                penalty_factor = 0.8  # 20% ê°ì 
            
            critical_words[word] = penalty_factor
        
        self.learned_patterns['critical_words'] = critical_words
        logger.info(f"âœ… {len(critical_words)}ê°œì˜ í•µì‹¬ ë‹¨ì–´ ì¤‘ìš”ë„ ìë™ í•™ìŠµ ì™„ë£Œ")
        logger.info(f"   - ë§¤ìš° ì¤‘ìš” (90% ê°ì ): {len([w for w, p in critical_words.items() if p <= 0.1])}ê°œ")
        logger.info(f"   - ì¤‘ìš” (80% ê°ì ): {len([w for w, p in critical_words.items() if 0.1 < p <= 0.2])}ê°œ")
        logger.info(f"   - ë³´í†µ (60% ê°ì ): {len([w for w, p in critical_words.items() if 0.2 < p <= 0.4])}ê°œ")
        return critical_words
    
    def learn_grammar_patterns(self) -> Dict[str, List[str]]:
        """
        ë¬¸ë²•ì  ë³€í™” íŒ¨í„´ ìë™ í•™ìŠµ (ì–´ë¯¸ ë³€í™”, ì¡°ì‚¬ ë³€í™”, ì–´ìˆœ ë³€í™” ë“±)
        
        Returns:
            Dict[str, List[str]]: ë¬¸ë²•ì  ë³€í™” íŒ¨í„´ ë§¤í•‘
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        grammar_patterns = defaultdict(list)
        
        # ì–´ë¯¸ ë³€í™” íŒ¨í„´ í•™ìŠµ
        ending_patterns = {
            'ë‹¤': ['ëŠ”ë‹¤', 'ë¼', 'ì–´', 'ì•„'],
            'ëŠ”ë‹¤': ['ë‹¤', 'ã„´ë‹¤', 'ì–´', 'ì•„'],
            'ë¼': ['ë‹¤', 'ì–´ë¼', 'ì•„ë¼', 'ì–´', 'ì•„'],
            'ì–´': ['ë‹¤', 'ì–´ë¼', 'ì•„ë¼', 'ë¼'],
            'ì•„': ['ë‹¤', 'ì–´ë¼', 'ì•„ë¼', 'ë¼'],
            'ì´ë‹¤': ['ì´ì•¼', 'ì´ì—ìš”', 'ì…ë‹ˆë‹¤'],
            'ì´ì•¼': ['ì´ë‹¤', 'ì´ì—ìš”', 'ì…ë‹ˆë‹¤'],
            'ì´ì—ìš”': ['ì´ë‹¤', 'ì´ì•¼', 'ì…ë‹ˆë‹¤'],
            'ì…ë‹ˆë‹¤': ['ì´ë‹¤', 'ì´ì•¼', 'ì´ì—ìš”'],
        }
        
        # ì¡°ì‚¬ ë³€í™” íŒ¨í„´ í•™ìŠµ
        particle_patterns = {
            'ì´': ['ê°€', 'ì€', 'ëŠ”'],
            'ê°€': ['ì´', 'ì€', 'ëŠ”'],
            'ì„': ['ë¥¼', 'ì—', 'ì—ì„œ'],
            'ë¥¼': ['ì„', 'ì—', 'ì—ì„œ'],
            'ì—': ['ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ'],
            'ì—ì„œ': ['ì—', 'ë¡œ', 'ìœ¼ë¡œ'],
            'ë¡œ': ['ìœ¼ë¡œ', 'ì—', 'ì—ì„œ'],
            'ìœ¼ë¡œ': ['ë¡œ', 'ì—', 'ì—ì„œ'],
        }
        
        # ëª¨ë“  ë‹¨ì–´ë“¤ ì¶”ì¶œ
        all_words = set()
        for proverb in self.proverb_data:
            words = re.findall(r'\w+', proverb['answer'] + ' ' + proverb['question'])
            all_words.update([w for w in words if len(w) >= 2])
        
        # ì–´ë¯¸ ë³€í™” íŒ¨í„´ ì ìš©
        for word in all_words:
            for base_form, variations in ending_patterns.items():
                if word.endswith(base_form):
                    root = word[:-len(base_form)]
                    for variation in variations:
                        new_word = root + variation
                        if new_word in all_words:
                            grammar_patterns[word].append(new_word)
        
        # ì¡°ì‚¬ ë³€í™” íŒ¨í„´ ì ìš©
        for word in all_words:
            for particle, alternatives in particle_patterns.items():
                if word.endswith(particle):
                    root = word[:-len(particle)]
                    for alt in alternatives:
                        new_word = root + alt
                        if new_word in all_words:
                            grammar_patterns[word].append(new_word)
        
        self.learned_patterns['grammar_patterns'] = dict(grammar_patterns)
        logger.info(f"âœ… {len(grammar_patterns)}ê°œì˜ ë¬¸ë²•ì  ë³€í™” íŒ¨í„´ í•™ìŠµ ì™„ë£Œ")
        return dict(grammar_patterns)
    
    def learn_context_patterns(self) -> Dict[str, List[str]]:
        """
        ë¬¸ë§¥ì  ì¼ê´€ì„± íŒ¨í„´ ìë™ í•™ìŠµ (ì‹œê°„, ì¥ì†Œ, ìƒí™© ë“±)
        
        Returns:
            Dict[str, List[str]]: ë¬¸ë§¥ì  íŒ¨í„´ ë§¤í•‘
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        # ì‹œê°„ ê´€ë ¨ íŒ¨í„´
        temporal_words = set()
        # ê³µê°„ ê´€ë ¨ íŒ¨í„´
        spatial_words = set()
        # ê°ì • ê´€ë ¨ íŒ¨í„´
        emotional_words = set()
        # í–‰ë™ ê´€ë ¨ íŒ¨í„´
        action_words = set()
        # ì‚¬ë¬¼ ê´€ë ¨ íŒ¨í„´
        object_words = set()
        # ì„±ì§ˆ ê´€ë ¨ íŒ¨í„´
        quality_words = set()
        
        # íŒ¨í„´ ì •ì˜
        patterns = {
            'temporal': [
                r'[ì•„ì¹¨|ì ì‹¬|ì €ë…|ë°¤|ë‚®|ìƒˆë²½|ì´ë¥¸|ëŠ¦ì€]',
                r'[ë´„|ì—¬ë¦„|ê°€ì„|ê²¨ìš¸|ê³„ì ˆ]',
                r'[ì–´ì œ|ì˜¤ëŠ˜|ë‚´ì¼|ì–¸ì œ|ë•Œ|ì‹œì ˆ|ì ]',
                r'[ë¹¨ë¦¬|ì²œì²œíˆ|ê¸‰í•˜ê²Œ|ì„œë‘˜ëŸ¬]'
            ],
            'spatial': [
                r'[ìœ„|ì•„ë˜|ì•|ë’¤|ì¢Œ|ìš°|ì˜†|ê°€ìš´ë°|ì¤‘ê°„]',
                r'[ì§‘|ê¸¸|ë§ˆì„|ë„ì‹œ|ë‚˜ë¼|ì„¸ìƒ|í•˜ëŠ˜|ë•…]',
                r'[ì•ˆ|ë°–|ì†|ê²‰|ê·¼ì²˜|ë©€ë¦¬|ê°€ê¹Œì´]'
            ],
            'emotional': [
                r'[ê¸°ì˜|ìŠ¬í”„|í™”ë‚˜|ë¬´ì„œ|ë†€ë¼|ë¶€ëŸ¬|ì§ˆíˆ¬]',
                r'[ì¢‹|ë‚˜ì˜|ì˜ˆì˜|ëª»ìƒê¸°|ê·€ì—½|ì‚¬ë‘]',
                r'[ì›ƒ|ìš¸|í™”|ê¸°ì¨|ìŠ¬í””|ë¶„ë…¸|ê³µí¬]'
            ],
            'action': [
                r'[ê°€|ì˜¤|ì„œ|ì•‰|ì¼ì–´ë‚˜|ë›°|ê±¸|ë‹¬ë¦¬]',
                r'[ë¨¹|ë§ˆì‹œ|ì|ì¼|ë†€|ê³µë¶€|ì¼í•˜]',
                r'[ë§Œë“¤|ì§“|ê³ ì¹˜|ë¶€ìˆ˜|ëœ¯|ì°¢]'
            ],
            'object': [
                r'[ì§‘|ë¬¸|ì°½ë¬¸|ì±…|íœ|ê°€ë°©|ì˜·|ì‹ ë°œ]',
                r'[ë°¥|ë¬¼|ìŒì‹|ê³¼ì¼|ì±„ì†Œ|ê³ ê¸°]',
                r'[ëˆ|ëˆ|ì¬ì‚°|ë³´ë¬¼|ê°’|ê°€ê²©]'
            ],
            'quality': [
                r'[í¬|ì‘|ê¸¸|ì§§|ë‘ê»|ì–‡|ë¬´ê±°|ê°€ë²¼]',
                r'[ë¹¨|ë…¸|íŒŒ|ì´ˆ|ê²€|í°|ë°|ì–´ë‘¡]',
                r'[ë”°ëœ»|ì°¨|ëœ¨ê±°|ì‹œì›|ë§›ìˆ|ë§›ì—†]'
            ]
        }
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ ì¶”ì¶œ
        for proverb in self.proverb_data:
            text = proverb['answer'] + ' ' + proverb['question']
            
            for category, pattern_list in patterns.items():
                for pattern in pattern_list:
                    matches = re.findall(pattern, text)
                    if category == 'temporal':
                        temporal_words.update(matches)
                    elif category == 'spatial':
                        spatial_words.update(matches)
                    elif category == 'emotional':
                        emotional_words.update(matches)
                    elif category == 'action':
                        action_words.update(matches)
                    elif category == 'object':
                        object_words.update(matches)
                    elif category == 'quality':
                        quality_words.update(matches)
        
        # íŒ¨í„´ ì €ì¥
        self.learned_patterns['temporal_patterns'] = temporal_words
        self.learned_patterns['spatial_patterns'] = spatial_words
        self.learned_patterns['emotional_patterns'] = emotional_words
        self.learned_patterns['action_patterns'] = action_words
        self.learned_patterns['object_patterns'] = object_words
        self.learned_patterns['quality_patterns'] = quality_words
        
        logger.info(f"âœ… ë¬¸ë§¥ì  íŒ¨í„´ í•™ìŠµ ì™„ë£Œ:")
        logger.info(f"   - ì‹œê°„ ê´€ë ¨: {len(temporal_words)}ê°œ")
        logger.info(f"   - ê³µê°„ ê´€ë ¨: {len(spatial_words)}ê°œ")
        logger.info(f"   - ê°ì • ê´€ë ¨: {len(emotional_words)}ê°œ")
        logger.info(f"   - í–‰ë™ ê´€ë ¨: {len(action_words)}ê°œ")
        logger.info(f"   - ì‚¬ë¬¼ ê´€ë ¨: {len(object_words)}ê°œ")
        logger.info(f"   - ì„±ì§ˆ ê´€ë ¨: {len(quality_words)}ê°œ")
        
        return {
            'temporal': list(temporal_words),
            'spatial': list(spatial_words),
            'emotional': list(emotional_words),
            'action': list(action_words),
            'object': list(object_words),
            'quality': list(quality_words)
        }
    
    def learn_phonetic_patterns(self) -> Dict[str, List[str]]:
        """
        ìŒì„±í•™ì  ìœ ì‚¬ì„± íŒ¨í„´ ìë™ í•™ìŠµ (ë°œìŒ ë¹„ìŠ·í•œ ë‹¨ì–´, ì˜¤íƒ€ ë“±)
        
        Returns:
            Dict[str, List[str]]: ìŒì„±í•™ì  ìœ ì‚¬ì„± íŒ¨í„´ ë§¤í•‘
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        phonetic_patterns = defaultdict(list)
        
        # í•œê¸€ ììŒ/ëª¨ìŒ ë¶„í•´ í•¨ìˆ˜
        def decompose_korean(char):
            if 'ê°€' <= char <= 'í£':
                base = ord(char) - ord('ê°€')
                cho = base // (21 * 28)
                jung = (base % (21 * 28)) // 28
                jong = base % 28
                return (cho, jung, jong)
            return None
        
        # í•œê¸€ ì¬ì¡°í•© í•¨ìˆ˜
        def compose_korean(cho, jung, jong):
            return chr(ord('ê°€') + cho * 21 * 28 + jung * 28 + jong)
        
        # ëª¨ë“  ë‹¨ì–´ë“¤ ì¶”ì¶œ
        all_words = set()
        for proverb in self.proverb_data:
            words = re.findall(r'\w+', proverb['answer'] + ' ' + proverb['question'])
            all_words.update([w for w in words if len(w) >= 2])
        
        # ìŒì„±í•™ì  ìœ ì‚¬ì„± ê³„ì‚°
        for word1 in all_words:
            for word2 in all_words:
                if word1 != word2 and len(word1) == len(word2):
                    similarity = self._calculate_phonetic_similarity(word1, word2)
                    if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬
                        phonetic_patterns[word1].append(word2)
        
        self.learned_patterns['phonetic_patterns'] = dict(phonetic_patterns)
        logger.info(f"âœ… {len(phonetic_patterns)}ê°œì˜ ìŒì„±í•™ì  ìœ ì‚¬ì„± íŒ¨í„´ í•™ìŠµ ì™„ë£Œ")
        return dict(phonetic_patterns)
    
    def _calculate_phonetic_similarity(self, word1: str, word2: str) -> float:
        """
        ë‘ ë‹¨ì–´ì˜ ìŒì„±í•™ì  ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            word1: ì²« ë²ˆì§¸ ë‹¨ì–´
            word2: ë‘ ë²ˆì§¸ ë‹¨ì–´
            
        Returns:
            float: ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        if len(word1) != len(word2):
            return 0.0
        
        total_similarity = 0.0
        for c1, c2 in zip(word1, word2):
            decomp1 = decompose_korean(c1)
            decomp2 = decompose_korean(c2)
            
            if decomp1 and decomp2:
                cho1, jung1, jong1 = decomp1
                cho2, jung2, jong2 = decomp2
                
                # ììŒ ìœ ì‚¬ë„
                cho_sim = 1.0 if cho1 == cho2 else 0.0
                # ëª¨ìŒ ìœ ì‚¬ë„
                jung_sim = 1.0 if jung1 == jung2 else 0.0
                # ë°›ì¹¨ ìœ ì‚¬ë„
                jong_sim = 1.0 if jong1 == jong2 else 0.0
                
                char_similarity = (cho_sim * 0.4 + jung_sim * 0.4 + jong_sim * 0.2)
                total_similarity += char_similarity
            else:
                # í•œê¸€ì´ ì•„ë‹Œ ê²½ìš° ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
                total_similarity += 1.0 if c1 == c2 else 0.0
        
        return total_similarity / len(word1)
    
    def learn_structural_patterns(self) -> Dict[str, List[str]]:
        """
        êµ¬ì¡°ì  íŒ¨í„´ ìë™ í•™ìŠµ (ë¬¸ì¥ êµ¬ì¡°, ì–´ìˆœ, ê¸¸ì´ ë“±)
        
        Returns:
            Dict[str, List[str]]: êµ¬ì¡°ì  íŒ¨í„´ ë§¤í•‘
        """
        if not self.proverb_data:
            self.load_proverb_data()
        
        structural_patterns = defaultdict(list)
        
        # ë¬¸ì¥ êµ¬ì¡° íŒ¨í„´ ë¶„ì„
        for proverb in self.proverb_data:
            answer = proverb['answer']
            question = proverb['question']
            
            # ê¸¸ì´ë³„ ë¶„ë¥˜
            length_category = self._categorize_by_length(answer)
            
            # ì–´ìˆœ íŒ¨í„´ ë¶„ì„
            word_order = self._analyze_word_order(answer)
            
            # ë¬¸ì¥ êµ¬ì¡° íŒ¨í„´
            structure = self._analyze_sentence_structure(answer)
            
            # íŒ¨í„´ ì €ì¥
            pattern_key = f"{length_category}_{word_order}_{structure}"
            structural_patterns[pattern_key].append(answer)
        
        self.learned_patterns['structural_patterns'] = dict(structural_patterns)
        logger.info(f"âœ… {len(structural_patterns)}ê°œì˜ êµ¬ì¡°ì  íŒ¨í„´ í•™ìŠµ ì™„ë£Œ")
        return dict(structural_patterns)
    
    def _categorize_by_length(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ê¸¸ì´ë³„ ë¶„ë¥˜"""
        length = len(text)
        if length <= 3:
            return "very_short"
        elif length <= 6:
            return "short"
        elif length <= 10:
            return "medium"
        elif length <= 15:
            return "long"
        else:
            return "very_long"
    
    def _analyze_word_order(self, text: str) -> str:
        """ì–´ìˆœ íŒ¨í„´ ë¶„ì„"""
        words = re.findall(r'\w+', text)
        if len(words) <= 2:
            return "simple"
        elif len(words) <= 4:
            return "moderate"
        else:
            return "complex"
    
    def _analyze_sentence_structure(self, text: str) -> str:
        """ë¬¸ì¥ êµ¬ì¡° ë¶„ì„"""
        if 'ì€' in text or 'ëŠ”' in text:
            return "topic_marker"
        elif 'ì´' in text or 'ê°€' in text:
            return "subject_marker"
        elif 'ì„' in text or 'ë¥¼' in text:
            return "object_marker"
        else:
            return "simple_structure"
    
    def save_learned_patterns(self, filepath: str = "learned_patterns.pkl"):
        """í•™ìŠµëœ íŒ¨í„´ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filepath, 'wb') as f:
                pickle.dump(self.learned_patterns, f)
            logger.info(f"âœ… í•™ìŠµëœ íŒ¨í„´ ì €ì¥ ì™„ë£Œ: {filepath}")
        except Exception as e:
            logger.error(f"íŒ¨í„´ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_learned_patterns(self, filepath: str = "learned_patterns.pkl"):
        """ì €ì¥ëœ íŒ¨í„´ë“¤ì„ íŒŒì¼ì—ì„œ ë¡œë“œ"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'rb') as f:
                    self.learned_patterns = pickle.load(f)
                logger.info(f"âœ… í•™ìŠµëœ íŒ¨í„´ ë¡œë“œ ì™„ë£Œ: {filepath}")
                return True
        except Exception as e:
            logger.error(f"íŒ¨í„´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    
    def full_learning_process(self):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ğŸ§  AI í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        # 1. ë°ì´í„° ë¡œë“œ
        self.load_proverb_data()
        
        # 2. ê¸°ë³¸ íŒ¨í„´ í•™ìŠµ
        self.extract_animals_from_data()
        self.extract_negation_patterns()
        self.learn_semantic_opposites()
        self.learn_synonyms_and_variations()
        self.learn_context_groups()
        self.learn_critical_words()
        
        # 3. ê³ ê¸‰ íŒ¨í„´ í•™ìŠµ (ìƒˆë¡œ ì¶”ê°€)
        self.learn_grammar_patterns()
        self.learn_context_patterns()
        self.learn_phonetic_patterns()
        self.learn_structural_patterns()
        
        # 4. í•™ìŠµ ê²°ê³¼ ì €ì¥
        self.save_learned_patterns()
        
        logger.info("âœ… AI í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        return self.learned_patterns


class IntelligentProverbChecker:
    """ì§€ëŠ¥ì  ì†ë‹´ ê²€ì‚¬ í´ë˜ìŠ¤ (í•™ìŠµëœ íŒ¨í„´ í™œìš©)"""
    
    def __init__(self, learner: ProverbDataLearner = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            learner: í•™ìŠµëœ íŒ¨í„´ì„ ê°€ì§„ ProverbDataLearner ì¸ìŠ¤í„´ìŠ¤
        """
        self.learner = learner or ProverbDataLearner()
        
        # BERT ëª¨ë¸ ë¡œë“œ
        self.bert_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
        
        # í•™ìŠµëœ íŒ¨í„´ ë¡œë“œ ì‹œë„
        if not self.learner.learned_patterns.get('animals'):
            logger.info("í•™ìŠµëœ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.learner.full_learning_process()
    
    def detect_animal_confusion(self, original: str, user_input: str) -> float:
        """
        ë™ë¬¼ í˜¼ë™ ê°ì§€ (í•™ìŠµëœ íŒ¨í„´ ê¸°ë°˜)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (1.0 = ì •ìƒ, 0.01 = 99% ê°ì )
        """
        animal_groups = self.learner.learned_patterns.get('animal_groups', {})
        
        # ì›ë³¸ê³¼ ì‚¬ìš©ì ì…ë ¥ì—ì„œ ë™ë¬¼ ì¶”ì¶œ
        orig_animals = self._extract_animals_from_text(original)
        user_animals = self._extract_animals_from_text(user_input)
        
        if not orig_animals or not user_animals:
            return 1.0
        
        # ë‹¤ë¥¸ ê·¸ë£¹ì˜ ë™ë¬¼ë¡œ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸
        for orig_animal in orig_animals:
            orig_group = self._get_animal_group(orig_animal, animal_groups)
            
            for user_animal in user_animals:
                user_group = self._get_animal_group(user_animal, animal_groups)
                
                # ë‹¤ë¥¸ ê·¸ë£¹ì˜ ë™ë¬¼ë¡œ ë°”ë€Œì—ˆìœ¼ë©´ í° íŒ¨ë„í‹°
                if orig_group != user_group and orig_group != 'unknown' and user_group != 'unknown':
                    logger.info(f"ë™ë¬¼ ê·¸ë£¹ ë³€ê²½ ê°ì§€: {orig_animal}({orig_group}) â†’ {user_animal}({user_group})")
                    return 0.01  # 99% ê°ì 
        
        return 1.0
    
    def detect_semantic_opposite(self, original: str, user_input: str) -> float:
        """
        ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ê°ì§€ (ì™„ì „ ìë™í™” - BERT ê¸°ë°˜)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜
        """
        # 1. í•™ìŠµëœ íŒ¨í„´ ê¸°ë°˜ ê²€ì‚¬
        opposite_pairs = self.learner.learned_patterns.get('opposite_pairs', {})
        
        # ë‹¨ì–´ë³„ ë°˜ëŒ€ ê´€ê³„ í™•ì¸
        orig_words = re.findall(r'\w+', original)
        user_words = re.findall(r'\w+', user_input)
        
        opposite_count = 0
        total_checks = 0
        
        for orig_word in orig_words:
            if orig_word in opposite_pairs:
                total_checks += 1
                if orig_word in user_words:
                    continue  # ì›ë³¸ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì •ìƒ
                
                # ë°˜ëŒ€ ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
                for opposite_word in opposite_pairs[orig_word]:
                    if opposite_word in user_words:
                        opposite_count += 1
                        logger.info(f"í•™ìŠµëœ ë°˜ëŒ€ ê´€ê³„ ê°ì§€: {orig_word} â†’ {opposite_word}")
                        break
        
        # 2. BERT ê¸°ë°˜ ì‹¤ì‹œê°„ ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ê°ì§€
        if self.bert_model and len(orig_words) > 0 and len(user_words) > 0:
            try:
                # ì›ë³¸ê³¼ ì‚¬ìš©ì ì…ë ¥ì˜ ì „ì²´ ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚°
                embeddings = self.bert_model.encode([original, user_input])
                sentence_similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                
                # ìœ ì‚¬ë„ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ ë°˜ëŒ€ ì˜ë¯¸ë¡œ íŒë‹¨
                if sentence_similarity < 0.3:
                    opposite_count += 1
                    total_checks += 1
                    logger.info(f"BERT ê¸°ë°˜ ì˜ë¯¸ ë°˜ì „ ê°ì§€: ìœ ì‚¬ë„ {sentence_similarity:.3f}")
                
                # ê°œë³„ ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ë„ í™•ì¸
                for orig_word in orig_words:
                    if len(orig_word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                        for user_word in user_words:
                            if len(user_word) >= 2 and orig_word != user_word:
                                word_embeddings = self.bert_model.encode([orig_word, user_word])
                                word_similarity = cosine_similarity([word_embeddings[0]], [word_embeddings[1]])[0][0]
                                
                                # ë‹¨ì–´ ìœ ì‚¬ë„ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ ë°˜ëŒ€ ì˜ë¯¸ë¡œ íŒë‹¨
                                if word_similarity < 0.2:
                                    opposite_count += 1
                                    total_checks += 1
                                    logger.info(f"BERT ê¸°ë°˜ ë‹¨ì–´ ë°˜ëŒ€ ê°ì§€: {orig_word} â†” {user_word} (ìœ ì‚¬ë„: {word_similarity:.3f})")
                                    break
                                    
            except Exception as e:
                logger.warning(f"BERT ê¸°ë°˜ ë°˜ëŒ€ ê´€ê³„ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        if total_checks == 0:
            return 1.0
        
        opposite_ratio = opposite_count / total_checks
        
        if opposite_ratio > 0.5:
            return 0.01  # 99% ê°ì 
        elif opposite_ratio > 0.3:
            return 0.1   # 90% ê°ì 
        else:
            return 1.0   # ì •ìƒ
    
    def detect_synonyms_and_variations(self, original: str, user_input: str) -> float:
        """
        ë™ì˜ì–´ ë° ìœ ì‚¬ í‘œí˜„ ê°ì§€ (í˜„ì‹¤ì ì¸ í—ˆìš© ë²”ìœ„)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: ë³´ë„ˆìŠ¤ ê³„ìˆ˜ (1.0 = ë³´ë„ˆìŠ¤ ì—†ìŒ, >1.0 = ë³´ë„ˆìŠ¤)
        """
        synonym_pairs = self.learner.learned_patterns.get('synonym_pairs', {})
        
        # ë‹¨ì–´ë³„ ë™ì˜ì–´ í™•ì¸
        orig_words = re.findall(r'\w+', original)
        user_words = re.findall(r'\w+', user_input)
        
        # í•µì‹¬ ëª…ì‚¬/ì£¼ì–´ëŠ” ë³€ê²½ í—ˆìš©í•˜ì§€ ì•ŠìŒ (ë„ˆë¬´ ê³¼í•œ ë³€í˜• ë°©ì§€)
        critical_nouns = ['ë°©ê·€ë€', 'ë†ˆì´', 'ê°œêµ¬ë¦¬', 'ì˜¬ì±™ì´', 'ê²Œ', 'ê°œ', 'ì†Œ', 'ë§', 'ê³ ì–‘ì´', 'í˜¸ë‘ì´']
        
        # í•µì‹¬ ëª…ì‚¬ê°€ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸
        for noun in critical_nouns:
            if noun in original and noun not in user_input:
                # í•µì‹¬ ëª…ì‚¬ê°€ ì—†ì–´ì¡Œìœ¼ë©´ ë‹¤ë¥¸ í•µì‹¬ ëª…ì‚¬ë¡œ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸
                other_critical_found = False
                for other_noun in critical_nouns:
                    if other_noun != noun and other_noun in user_input and other_noun not in original:
                        other_critical_found = True
                        break
                
                if other_critical_found:
                    logger.info(f"í•µì‹¬ ëª…ì‚¬ ë³€ê²½ ê°ì§€: {noun} â†’ ë‹¤ë¥¸ í•µì‹¬ ëª…ì‚¬ (ê³¼í•œ ë³€í˜•)")
                    return 0.5  # 50% ê°ì  (ê³¼í•œ ë³€í˜•)
        
        synonym_matches = 0
        total_words = len(orig_words)
        
        if total_words == 0:
            return 1.0
        
        for orig_word in orig_words:
            if orig_word in user_words:
                synonym_matches += 1  # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹˜
                continue
            
            # ë™ì˜ì–´ í™•ì¸ (í•µì‹¬ ëª…ì‚¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
            if orig_word in synonym_pairs and orig_word not in critical_nouns:
                for synonym in synonym_pairs[orig_word]:
                    if synonym in user_words:
                        synonym_matches += 1
                        logger.info(f"ë™ì˜ì–´ ê°ì§€: {orig_word} â†” {synonym}")
                        break
        
        # ë™ì˜ì–´ ë§¤ì¹˜ ë¹„ìœ¨ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ë” ì—„ê²©í•˜ê²Œ)
        match_ratio = synonym_matches / total_words
        
        if match_ratio >= 0.9:  # 90% ì´ìƒ ë§¤ì¹˜ (ë” ì—„ê²©)
            return 1.1  # 10% ë³´ë„ˆìŠ¤ (ë” ì ê²Œ)
        elif match_ratio >= 0.7:  # 70% ì´ìƒ ë§¤ì¹˜
            return 1.05  # 5% ë³´ë„ˆìŠ¤ (ë” ì ê²Œ)
        else:
            return 1.0  # ë³´ë„ˆìŠ¤ ì—†ìŒ
    
    def detect_critical_word_missing(self, original: str, user_input: str) -> float:
        """
        í•µì‹¬ ë‹¨ì–´ ëˆ„ë½ ê°ì§€ (ë” ì—„ê²©í•˜ê²Œ)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜
        """
        critical_words = self.learner.learned_patterns.get('critical_words', {})
        
        penalty = 1.0
        missing_words = []
        critical_missing_count = 0
        
        # ì›ë³¸ ì†ë‹´ì˜ ëª¨ë“  ë‹¨ì–´ í™•ì¸
        orig_words = re.findall(r'\w+', original)
        user_words = re.findall(r'\w+', user_input)
        
        for word in orig_words:
            if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ ì²´í¬
                if word in user_words:
                    continue  # ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì •ìƒ
                
                # í•µì‹¬ ë‹¨ì–´ì¸ì§€ í™•ì¸
                if word in critical_words:
                    penalty_factor = critical_words[word]
                    penalty *= penalty_factor
                    missing_words.append(word)
                    
                    # ë§¤ìš° ì¤‘ìš”í•œ ë‹¨ì–´ ëˆ„ë½ì‹œ ì¹´ìš´íŠ¸
                    if penalty_factor <= 0.2:  # 80% ì´ìƒ ê°ì í•˜ëŠ” ë‹¨ì–´
                        critical_missing_count += 1
        
        # ë§¤ìš° ì¤‘ìš”í•œ ë‹¨ì–´ê°€ 2ê°œ ì´ìƒ ëˆ„ë½ë˜ë©´ ê±°ì˜ ì˜¤ë‹µ ì²˜ë¦¬
        if critical_missing_count >= 2:
            penalty = 0.01  # 99% ê°ì 
            logger.info(f"í•µì‹¬ ë‹¨ì–´ 2ê°œ ì´ìƒ ëˆ„ë½: {missing_words}, ê±°ì˜ ì˜¤ë‹µ ì²˜ë¦¬")
        elif critical_missing_count >= 1:
            penalty = min(penalty, 0.1)  # ìµœëŒ€ 90% ê°ì 
            logger.info(f"í•µì‹¬ ë‹¨ì–´ 1ê°œ ëˆ„ë½: {missing_words}, 90% ê°ì ")
        elif missing_words:
            logger.info(f"ì¼ë°˜ ë‹¨ì–´ ëˆ„ë½: {missing_words}, íŒ¨ë„í‹°: {penalty}")
        
        return penalty
    
    def detect_number_quantity_accuracy(self, original: str, user_input: str) -> float:
        """
        ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ ì •í™•ì„± ê°ì§€ (ë§¤ìš° ì—„ê²©í•œ ì •í™• ë§¤ì¹­ë§Œ í—ˆìš©)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (1.0 = ì •ìƒ, 0.01 = 99% ê°ì )
        """
        # ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ íŒ¨í„´ (ìë™ ê°ì§€)
        number_patterns = [
            r'\d+',  # ìˆ«ì
            r'[í•˜ë‚˜ë‘˜ì…‹ë„·ë‹¤ì„¯ì—¬ì„¯ì¼ê³±ì—¬ëŸì•„í™‰ì—´]',  # í•œê¸€ ìˆ«ì
            r'[ì¼ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬ì‹­]',  # í•œì ìˆ«ì
            r'[ì„ë„‰]'  # íŠ¹ìˆ˜ ìˆ˜ëŸ‰ í‘œí˜„
        ]
        
        # ì›ë³¸ê³¼ ì‚¬ìš©ì ì…ë ¥ì—ì„œ ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ ì¶”ì¶œ
        orig_numbers = []
        user_numbers = []
        
        for pattern in number_patterns:
            orig_matches = re.findall(pattern, original)
            user_matches = re.findall(pattern, user_input)
            orig_numbers.extend(orig_matches)
            user_numbers.extend(user_matches)
        
        # ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ì´ ì—†ìœ¼ë©´ ì •ìƒ
        if not orig_numbers:
            return 1.0
        
        # 1. ì •í™•í•œ ë§¤ì¹­ í™•ì¸ (ìµœìš°ì„ )
        if orig_numbers == user_numbers:
            return 1.0  # ì •ìƒ
        
        # 2. ì—„ê²©í•œ ë™ì˜ì–´ ë§¤í•‘ë§Œ í—ˆìš© (BERT ê¸°ë°˜ ìœ ì‚¬ë„ ì œê±°)
        number_mappings = {
            'í•˜ë‚˜': ['1', 'ì¼'],
            'ë‘˜': ['2', 'ì´'],
            'ì…‹': ['3', 'ì‚¼'],
            'ë„·': ['4', 'ì‚¬'],
            'ë‹¤ì„¯': ['5', 'ì˜¤'],
            'ì—¬ì„¯': ['6', 'ìœ¡'],
            'ì¼ê³±': ['7', 'ì¹ '],
            'ì—¬ëŸ': ['8', 'íŒ”'],
            'ì•„í™‰': ['9', 'êµ¬'],
            'ì—´': ['10', 'ì‹­'],
            'ì„': ['3', 'ì‚¼', 'ì…‹'],
            'ë„‰': ['4', 'ì‚¬', 'ë„·']
        }
        
        # ë§¤í•‘ ê¸°ë°˜ ë™ì˜ì–´ í™•ì¸ (ì •í™•í•œ ë§¤í•‘ë§Œ, ì•½ê°„ì˜ ê°ì  ì ìš©)
        for orig_num in orig_numbers:
            if orig_num in number_mappings:
                for user_num in user_numbers:
                    if user_num in number_mappings[orig_num]:
                        logger.info(f"ì—„ê²©í•œ ë™ì˜ì–´ ì¸ì •: {orig_num} â†” {user_num}")
                        return 0.95  # 5% ê°ì  (ë™ì˜ì–´ í—ˆìš©í•˜ì§€ë§Œ ì•½ê°„ì˜ ê°ì )
        
        # 3. ëª¨ë“  ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ì™„ì „ ì˜¤ë‹µ ì²˜ë¦¬
        logger.info(f"ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ ë¶ˆì¼ì¹˜: ì›ë³¸ {orig_numbers} vs ì…ë ¥ {user_numbers}")
        return 0.01  # 99% ê°ì  (ì™„ì „ ì˜¤ë‹µ)
    
    def detect_grammar_variations(self, original: str, user_input: str) -> float:
        """
        ë¬¸ë²•ì  ë³€í™” ê°ì§€ (ì–´ë¯¸ ë³€í™”, ì¡°ì‚¬ ë³€í™” ë“±)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: ë³´ë„ˆìŠ¤ ê³„ìˆ˜ (1.0 = ë³´ë„ˆìŠ¤ ì—†ìŒ, >1.0 = ë³´ë„ˆìŠ¤)
        """
        grammar_patterns = self.learner.learned_patterns.get('grammar_patterns', {})
        
        # ë‹¨ì–´ë³„ ë¬¸ë²•ì  ë³€í™” í™•ì¸
        orig_words = re.findall(r'\w+', original)
        user_words = re.findall(r'\w+', user_input)
        
        grammar_matches = 0
        total_words = len(orig_words)
        
        if total_words == 0:
            return 1.0
        
        for orig_word in orig_words:
            if orig_word in user_words:
                grammar_matches += 1  # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹˜
                continue
            
            # ë¬¸ë²•ì  ë³€í™” í™•ì¸
            if orig_word in grammar_patterns:
                for variation in grammar_patterns[orig_word]:
                    if variation in user_words:
                        grammar_matches += 1
                        logger.info(f"ë¬¸ë²•ì  ë³€í™” ê°ì§€: {orig_word} â†” {variation}")
                        break
        
        # ë¬¸ë²•ì  ë§¤ì¹˜ ë¹„ìœ¨ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        match_ratio = grammar_matches / total_words
        
        if match_ratio >= 0.9:  # 90% ì´ìƒ ë§¤ì¹˜
            return 1.05  # 5% ë³´ë„ˆìŠ¤
        elif match_ratio >= 0.7:  # 70% ì´ìƒ ë§¤ì¹˜
            return 1.02  # 2% ë³´ë„ˆìŠ¤
        else:
            return 1.0  # ë³´ë„ˆìŠ¤ ì—†ìŒ
    
    def detect_context_consistency(self, original: str, user_input: str) -> float:
        """
        ë¬¸ë§¥ì  ì¼ê´€ì„± ê°ì§€ (ì‹œê°„, ì¥ì†Œ, ìƒí™© ë“±)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (1.0 = ì •ìƒ, 0.01 = 99% ê°ì )
        """
        # ë¬¸ë§¥ì  íŒ¨í„´ë“¤
        temporal_patterns = self.learner.learned_patterns.get('temporal_patterns', set())
        spatial_patterns = self.learner.learned_patterns.get('spatial_patterns', set())
        emotional_patterns = self.learner.learned_patterns.get('emotional_patterns', set())
        action_patterns = self.learner.learned_patterns.get('action_patterns', set())
        object_patterns = self.learner.learned_patterns.get('object_patterns', set())
        quality_patterns = self.learner.learned_patterns.get('quality_patterns', set())
        
        # ì›ë³¸ê³¼ ì‚¬ìš©ì ì…ë ¥ì—ì„œ ë¬¸ë§¥ì  ë‹¨ì–´ ì¶”ì¶œ
        orig_context_words = self._extract_context_words(original, temporal_patterns, spatial_patterns, 
                                                        emotional_patterns, action_patterns, object_patterns, quality_patterns)
        user_context_words = self._extract_context_words(user_input, temporal_patterns, spatial_patterns,
                                                        emotional_patterns, action_patterns, object_patterns, quality_patterns)
        
        if not orig_context_words:
            return 1.0  # ë¬¸ë§¥ì  ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ ì •ìƒ
        
        # ë¬¸ë§¥ì  ì¼ê´€ì„± ê³„ì‚°
        context_overlap = len(orig_context_words & user_context_words)
        context_total = len(orig_context_words | user_context_words)
        
        if context_total == 0:
            return 1.0
        
        consistency_ratio = context_overlap / context_total
        
        if consistency_ratio < 0.2:  # 20% ë¯¸ë§Œ ì¼ì¹˜
            logger.info(f"ë¬¸ë§¥ì  ì¼ê´€ì„± ë¶€ì¡±: {orig_context_words} vs {user_context_words}")
            return 0.01  # 99% ê°ì 
        elif consistency_ratio < 0.5:  # 50% ë¯¸ë§Œ ì¼ì¹˜
            return 0.1   # 90% ê°ì 
        else:
            return 1.0   # ì •ìƒ
    
    def _extract_context_words(self, text: str, temporal: set, spatial: set, emotional: set, 
                              action: set, object: set, quality: set) -> set:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ë§¥ì  ë‹¨ì–´ ì¶”ì¶œ"""
        words = re.findall(r'\w+', text)
        context_words = set()
        
        for word in words:
            if word in temporal or word in spatial or word in emotional or \
               word in action or word in object or word in quality:
                context_words.add(word)
        
        return context_words
    
    def detect_phonetic_similarity(self, original: str, user_input: str) -> float:
        """
        ìŒì„±í•™ì  ìœ ì‚¬ì„± ê°ì§€ (ë°œìŒ ë¹„ìŠ·í•œ ë‹¨ì–´, ì˜¤íƒ€ ë“±)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: ë³´ë„ˆìŠ¤ ê³„ìˆ˜ (1.0 = ë³´ë„ˆìŠ¤ ì—†ìŒ, >1.0 = ë³´ë„ˆìŠ¤)
        """
        phonetic_patterns = self.learner.learned_patterns.get('phonetic_patterns', {})
        
        # ë‹¨ì–´ë³„ ìŒì„±í•™ì  ìœ ì‚¬ì„± í™•ì¸
        orig_words = re.findall(r'\w+', original)
        user_words = re.findall(r'\w+', user_input)
        
        phonetic_matches = 0
        total_words = len(orig_words)
        
        if total_words == 0:
            return 1.0
        
        for orig_word in orig_words:
            if orig_word in user_words:
                phonetic_matches += 1  # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹˜
                continue
            
            # ìŒì„±í•™ì  ìœ ì‚¬ì„± í™•ì¸
            if orig_word in phonetic_patterns:
                for similar_word in phonetic_patterns[orig_word]:
                    if similar_word in user_words:
                        phonetic_matches += 1
                        logger.info(f"ìŒì„±í•™ì  ìœ ì‚¬ì„± ê°ì§€: {orig_word} â†” {similar_word}")
                        break
        
        # ìŒì„±í•™ì  ë§¤ì¹˜ ë¹„ìœ¨ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        match_ratio = phonetic_matches / total_words
        
        if match_ratio >= 0.9:  # 90% ì´ìƒ ë§¤ì¹˜
            return 1.03  # 3% ë³´ë„ˆìŠ¤
        elif match_ratio >= 0.7:  # 70% ì´ìƒ ë§¤ì¹˜
            return 1.01  # 1% ë³´ë„ˆìŠ¤
        else:
            return 1.0  # ë³´ë„ˆìŠ¤ ì—†ìŒ
    
    def detect_partial_answer_acceptance(self, original: str, user_input: str) -> float:
        """
        ë¶€ë¶„ ë‹µë³€ í—ˆìš© ì—¬ë¶€ ê°ì§€ (ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ë¶€ë¶„ ë‹µë³€ë§Œ í—ˆìš©)
        
        Args:
            original: ì›ë³¸ ì†ë‹´ (ì •ë‹µ ë¶€ë¶„)
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: ë³´ë„ˆìŠ¤ ê³„ìˆ˜ (1.0 = ë³´ë„ˆìŠ¤ ì—†ìŒ, >1.0 = ë³´ë„ˆìŠ¤)
        """
        # ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” ì˜ë¯¸ ìˆëŠ” ë¶€ë¶„ ë‹µë³€ë“¤ (í•˜ë“œì½”ë”©ëœ íŒ¨í„´)
        meaningful_partial_answers = {
            # ë¹„êµ êµ¬ì¡° - ì‹¤ì œë¡œ ì´ë ‡ê²Œ ë§í•˜ëŠ” ê²½ìš°ë“¤
            "ë°°ê¼½ì´ í¬ë‹¤": ["ë°°ê¼½"],
            "ìš© ë‚œë‹¤": ["ìš© ë‚œë‹¤"],
            "ì˜¤ëŠ” ë§ì´ ê³±ë‹¤": ["ì˜¤ëŠ” ë§ì´ ê³±ë‹¤"],
            
            # ì™„ì „í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë¶€ë¶„ ë‹µë³€ë“¤
            "ì˜¬ì±™ì´ ì‹œì ˆ": ["ì˜¬ì±™ì´ ì‹œì ˆ"],
            "ì™¸ì–‘ê°„ ê³ ì¹œë‹¤": ["ì™¸ì–‘ê°„ ê³ ì¹œë‹¤"],
            "ë¶€ì±„ì§ˆí•œë‹¤": ["ë¶€ì±„ì§ˆí•œë‹¤"],
            "ê¹€ì„œë°© ì°¾ê¸°": ["ê¹€ì„œë°© ì°¾ê¸°"],
            "ì—¬ë“ ê¹Œì§€ ê°„ë‹¤": ["ì—¬ë“ ê¹Œì§€ ê°„ë‹¤"],
            "ë¨¹ì„ ê²ƒ ì—†ë‹¤": ["ë¨¹ì„ ê²ƒ ì—†ë‹¤"],
            "ì½”ê°€ ê¹¨ì§„ë‹¤": ["ì½”ê°€ ê¹¨ì§„ë‹¤"],
            "ë¶€ëšœë§‰ì— ë¨¼ì € ì˜¬ë¼ê°„ë‹¤": ["ë¶€ëšœë§‰ì— ë¨¼ì € ì˜¬ë¼ê°„ë‹¤"],
            "ì³ë‹¤ë³´ì§€ë„ ë§ì•„ë¼": ["ì³ë‹¤ë³´ì§€ë„ ë§ì•„ë¼"],
            "ê²¨ì ë¨¹ê¸°": ["ê²¨ì ë¨¹ê¸°"],
        }
        
        # ì›ë³¸ ì†ë‹´ì´ ë¶€ë¶„ ë‹µë³€ í—ˆìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if original not in meaningful_partial_answers:
            return 1.0  # ë¶€ë¶„ ë‹µë³€ í—ˆìš© ì•ˆí•¨
        
        # í—ˆìš©ë˜ëŠ” ë¶€ë¶„ ë‹µë³€ ëª©ë¡ì—ì„œ í™•ì¸
        allowed_partials = meaningful_partial_answers[original]
        
        # ì •í™•í•œ ë¶€ë¶„ ë‹µë³€ ë§¤ì¹­ í™•ì¸
        for allowed_partial in allowed_partials:
            if user_input == allowed_partial:
                logger.info(f"ì˜ë¯¸ ìˆëŠ” ë¶€ë¶„ ë‹µë³€ ì¸ì •: {original} â†’ {user_input}")
                return 1.2  # 20% ë³´ë„ˆìŠ¤ (ë¶€ë¶„ ë‹µë³€ í—ˆìš©)
        
        # BERT ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¡œ ì¶”ê°€ í™•ì¸
        if self.bert_model:
            try:
                embeddings = self.bert_model.encode([original, user_input])
                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                
                # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (90% ì´ìƒ)ì—ì„œë§Œ ë¶€ë¶„ ë‹µë³€ ì¸ì •
                if similarity >= 0.9:
                    logger.info(f"BERT ê¸°ë°˜ ë¶€ë¶„ ë‹µë³€ ì¸ì •: {original} â†’ {user_input} (ìœ ì‚¬ë„: {similarity:.3f})")
                    return 1.1  # 10% ë³´ë„ˆìŠ¤ (ë¶€ë¶„ ë‹µë³€ í—ˆìš©)
                else:
                    logger.info(f"ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¶€ì¡±: {original} â†’ {user_input} (ìœ ì‚¬ë„: {similarity:.3f})")
                    return 1.0  # ë³´ë„ˆìŠ¤ ì—†ìŒ
                    
            except Exception as e:
                logger.warning(f"ë¶€ë¶„ ë‹µë³€ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        return 1.0  # ê¸°ë³¸ê°’

    def detect_structural_consistency(self, original: str, user_input: str) -> float:
        """
        êµ¬ì¡°ì  ì¼ê´€ì„± ê°ì§€ (ë¬¸ì¥ êµ¬ì¡°, ì–´ìˆœ, ê¸¸ì´ ë“±)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (1.0 = ì •ìƒ, 0.01 = 99% ê°ì )
        """
        # ê¸¸ì´ ì¼ê´€ì„± í™•ì¸
        length_ratio = len(user_input) / len(original) if len(original) > 0 else 1.0
        
        if length_ratio < 0.5 or length_ratio > 2.0:  # ê¸¸ì´ê°€ ë„ˆë¬´ ë‹¤ë¦„
            logger.info(f"êµ¬ì¡°ì  ì¼ê´€ì„± ë¶€ì¡±: ê¸¸ì´ ë¹„ìœ¨ {length_ratio:.2f}")
            return 0.1  # 90% ê°ì 
        
        # ì–´ìˆœ ì¼ê´€ì„± í™•ì¸
        orig_word_order = self._analyze_word_order(original)
        user_word_order = self._analyze_word_order(user_input)
        
        if orig_word_order != user_word_order:
            logger.info(f"ì–´ìˆœ ì¼ê´€ì„± ë¶€ì¡±: {orig_word_order} vs {user_word_order}")
            return 0.5  # 50% ê°ì 
        
        # ë¬¸ì¥ êµ¬ì¡° ì¼ê´€ì„± í™•ì¸
        orig_structure = self._analyze_sentence_structure(original)
        user_structure = self._analyze_sentence_structure(user_input)
        
        if orig_structure != user_structure:
            logger.info(f"ë¬¸ì¥ êµ¬ì¡° ì¼ê´€ì„± ë¶€ì¡±: {orig_structure} vs {user_structure}")
            return 0.7  # 30% ê°ì 
        
        return 1.0  # ì •ìƒ
    
    def _extract_animals_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë™ë¬¼ ë‹¨ì–´ ì¶”ì¶œ"""
        animals = self.learner.learned_patterns.get('animals', set())
        found_animals = []
        
        for animal in animals:
            if animal in text:
                found_animals.append(animal)
        
        return found_animals
    
    def _get_animal_group(self, animal: str, animal_groups: Dict) -> str:
        """ë™ë¬¼ì˜ ê·¸ë£¹ ì°¾ê¸°"""
        for group, animals in animal_groups.items():
            if animal in animals:
                return group
        return 'unknown'
    
    def comprehensive_check(self, original: str, user_input: str, base_similarity: float) -> float:
        """
        ì¢…í•©ì ì¸ ì§€ëŠ¥ì  ê²€ì‚¬ (ëª¨ë“  íŒ¨í„´ í†µí•©)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            base_similarity: ê¸°ë³¸ ìœ ì‚¬ë„ ì ìˆ˜
            
        Returns:
            float: ìµœì¢… ìœ ì‚¬ë„ ì ìˆ˜
        """
        final_score = base_similarity
        
        # 1. ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ ì •í™•ì„± ê°ì§€ (ìµœìš°ì„  - ë§¤ìš° ì—„ê²©)
        number_penalty = self.detect_number_quantity_accuracy(original, user_input)
        final_score *= number_penalty
        
        # 2. ë™ë¬¼ í˜¼ë™ ê°ì§€
        animal_penalty = self.detect_animal_confusion(original, user_input)
        final_score *= animal_penalty
        
        # 3. ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ê°ì§€
        opposite_penalty = self.detect_semantic_opposite(original, user_input)
        final_score *= opposite_penalty
        
        # 4. í•µì‹¬ ë‹¨ì–´ ëˆ„ë½ ê°ì§€
        critical_penalty = self.detect_critical_word_missing(original, user_input)
        final_score *= critical_penalty
        
        # 5. ë¬¸ë§¥ì  ì¼ê´€ì„± ê°ì§€ (ìƒˆë¡œ ì¶”ê°€)
        context_penalty = self.detect_context_consistency(original, user_input)
        final_score *= context_penalty
        
        # 6. êµ¬ì¡°ì  ì¼ê´€ì„± ê°ì§€ (ìƒˆë¡œ ì¶”ê°€)
        structural_penalty = self.detect_structural_consistency(original, user_input)
        final_score *= structural_penalty
        
        # 7. ë™ì˜ì–´ ë° ìœ ì‚¬ í‘œí˜„ ê°ì§€ (ë³´ë„ˆìŠ¤)
        synonym_bonus = self.detect_synonyms_and_variations(original, user_input)
        final_score *= synonym_bonus
        
        # 8. ë¬¸ë²•ì  ë³€í™” ê°ì§€ (ë³´ë„ˆìŠ¤ - ìƒˆë¡œ ì¶”ê°€)
        grammar_bonus = self.detect_grammar_variations(original, user_input)
        final_score *= grammar_bonus
        
        # 9. ìŒì„±í•™ì  ìœ ì‚¬ì„± ê°ì§€ (ë³´ë„ˆìŠ¤ - ìƒˆë¡œ ì¶”ê°€)
        phonetic_bonus = self.detect_phonetic_similarity(original, user_input)
        final_score *= phonetic_bonus
        
        # 10. ë¶€ë¶„ ë‹µë³€ í—ˆìš© ê°ì§€ (ë³´ë„ˆìŠ¤ - ìƒˆë¡œ ì¶”ê°€)
        partial_bonus = self.detect_partial_answer_acceptance(original, user_input)
        final_score *= partial_bonus
        
        return min(final_score, 1.0)


def test_ai_learning_system():
    """AI í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  AI í•™ìŠµ ê¸°ë°˜ ì§€ëŠ¥ì  ì†ë‹´ ê²€ì‚¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    learner = ProverbDataLearner()
    patterns = learner.full_learning_process()
    
    # ì§€ëŠ¥ì  ê²€ì‚¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    checker = IntelligentProverbChecker(learner)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        ("ë§ˆíŒŒëŒì— ê²Œëˆˆ ê°ì¶”ë“¯", "ë§ˆíŒŒëŒì— ê°œëˆˆ ê°ì¶”ë“¯", "ê²Œâ†’ê°œ ë™ë¬¼ í˜¼ë™"),
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼", "ê°€ëŠ” ë§ì´ ë‚˜ì˜ë©´", "ì˜ë¯¸ ë°˜ì „"),
        ("ê°œêµ¬ë¦¬ ì˜¬ì±™ì´ ì‹œì ˆ", "ê°œêµ¬ë¦¬ ì˜¬ì±™ì´ ì‹œì ˆ", "ì •ìƒ ì¼€ì´ìŠ¤"),
        ("ì—´ ë²ˆ ì°ì–´ ì•ˆ ë„˜ì–´ê°€ëŠ”", "ì—´ ë²ˆ ì°ì–´ ë„˜ì–´ê°€ëŠ”", "ë¶€ì •ì–´ ì œê±°"),
        ("ë‚´ ì½”ê°€ ì„ ì", "ë‚´ ì½”ê°€ ë„‰ ì", "ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ ë¶ˆì¼ì¹˜ (ì„ vs ë„‰)"),
        ("ë‚´ ì½”ê°€ ì„ ì", "ë‚´ ì½”ê°€ ì„ ì", "ìˆ«ì/ìˆ˜ëŸ‰ í‘œí˜„ ì •í™•"),
    ]
    
    for original, user_input, description in test_cases:
        # ê¸°ë³¸ ìœ ì‚¬ë„ ê³„ì‚°
        embeddings = checker.bert_model.encode([original, user_input])
        base_similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        
        # ì§€ëŠ¥ì  ê²€ì‚¬ ì ìš©
        final_score = checker.comprehensive_check(original, user_input, base_similarity)
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: {description}")
        print(f"ì›ë³¸: {original}")
        print(f"ì…ë ¥: {user_input}")
        print(f"ê¸°ë³¸ ìœ ì‚¬ë„: {base_similarity:.3f}")
        print(f"ìµœì¢… ì ìˆ˜: {final_score:.3f}")
        print(f"ê²°ê³¼: {'âœ… ì •ë‹µ' if final_score >= 0.5 else 'âŒ ì˜¤ë‹µ'}")


if __name__ == "__main__":
    test_ai_learning_system()
