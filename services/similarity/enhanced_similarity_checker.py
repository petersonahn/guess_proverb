#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ í–¥ìƒëœ ì†ë‹´ ìœ ì‚¬ë„ ê²€ì‚¬ ì‹œìŠ¤í…œ (Enhanced Proverb Similarity Checker)

ê¸°ì¡´ í•˜ë“œì½”ë”© ë°©ì‹ì„ ì™„ì „íˆ ëœ¯ì–´ê³ ì³ì„œ AI ê¸°ë°˜ ì§€ëŠ¥ì  ì˜ë¯¸ ë¶„ì„ìœ¼ë¡œ êµì²´
- ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ ì‹œìŠ¤í…œ í†µí•©
- ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„
- ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ìë™ ê°ì§€
- ì£¼ì œ ì¼ê´€ì„± ê²€ì‚¬
- ë¬¸ë§¥ì  ê´€ë ¨ì„± ë¶„ì„
"""

import mysql.connector
from sentence_transformers import SentenceTransformer, util
import random
import time
import re
from typing import List, Tuple, Dict, Set
from functools import lru_cache
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ ì‹œìŠ¤í…œì€ ì•„ë˜ì— ì§ì ‘ êµ¬í˜„ë¨
INTELLIGENT_CHECKER_AVAILABLE = True

# ë™ì  í‚¤ì›Œë“œ ê²€ì‚¬ ì‹œìŠ¤í…œì€ similarity_check.pyì— êµ¬í˜„ë¨
KEYWORD_CHECKER_AVAILABLE = True

# í†µí•©ëœ ì„ê³„ê°’ ì„¤ì • ì„í¬íŠ¸
try:
    from .similarity_check import get_threshold_by_length, THRESHOLD_MAP
    UNIFIED_THRESHOLD_AVAILABLE = True
except ImportError:
    UNIFIED_THRESHOLD_AVAILABLE = False
    print("ê²½ê³ : í†µí•©ëœ ì„ê³„ê°’ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# ===== ìƒìˆ˜ ì •ì˜ =====
# í•˜ë“œì½”ë”©ëœ ìƒìˆ˜ë“¤ì€ AI í•™ìŠµ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´ë¨
# ProverbDataLearner í´ë˜ìŠ¤ê°€ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.

# ===== ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ ì‹œìŠ¤í…œ =====
class IntelligentSemanticChecker:
    """ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  í•œêµ­ì–´ BERT ëª¨ë¸ëª…
        """
        self.model_name = model_name
        self.model = None
        self._load_model()
        
        # í•˜ë“œì½”ë”©ëœ íŒ¨í„´ë“¤ì€ AI í•™ìŠµ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´ë¨
        # ProverbDataLearner í´ë˜ìŠ¤ê°€ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        self.opposite_patterns = {}
        self.topic_keywords = {}
        self.intensity_patterns = {}
    
    def _load_model(self):
        """BERT ëª¨ë¸ ë¡œë”©"""
        try:
            self.model = SentenceTransformer(self.model_name)
            logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name}")
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.model = None
    
    def extract_keywords(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡
        """
        if not text:
            return []
        
        # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
        clean_text = re.sub(r'[^\w\s]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        words = clean_text.split()
        
        # 2. ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ í•„í„°ë§ (1ê¸€ì ì œì™¸, ì¡°ì‚¬ ì œì™¸)
        meaningful_words = []
        for word in words:
            if len(word) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
                # ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
                clean_word = re.sub(r'[ì´ê°€ì„ë¥¼ì—ì„œìœ¼ë¡œì—ê²Œë¶€í„°ê¹Œì§€ì™€ê³¼ë„ëŠ”ì€]', '', word)
                if len(clean_word) >= 2:
                    meaningful_words.append(clean_word)
        
        # 3. ë¹ˆë„ ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
        word_importance = {}
        for word in meaningful_words:
            # ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡, ë¹ˆë„ê°€ ë‚®ì„ìˆ˜ë¡ ì¤‘ìš”
            importance = len(word) * (1 / (meaningful_words.count(word) + 1))
            word_importance[word] = importance
        
        # 4. ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ (ìµœëŒ€ 5ê°œ)
        sorted_words = sorted(word_importance.items(), key=lambda x: x[1], reverse=True)
        keywords = [word for word, _ in sorted_words[:5]]
        
        return keywords
    
    def detect_semantic_opposite(self, original: str, user_input: str) -> float:
        """
        ì˜ë¯¸ì  ë°˜ëŒ€ ê´€ê³„ ìë™ ê°ì§€
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (1.0 = ì •ìƒ, 0.01 = 99% ê°ì )
        """
        if not self.model:
            return 1.0
        
        try:
            # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
            orig_keywords = self.extract_keywords(original)
            user_keywords = self.extract_keywords(user_input)
            
            if not orig_keywords or not user_keywords:
                return 1.0
            
            # 2. ë°˜ì˜ì–´ íŒ¨í„´ ë§¤ì¹­
            opposite_count = 0
            total_matches = 0
            
            for orig_word in orig_keywords:
                for user_word in user_keywords:
                    total_matches += 1
                    
                    # ì§ì ‘ì ì¸ ë°˜ì˜ì–´ ë§¤ì¹­
                    if orig_word in self.opposite_patterns:
                        if user_word in self.opposite_patterns[orig_word]:
                            opposite_count += 1
                    elif user_word in self.opposite_patterns:
                        if orig_word in self.opposite_patterns[user_word]:
                            opposite_count += 1
            
            # 3. AI ëª¨ë¸ ê¸°ë°˜ ì˜ë¯¸ ë°˜ëŒ€ ê°ì§€
            if self.model and len(orig_keywords) > 0 and len(user_keywords) > 0:
                # í‚¤ì›Œë“œë“¤ì„ ë¬¸ì¥ìœ¼ë¡œ ê²°í•©
                orig_sentence = ' '.join(orig_keywords)
                user_sentence = ' '.join(user_keywords)
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
                embeddings = self.model.encode([orig_sentence, user_sentence])
                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                
                # ìœ ì‚¬ë„ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ ë°˜ëŒ€ ì˜ë¯¸ë¡œ íŒë‹¨
                if similarity < 0.3:
                    opposite_count += 1
                    total_matches += 1
            
            # 4. ë°˜ëŒ€ ë¹„ìœ¨ ê³„ì‚°
            if total_matches == 0:
                return 1.0
            
            opposite_ratio = opposite_count / total_matches
            
            # 5. íŒ¨ë„í‹° ì ìš©
            if opposite_ratio > 0.5:  # 50% ì´ìƒì´ ë°˜ëŒ€ ì˜ë¯¸
                return 0.01  # 99% ê°ì 
            elif opposite_ratio > 0.3:  # 30% ì´ìƒì´ ë°˜ëŒ€ ì˜ë¯¸
                return 0.1   # 90% ê°ì 
            else:
                return 1.0   # ì •ìƒ
                
        except Exception as e:
            logger.error(f"ì˜ë¯¸ ë°˜ëŒ€ ê°ì§€ ì˜¤ë¥˜: {e}")
            return 1.0
    
    def detect_topic_consistency(self, original: str, user_input: str) -> float:
        """
        ì£¼ì œ ì¼ê´€ì„± ìë™ ê°ì§€
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (1.0 = ì •ìƒ, 0.01 = 99% ê°ì )
        """
        if not self.model:
            return 1.0
        
        try:
            # 1. ì£¼ì œë³„ í‚¤ì›Œë“œ ë§¤ì¹­
            orig_topics = self._classify_topics(original)
            user_topics = self._classify_topics(user_input)
            
            # 2. ì£¼ì œ ê²¹ì¹¨ ê³„ì‚°
            topic_overlap = len(orig_topics & user_topics)
            total_topics = len(orig_topics | user_topics)
            
            if total_topics == 0:
                return 1.0
            
            overlap_ratio = topic_overlap / total_topics
            
            # 3. AI ëª¨ë¸ ê¸°ë°˜ ì£¼ì œ ìœ ì‚¬ë„
            if self.model:
                embeddings = self.model.encode([original, user_input])
                semantic_similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                
                # ì£¼ì œ ê²¹ì¹¨ê³¼ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¢…í•©
                combined_score = (overlap_ratio * 0.6) + (semantic_similarity * 0.4)
            else:
                combined_score = overlap_ratio
            
            # 4. íŒ¨ë„í‹° ì ìš©
            if combined_score < 0.2:  # 20% ë¯¸ë§Œ ì¼ì¹˜
                return 0.01  # 99% ê°ì  (ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œ)
            elif combined_score < 0.4:  # 40% ë¯¸ë§Œ ì¼ì¹˜
                return 0.1   # 90% ê°ì  (ì£¼ì œê°€ ë§ì´ ë‹¤ë¦„)
            else:
                return 1.0   # ì •ìƒ
                
        except Exception as e:
            logger.error(f"ì£¼ì œ ì¼ê´€ì„± ê°ì§€ ì˜¤ë¥˜: {e}")
            return 1.0
    
    def _classify_topics(self, text: str) -> Set[str]:
        """
        í…ìŠ¤íŠ¸ì˜ ì£¼ì œ ë¶„ë¥˜
        
        Args:
            text: ë¶„ë¥˜í•  í…ìŠ¤íŠ¸
            
        Returns:
            Set[str]: í•´ë‹¹í•˜ëŠ” ì£¼ì œë“¤
        """
        topics = set()
        
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    topics.add(topic)
                    break
        
        return topics
    
    def detect_contextual_relevance(self, original: str, user_input: str) -> float:
        """
        ë¬¸ë§¥ì  ê´€ë ¨ì„± ìë™ ê°ì§€
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (1.0 = ì •ìƒ, 0.01 = 99% ê°ì )
        """
        if not self.model:
            return 1.0
        
        try:
            # 1. ì „ì²´ ë¬¸ì¥ ìœ ì‚¬ë„
            embeddings = self.model.encode([original, user_input])
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            # 2. í‚¤ì›Œë“œ ìœ ì‚¬ë„
            orig_keywords = self.extract_keywords(original)
            user_keywords = self.extract_keywords(user_input)
            
            keyword_similarity = 0.0
            if orig_keywords and user_keywords:
                orig_embedding = self.model.encode([' '.join(orig_keywords)])
                user_embedding = self.model.encode([' '.join(user_keywords)])
                keyword_similarity = cosine_similarity(orig_embedding, user_embedding)[0][0]
            
            # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            combined_score = (similarity * 0.7) + (keyword_similarity * 0.3)
            
            # 4. íŒ¨ë„í‹° ì ìš©
            if combined_score < 0.3:  # 30% ë¯¸ë§Œ ìœ ì‚¬
                return 0.01  # 99% ê°ì  (ê´€ë ¨ì„± ì—†ìŒ)
            elif combined_score < 0.5:  # 50% ë¯¸ë§Œ ìœ ì‚¬
                return 0.2   # 80% ê°ì  (ê´€ë ¨ì„± ë‚®ìŒ)
            else:
                return 1.0   # ì •ìƒ
                
        except Exception as e:
            logger.error(f"ë¬¸ë§¥ì  ê´€ë ¨ì„± ê°ì§€ ì˜¤ë¥˜: {e}")
            return 1.0
    
    def comprehensive_semantic_check(self, original: str, user_input: str) -> Dict[str, float]:
        """
        ì¢…í•©ì ì¸ ì˜ë¯¸ ê²€ì‚¬
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            Dict[str, float]: ê° ê²€ì‚¬ í•­ëª©ë³„ íŒ¨ë„í‹° ê³„ìˆ˜
        """
        results = {
            'semantic_opposite': self.detect_semantic_opposite(original, user_input),
            'topic_consistency': self.detect_topic_consistency(original, user_input),
            'contextual_relevance': self.detect_contextual_relevance(original, user_input)
        }
        
        # ì „ì²´ íŒ¨ë„í‹° ê³„ì‚° (ê°€ì¥ ì—„ê²©í•œ ê¸°ì¤€ ì ìš©)
        min_penalty = min(results.values())
        
        results['final_penalty'] = min_penalty
        results['is_valid'] = min_penalty >= 0.5  # 50% ì´ìƒì´ë©´ ìœ íš¨í•œ ë‹µì•ˆ
        
        return results


# ===== í–¥ìƒëœ ì†ë‹´ ìœ ì‚¬ë„ ê²€ì‚¬ ì‹œìŠ¤í…œ =====
class EnhancedProverbSimilarityChecker:
    """í–¥ìƒëœ ì†ë‹´ ìœ ì‚¬ë„ ê²€ì‚¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  í•œêµ­ì–´ BERT ëª¨ë¸ëª…
        """
        self.model_name = model_name
        self.model = None
        self.intelligent_checker = None
        self._load_models()
    
    def _load_models(self):
        """ëª¨ë¸ë“¤ ë¡œë”©"""
        try:
            # SentenceTransformer ëª¨ë¸ ë¡œë”©
            self.model = SentenceTransformer(self.model_name)
            print(f"âœ… SentenceTransformer ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name}")
            
            # ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ ì‹œìŠ¤í…œ ë¡œë”©
            if INTELLIGENT_CHECKER_AVAILABLE:
                self.intelligent_checker = IntelligentSemanticChecker(self.model_name)
                print("âœ… ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ ì‹œìŠ¤í…œ ë¡œë”© ì™„ë£Œ")
            else:
                print("âš ï¸ ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.model = None
            self.intelligent_checker = None
    
    def get_threshold_by_length(self, answer: str) -> float:
        """
        ì†ë‹´ ê¸¸ì´ì— ë”°ë¥¸ í†µí•©ëœ ì„ê³„ê°’ ì„¤ì •
        
        í†µí•©ëœ ì„ê³„ê°’ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ìœ ì‚¬ë„ ê²€ì‚¬ ì œê³µ
        
        Args:
            answer: ì†ë‹´ ë‹µì•ˆ
            
        Returns:
            float: ì„ê³„ê°’ (0.0 ~ 1.0)
        """
        # í†µí•©ëœ ì„ê³„ê°’ ì„¤ì • ì‚¬ìš©
        if UNIFIED_THRESHOLD_AVAILABLE:
            return get_threshold_by_length(answer)
        
        # í†µí•© ì„¤ì • ì‚¬ìš© ë¶ˆê°€ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return 0.70  # ê¸°ë³¸ ì„ê³„ê°’
    
    def detect_compound_word_split(self, original: str, user_input: str) -> bool:
        """
        ë³µí•©ì–´ ë¶„ë¦¬ ê°ì§€ (ìì—°ìŠ¤ëŸ¬ìš´ ë¶„ë¦¬ëŠ” í—ˆìš©)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            bool: ë³µí•©ì–´ ë¶„ë¦¬ ì—¬ë¶€
        """
        # ë³µí•©ì–´ íŒ¨í„´ë“¤
        compound_patterns = [
            r'(\w+)(\w+)',  # ì¼ë°˜ì ì¸ ë³µí•©ì–´
            r'(\w+)(\w+)(\w+)',  # 3ê¸€ì ë³µí•©ì–´
        ]
        
        for pattern in compound_patterns:
            matches = re.finditer(pattern, original)
            for match in matches:
                compound_word = match.group()
                if len(compound_word) >= 4:  # 4ê¸€ì ì´ìƒ ë³µí•©ì–´ë§Œ
                    # ë¶„ë¦¬ëœ í˜•íƒœë¡œ ê²€ì‚¬
                    if len(compound_word) == 4:
                        split1 = compound_word[:2]
                        split2 = compound_word[2:]
                        if split1 in user_input and split2 in user_input:
                            return True
                    elif len(compound_word) == 6:
                        split1 = compound_word[:2]
                        split2 = compound_word[2:4]
                        split3 = compound_word[4:]
                        if split1 in user_input and split2 in user_input and split3 in user_input:
                            return True
        
        return False
    
    def detect_korean_negation_change(self, original: str, user_input: str) -> bool:
        """
        í•œêµ­ì–´ ë¶€ì • í‘œí˜„ ë³€í™” ê°ì§€
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            bool: ë¶€ì • í‘œí˜„ ë³€í™” ì—¬ë¶€
        """
        # ë¶€ì • í‘œí˜„ íŒ¨í„´ë“¤
        negation_patterns = [
            ('ì•ŠëŠ”ë‹¤', 'í•œë‹¤'),
            ('ì•ˆ í•œë‹¤', 'í•œë‹¤'),
            ('ëª¨ë¥¸ë‹¤', 'ì•ˆë‹¤'),
            ('ì—†ë‹¤', 'ìˆë‹¤'),
            ('ì•ˆëœë‹¤', 'ëœë‹¤'),
            ('ì•ˆë¼', 'ë¼'),
        ]
        
        for neg_pattern, pos_pattern in negation_patterns:
            if (neg_pattern in original and pos_pattern in user_input) or \
               (pos_pattern in original and neg_pattern in user_input):
                return True
        
        return False
    
    def detect_critical_typos(self, original: str, user_input: str) -> float:
        """
        í•µì‹¬ ì˜¤íƒ€ ê°ì§€ (AI í•™ìŠµ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´ë¨)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            float: íŒ¨ë„í‹° ê³„ìˆ˜ (ê¸°ë³¸ê°’ 1.0)
        """
        # AI í•™ìŠµ ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜
        return 1.0
    
    def enhanced_similarity_check(self, original: str, user_input: str, base_similarity: float) -> float:
        """
        í–¥ìƒëœ ìœ ì‚¬ë„ ê²€ì‚¬ (ì§€ëŠ¥ì  ì˜ë¯¸ ë¶„ì„ í†µí•©)
        
        Args:
            original: ì›ë³¸ ì†ë‹´
            user_input: ì‚¬ìš©ì ì…ë ¥
            base_similarity: ê¸°ë³¸ ìœ ì‚¬ë„ ì ìˆ˜
            
        Returns:
            float: ìµœì¢… ìœ ì‚¬ë„ ì ìˆ˜
        """
        final_score = base_similarity
        
        # 1. ë³µí•©ì–´ ë¶„ë¦¬ ê°ì§€ (ë³´ë„ˆìŠ¤)
        if self.detect_compound_word_split(original, user_input):
            final_score *= 1.05  # 5% ë³´ë„ˆìŠ¤
        
        # 2. ì§€ëŠ¥ì  ì˜ë¯¸ ê²€ì‚¬ (ìµœìš°ì„ )
        if self.intelligent_checker:
            semantic_results = self.intelligent_checker.comprehensive_semantic_check(original, user_input)
            final_score *= semantic_results['final_penalty']
            
            # ì˜ë¯¸ì ìœ¼ë¡œ ì™„ì „íˆ ë¬´íš¨í•œ ê²½ìš°
            if not semantic_results['is_valid']:
                return 0.01  # 99% ê°ì 
        
        # 3. í•œêµ­ì–´ ë¶€ì • í‘œí˜„ ë³€í™” ê°ì§€
        if self.detect_korean_negation_change(original, user_input):
            final_score *= 0.1  # 90% ê°ì 
        
        # 4. í•µì‹¬ ì˜¤íƒ€ ê°ì§€
        typo_penalty = self.detect_critical_typos(original, user_input)
        final_score *= typo_penalty
        
        # 5. ê¸¸ì´ ë¹„ìœ¨ ê²€ì‚¬
        if len(original) > 0:
            input_length_ratio = len(user_input) / len(original)
            
            # ì§§ì€ ì†ë‹´ì€ ê¸¸ì´ ë¹„ìœ¨ì´ ì¤‘ìš”
            if len(original) <= 5:
                if input_length_ratio < 0.8:  # 80% ë¯¸ë§Œ
                    final_score *= 0.5  # 50% ê°ì 
            # ê¸´ ì†ë‹´ì€ ì˜ë¯¸ê°€ ë” ì¤‘ìš”
            elif len(original) > 10:
                if input_length_ratio < 0.6:  # 60% ë¯¸ë§Œ
                    final_score *= 0.7  # 30% ê°ì 
        
        # 6. ì™„ì „ ë™ì¼ ë³´ë„ˆìŠ¤
        if original.replace(' ', '') == user_input.replace(' ', ''):
            final_score *= 1.1  # 10% ë³´ë„ˆìŠ¤
        
        return min(final_score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def check_proverb(self, user_input: str, answers: List[str], threshold: float) -> Tuple[bool, str, float]:
        """
        ì†ë‹´ ìœ ì‚¬ë„ ê²€ì‚¬
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥
            answers: ê°€ëŠ¥í•œ ë‹µì•ˆ ëª©ë¡
            threshold: ì„ê³„ê°’
            
        Returns:
            Tuple[bool, str, float]: (ì •ë‹µ ì—¬ë¶€, ë§¤ì¹­ëœ ë‹µì•ˆ, ìœ ì‚¬ë„ ì ìˆ˜)
        """
        if not self.model:
            return False, "", 0.0
        
        best_match = ""
        best_score = 0.0
        
        for answer in answers:
            # ê¸°ë³¸ ìœ ì‚¬ë„ ê³„ì‚°
            embeddings = self.model.encode([answer, user_input])
            base_similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            # í–¥ìƒëœ ìœ ì‚¬ë„ ê²€ì‚¬ ì ìš©
            enhanced_score = self.enhanced_similarity_check(answer, user_input, base_similarity)
            
            if enhanced_score > best_score:
                best_score = enhanced_score
                best_match = answer
        
        is_correct = best_score >= threshold
        return is_correct, best_match, best_score


def test_enhanced_checker():
    """í–¥ìƒëœ ê²€ì‚¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    checker = EnhancedProverbSimilarityChecker()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤ (ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì†ë‹´)
    test_cases = [
        # ì§§ì€ ì†ë‹´ë“¤ (ë§¤ìš° ì—„ê²©í•œ ì„ê³„ê°’: 88-95%)
        ("ê³±ë‹¤", "ê³±ë‹¤", "2ê¸€ì ì™„ì „ ì¼ì¹˜"),
        ("ê³±ë‹¤", "ê³ ì™€ì•¼", "2ê¸€ì ì˜ë¯¸ ë°˜ì „"),
        ("í¸ì´ë¼", "í¸ì´ì•¼", "3ê¸€ì ì–´ë¯¸ ë³€í™”"),
        ("í¸ì´ë¼", "í¸ì´ë‹¤", "3ê¸€ì ì˜ë¯¸ ìœ ì‚¬"),
        
        # ì¤‘ê°„ ê¸¸ì´ ì†ë‹´ë“¤ (ì ë‹¹íˆ ì—„ê²©í•œ ì„ê³„ê°’: 80-85%)
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼", "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼", "5ê¸€ì ì™„ì „ ì¼ì¹˜"),
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼", "ê°€ëŠ” ë§ì´ ë‚˜ì˜ë©´", "5ê¸€ì ì˜ë¯¸ ë°˜ì „"),
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼", "ê°€ëŠ” ë§ì´ ì¢‹ìœ¼ë©´", "5ê¸€ì ì˜ë¯¸ ìœ ì‚¬"),
        
        # ê¸´ ì†ë‹´ë“¤ (ì˜ë¯¸ ì¤‘ì‹¬ì´ì§€ë§Œ ì—„ê²©í•œ ì„ê³„ê°’: 75-78%)
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", "ì™„ì „ ì¼ì¹˜"),
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ë„ ê³±ë‹¤", "ì–´ë¯¸ ë³€í™”"),
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", "ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ì¢‹ë‹¤", "ì˜ë¯¸ ìœ ì‚¬"),
        
        # ë¬¸ì œê°€ ìˆëŠ” ë‹µì•ˆë“¤ (ì§€ëŠ¥ì  ì‹œìŠ¤í…œì´ ê±¸ëŸ¬ë‚´ì•¼ í•¨)
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", "íŒ¥ì‹¬ì€ë° ì½©ë‚œë‹¤", "ì™„ì „íˆ ë‹¤ë¥¸ ì˜ë¯¸"),
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", "ê°€ëŠ” ë§ì´ ë‚˜ì˜ë©´ ì˜¤ëŠ” ë§ì´ ë‚˜ì˜ë‹¤", "ì˜ë¯¸ ë°˜ì „"),
        ("ê°€ëŠ” ë§ì´ ê³ ì™€ì•¼ ì˜¤ëŠ” ë§ì´ ê³±ë‹¤", "ì†Œ ìƒê³  ì™¸ì–‘ê°„", "ë‹¤ë¥¸ ì£¼ì œ"),
    ]
    
    print("ğŸ¯ í–¥ìƒëœ ì†ë‹´ ìœ ì‚¬ë„ ê²€ì‚¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print("ğŸ“Š ìƒˆë¡œìš´ ì„ê³„ê°’ ì „ëµ (ì—…ë°ì´íŠ¸ë¨):")
    print("  - ì§§ì€ ì†ë‹´ (2-5ê¸€ì): 88-95% (ë§¤ìš° ì—„ê²©)")
    print("  - ì¤‘ê°„ ì†ë‹´ (6-10ê¸€ì): 80-85% (ì ë‹¹íˆ ì—„ê²©)")
    print("  - ê¸´ ì†ë‹´ (15ê¸€ì+): 75-78% (ì˜ë¯¸ ì¤‘ì‹¬ì´ì§€ë§Œ ì—„ê²©)")
    print("=" * 70)
    
    for original, user_input, description in test_cases:
        threshold = checker.get_threshold_by_length(original)
        is_correct, matched, score = checker.check_proverb(user_input, [original], threshold)
        
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: {description}")
        print(f"ì›ë³¸: {original} ({len(original)}ê¸€ì)")
        print(f"ì…ë ¥: {user_input}")
        print(f"ì„ê³„ê°’: {threshold:.3f}")
        print(f"ìœ ì‚¬ë„: {score:.3f}")
        print(f"ê²°ê³¼: {'âœ… ì •ë‹µ' if is_correct else 'âŒ ì˜¤ë‹µ'}")
        
        # ì„ê³„ê°’ ëŒ€ë¹„ ìœ ì‚¬ë„ ë¶„ì„
        if score >= threshold:
            print(f"ğŸ’¡ ë¶„ì„: ìœ ì‚¬ë„({score:.3f}) >= ì„ê³„ê°’({threshold:.3f}) â†’ ì •ë‹µ")
        else:
            print(f"ğŸ’¡ ë¶„ì„: ìœ ì‚¬ë„({score:.3f}) < ì„ê³„ê°’({threshold:.3f}) â†’ ì˜¤ë‹µ")


if __name__ == "__main__":
    test_enhanced_checker()
